{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %load_ext jupyternotify\n",
    "%reload_ext jupyternotify\n",
    "from task2_utils_niklas import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression,Ridge,Lasso\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "from matplotlib import pyplot\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN,RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.special import expit as sigmoid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\"./handout/train_features.csv\",index_col='pid')\n",
    "trains_labels = pd.read_csv(\"./handout/train_labels.csv\",index_col='pid')\n",
    "test_features = pd.read_csv(\"./handout/test_features.csv\",index_col='pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features has 227940 rows and 36 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Age</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>PTT</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Hgb</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>RRate</th>\n",
       "      <th>Fibrinogen</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>PaCO2</th>\n",
       "      <th>AST</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>ABPm</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>ABPd</th>\n",
       "      <th>Calcium</th>\n",
       "      <th>Alkalinephos</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>Bilirubin_direct</th>\n",
       "      <th>Chloride</th>\n",
       "      <th>Hct</th>\n",
       "      <th>Heartrate</th>\n",
       "      <th>Bilirubin_total</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>ABPs</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>227940.000000</td>\n",
       "      <td>227940.000000</td>\n",
       "      <td>9783.000000</td>\n",
       "      <td>10299.000000</td>\n",
       "      <td>20105.000000</td>\n",
       "      <td>10756.000000</td>\n",
       "      <td>81115.000000</td>\n",
       "      <td>22295.000000</td>\n",
       "      <td>12559.00000</td>\n",
       "      <td>19887.000000</td>\n",
       "      <td>187785.000000</td>\n",
       "      <td>2493.000000</td>\n",
       "      <td>11590.000000</td>\n",
       "      <td>19083.000000</td>\n",
       "      <td>17792.000000</td>\n",
       "      <td>21043.000000</td>\n",
       "      <td>5761.000000</td>\n",
       "      <td>26602.000000</td>\n",
       "      <td>18035.000000</td>\n",
       "      <td>13014.000000</td>\n",
       "      <td>47036.000000</td>\n",
       "      <td>195889.000000</td>\n",
       "      <td>17523.000000</td>\n",
       "      <td>28393.000000</td>\n",
       "      <td>152418.000000</td>\n",
       "      <td>17830.000000</td>\n",
       "      <td>5708.000000</td>\n",
       "      <td>195192.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>13917.000000</td>\n",
       "      <td>27297.000000</td>\n",
       "      <td>200128.000000</td>\n",
       "      <td>5326.000000</td>\n",
       "      <td>3776.000000</td>\n",
       "      <td>191650.000000</td>\n",
       "      <td>25046.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>7.014399</td>\n",
       "      <td>62.073809</td>\n",
       "      <td>32.883114</td>\n",
       "      <td>40.091310</td>\n",
       "      <td>23.192664</td>\n",
       "      <td>2.859716</td>\n",
       "      <td>36.852136</td>\n",
       "      <td>10.628208</td>\n",
       "      <td>23.48810</td>\n",
       "      <td>-1.239284</td>\n",
       "      <td>18.154043</td>\n",
       "      <td>262.496911</td>\n",
       "      <td>3.612519</td>\n",
       "      <td>11.738649</td>\n",
       "      <td>1.495777</td>\n",
       "      <td>41.115696</td>\n",
       "      <td>193.444888</td>\n",
       "      <td>0.701666</td>\n",
       "      <td>204.666426</td>\n",
       "      <td>93.010527</td>\n",
       "      <td>142.169407</td>\n",
       "      <td>82.117276</td>\n",
       "      <td>2.004149</td>\n",
       "      <td>4.152729</td>\n",
       "      <td>64.014711</td>\n",
       "      <td>7.161149</td>\n",
       "      <td>97.796163</td>\n",
       "      <td>97.663449</td>\n",
       "      <td>1.390723</td>\n",
       "      <td>106.260185</td>\n",
       "      <td>31.283090</td>\n",
       "      <td>84.522371</td>\n",
       "      <td>1.640941</td>\n",
       "      <td>7.269240</td>\n",
       "      <td>122.369877</td>\n",
       "      <td>7.367231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>4.716103</td>\n",
       "      <td>16.451854</td>\n",
       "      <td>7.802065</td>\n",
       "      <td>26.034961</td>\n",
       "      <td>20.024289</td>\n",
       "      <td>2.428368</td>\n",
       "      <td>0.875152</td>\n",
       "      <td>2.074859</td>\n",
       "      <td>4.40378</td>\n",
       "      <td>4.192677</td>\n",
       "      <td>5.037031</td>\n",
       "      <td>133.020910</td>\n",
       "      <td>1.384462</td>\n",
       "      <td>10.088872</td>\n",
       "      <td>1.898112</td>\n",
       "      <td>8.929873</td>\n",
       "      <td>682.836708</td>\n",
       "      <td>24.522126</td>\n",
       "      <td>104.156406</td>\n",
       "      <td>10.887271</td>\n",
       "      <td>56.894530</td>\n",
       "      <td>16.471871</td>\n",
       "      <td>0.437286</td>\n",
       "      <td>0.670168</td>\n",
       "      <td>13.920097</td>\n",
       "      <td>2.812067</td>\n",
       "      <td>122.773379</td>\n",
       "      <td>2.786186</td>\n",
       "      <td>2.792722</td>\n",
       "      <td>5.916082</td>\n",
       "      <td>5.770425</td>\n",
       "      <td>17.643437</td>\n",
       "      <td>3.244145</td>\n",
       "      <td>25.172442</td>\n",
       "      <td>23.273834</td>\n",
       "      <td>0.074384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>6.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>21.00000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>54.250000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>7.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>23.90000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>30.900000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>7.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>251.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.210000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>7.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>1179.000000</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>41.900000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9961.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>2322.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>952.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>3833.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>63.400000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>7.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            Age        EtCO2           PTT           BUN       Lactate          Temp           Hgb         HCO3    BaseExcess          RRate   Fibrinogen     Phosphate           WBC    Creatinine         PaCO2          AST          FiO2     Platelets          SaO2       Glucose           ABPm     Magnesium     Potassium           ABPd       Calcium  Alkalinephos           SpO2  Bilirubin_direct      Chloride           Hct      Heartrate  Bilirubin_total    TroponinI           ABPs            pH\n",
       "count  227940.000000  227940.000000  9783.000000  10299.000000  20105.000000  10756.000000  81115.000000  22295.000000  12559.00000  19887.000000  187785.000000  2493.000000  11590.000000  19083.000000  17792.000000  21043.000000  5761.000000  26602.000000  18035.000000  13014.000000  47036.000000  195889.000000  17523.000000  28393.000000  152418.000000  17830.000000   5708.000000  195192.000000        719.000000  13917.000000  27297.000000  200128.000000      5326.000000  3776.000000  191650.000000  25046.000000\n",
       "mean        7.014399      62.073809    32.883114     40.091310     23.192664      2.859716     36.852136     10.628208     23.48810     -1.239284      18.154043   262.496911      3.612519     11.738649      1.495777     41.115696   193.444888      0.701666    204.666426     93.010527    142.169407      82.117276      2.004149      4.152729      64.014711      7.161149     97.796163      97.663449          1.390723    106.260185     31.283090      84.522371         1.640941     7.269240     122.369877      7.367231\n",
       "std         4.716103      16.451854     7.802065     26.034961     20.024289      2.428368      0.875152      2.074859      4.40378      4.192677       5.037031   133.020910      1.384462     10.088872      1.898112      8.929873   682.836708     24.522126    104.156406     10.887271     56.894530      16.471871      0.437286      0.670168      13.920097      2.812067    122.773379       2.786186          2.792722      5.916082      5.770425      17.643437         3.244145    25.172442      23.273834      0.074384\n",
       "min         1.000000      15.000000    10.000000     12.500000      1.000000      0.200000     21.000000      3.300000      0.00000    -29.000000       1.000000    34.000000      0.200000      0.100000      0.100000     10.000000     5.000000      0.000000      2.000000     24.000000     15.000000      20.000000      0.500000      1.300000      20.000000      1.000000     12.000000      20.000000          0.010000     66.000000      9.400000      23.000000         0.100000     0.010000      21.000000      6.820000\n",
       "25%         4.000000      52.000000    28.500000     27.800000     12.000000      1.400000     36.000000      9.200000     21.00000     -3.000000      15.000000   177.000000      2.800000      7.600000      0.700000     36.000000    21.000000      0.400000    137.000000     95.000000    108.000000      71.000000      1.700000      3.700000      54.250000      7.300000     53.000000      96.000000          0.100000    103.000000     27.300000      72.000000         0.500000     0.030000     105.000000      7.330000\n",
       "50%         7.000000      64.000000    33.000000     32.200000     17.000000      2.100000     37.000000     10.500000     23.90000     -1.000000      18.000000   233.000000      3.400000     10.400000      0.900000     40.000000    36.000000      0.500000    189.000000     97.000000    130.000000      80.000000      2.000000      4.100000      62.000000      8.200000     72.000000      98.000000          0.300000    107.000000     30.900000      83.000000         0.800000     0.150000     119.000000      7.370000\n",
       "75%        10.000000      74.000000    38.000000     40.600000     27.000000      3.400000     37.000000     12.000000     26.00000      0.000000      21.000000   316.000000      4.200000     14.000000      1.380000     45.000000    84.000000      0.600000    251.000000     98.000000    160.000000      91.000000      2.200000      4.500000      72.000000      8.700000    104.000000     100.000000          1.210000    110.000000     35.000000      95.000000         1.400000     2.050000     137.000000      7.410000\n",
       "max       315.000000     100.000000   100.000000    250.000000    268.000000     31.000000     42.000000     23.800000     50.00000    100.000000      97.000000  1179.000000     16.400000    440.000000     41.900000    100.000000  9961.000000   4000.000000   2322.000000    100.000000    952.000000     300.000000      9.600000     10.750000     298.000000     20.600000   3833.000000     100.000000         21.200000    141.000000     63.400000     191.000000        46.500000   440.000000     287.000000      7.780000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"number of unique pid is {}\".format(np.size(train_features.pid.unique())))\n",
    "print(\"train_features has {} rows and {} columns\".format(train_features.shape[0],train_features.shape[1]))\n",
    "train_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Age</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>PTT</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Hgb</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>RRate</th>\n",
       "      <th>Fibrinogen</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>PaCO2</th>\n",
       "      <th>AST</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>ABPm</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>ABPd</th>\n",
       "      <th>Calcium</th>\n",
       "      <th>Alkalinephos</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>Bilirubin_direct</th>\n",
       "      <th>Chloride</th>\n",
       "      <th>Hct</th>\n",
       "      <th>Heartrate</th>\n",
       "      <th>Bilirubin_total</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>ABPs</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>218157</td>\n",
       "      <td>217641</td>\n",
       "      <td>207835</td>\n",
       "      <td>217184</td>\n",
       "      <td>146825</td>\n",
       "      <td>205645</td>\n",
       "      <td>215381</td>\n",
       "      <td>208053</td>\n",
       "      <td>40155</td>\n",
       "      <td>225447</td>\n",
       "      <td>216350</td>\n",
       "      <td>208857</td>\n",
       "      <td>210148</td>\n",
       "      <td>206897</td>\n",
       "      <td>222179</td>\n",
       "      <td>201338</td>\n",
       "      <td>209905</td>\n",
       "      <td>214926</td>\n",
       "      <td>180904</td>\n",
       "      <td>32051</td>\n",
       "      <td>210417</td>\n",
       "      <td>199547</td>\n",
       "      <td>75522</td>\n",
       "      <td>210110</td>\n",
       "      <td>222232</td>\n",
       "      <td>32748</td>\n",
       "      <td>227221</td>\n",
       "      <td>214023</td>\n",
       "      <td>200643</td>\n",
       "      <td>27812</td>\n",
       "      <td>222614</td>\n",
       "      <td>224164</td>\n",
       "      <td>36290</td>\n",
       "      <td>202894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time  Age   EtCO2     PTT     BUN  Lactate    Temp     Hgb    HCO3  BaseExcess  RRate  Fibrinogen  Phosphate     WBC  Creatinine   PaCO2     AST    FiO2  Platelets    SaO2  Glucose   ABPm  Magnesium  Potassium   ABPd  Calcium  Alkalinephos   SpO2  Bilirubin_direct  Chloride     Hct  Heartrate  Bilirubin_total  TroponinI   ABPs      pH\n",
       "0     0    0  218157  217641  207835   217184  146825  205645  215381      208053  40155      225447     216350  208857      210148  206897  222179  201338     209905  214926   180904  32051     210417     199547  75522   210110        222232  32748            227221    214023  200643      27812           222614     224164  36290  202894"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.isna().sum().to_frame().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, there are a lot of NA in the data which needs to be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trains_labels has 18995 rows and 15 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.00000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>18995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.268281</td>\n",
       "      <td>0.073704</td>\n",
       "      <td>0.239747</td>\n",
       "      <td>0.23622</td>\n",
       "      <td>0.240590</td>\n",
       "      <td>0.200211</td>\n",
       "      <td>0.099763</td>\n",
       "      <td>0.233693</td>\n",
       "      <td>0.033904</td>\n",
       "      <td>0.066017</td>\n",
       "      <td>0.057278</td>\n",
       "      <td>18.795960</td>\n",
       "      <td>82.511171</td>\n",
       "      <td>96.947311</td>\n",
       "      <td>84.119716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.443076</td>\n",
       "      <td>0.261295</td>\n",
       "      <td>0.426940</td>\n",
       "      <td>0.42477</td>\n",
       "      <td>0.427453</td>\n",
       "      <td>0.400168</td>\n",
       "      <td>0.299692</td>\n",
       "      <td>0.423190</td>\n",
       "      <td>0.180986</td>\n",
       "      <td>0.248319</td>\n",
       "      <td>0.232380</td>\n",
       "      <td>3.511241</td>\n",
       "      <td>12.745110</td>\n",
       "      <td>2.110957</td>\n",
       "      <td>14.718396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.550000</td>\n",
       "      <td>73.200000</td>\n",
       "      <td>95.900000</td>\n",
       "      <td>73.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>97.100000</td>\n",
       "      <td>83.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>90.200000</td>\n",
       "      <td>98.300000</td>\n",
       "      <td>93.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.100000</td>\n",
       "      <td>147.100000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>155.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       LABEL_BaseExcess  LABEL_Fibrinogen     LABEL_AST  LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  LABEL_TroponinI    LABEL_SaO2  LABEL_Bilirubin_direct   LABEL_EtCO2  LABEL_Sepsis   LABEL_RRate    LABEL_ABPm    LABEL_SpO2  LABEL_Heartrate\n",
       "count      18995.000000      18995.000000  18995.000000         18995.00000           18995.000000   18995.000000     18995.000000  18995.000000            18995.000000  18995.000000  18995.000000  18995.000000  18995.000000  18995.000000     18995.000000\n",
       "mean           0.268281          0.073704      0.239747             0.23622               0.240590       0.200211         0.099763      0.233693                0.033904      0.066017      0.057278     18.795960     82.511171     96.947311        84.119716\n",
       "std            0.443076          0.261295      0.426940             0.42477               0.427453       0.400168         0.299692      0.423190                0.180986      0.248319      0.232380      3.511241     12.745110      2.110957        14.718396\n",
       "min            0.000000          0.000000      0.000000             0.00000               0.000000       0.000000         0.000000      0.000000                0.000000      0.000000      0.000000      1.000000     26.000000     27.000000        30.200000\n",
       "25%            0.000000          0.000000      0.000000             0.00000               0.000000       0.000000         0.000000      0.000000                0.000000      0.000000      0.000000     16.550000     73.200000     95.900000        73.700000\n",
       "50%            0.000000          0.000000      0.000000             0.00000               0.000000       0.000000         0.000000      0.000000                0.000000      0.000000      0.000000     18.400000     81.000000     97.100000        83.400000\n",
       "75%            1.000000          0.000000      0.000000             0.00000               0.000000       0.000000         0.000000      0.000000                0.000000      0.000000      0.000000     20.600000     90.200000     98.300000        93.600000\n",
       "max            1.000000          1.000000      1.000000             1.00000               1.000000       1.000000         1.000000      1.000000                1.000000      1.000000      1.000000     41.100000    147.100000    100.000000       155.600000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"number of unique pid is {}\".format(np.size(trains_labels.pid.unique())))\n",
    "print(\"trains_labels has {} rows and {} columns\".format(trains_labels.shape[0],trains_labels.shape[1]))\n",
    "trains_labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_features has 151968 rows and 36 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Age</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>PTT</th>\n",
       "      <th>BUN</th>\n",
       "      <th>Lactate</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Hgb</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>RRate</th>\n",
       "      <th>Fibrinogen</th>\n",
       "      <th>Phosphate</th>\n",
       "      <th>WBC</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>PaCO2</th>\n",
       "      <th>AST</th>\n",
       "      <th>FiO2</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>ABPm</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>ABPd</th>\n",
       "      <th>Calcium</th>\n",
       "      <th>Alkalinephos</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>Bilirubin_direct</th>\n",
       "      <th>Chloride</th>\n",
       "      <th>Hct</th>\n",
       "      <th>Heartrate</th>\n",
       "      <th>Bilirubin_total</th>\n",
       "      <th>TroponinI</th>\n",
       "      <th>ABPs</th>\n",
       "      <th>pH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>151968.000000</td>\n",
       "      <td>151968.000000</td>\n",
       "      <td>6528.000000</td>\n",
       "      <td>6853.000000</td>\n",
       "      <td>13448.000000</td>\n",
       "      <td>7276.000000</td>\n",
       "      <td>54155.000000</td>\n",
       "      <td>14872.000000</td>\n",
       "      <td>8422.000000</td>\n",
       "      <td>13238.000000</td>\n",
       "      <td>125299.000000</td>\n",
       "      <td>1655.000000</td>\n",
       "      <td>7686.000000</td>\n",
       "      <td>12753.000000</td>\n",
       "      <td>11805.000000</td>\n",
       "      <td>13881.000000</td>\n",
       "      <td>3908.000000</td>\n",
       "      <td>17799.000000</td>\n",
       "      <td>11873.000000</td>\n",
       "      <td>8670.000000</td>\n",
       "      <td>31072.000000</td>\n",
       "      <td>130672.000000</td>\n",
       "      <td>11827.000000</td>\n",
       "      <td>19015.000000</td>\n",
       "      <td>101309.000000</td>\n",
       "      <td>11892.000000</td>\n",
       "      <td>3861.000000</td>\n",
       "      <td>130227.000000</td>\n",
       "      <td>478.00000</td>\n",
       "      <td>9417.000000</td>\n",
       "      <td>18249.000000</td>\n",
       "      <td>133565.000000</td>\n",
       "      <td>3549.000000</td>\n",
       "      <td>2469.000000</td>\n",
       "      <td>128317.000000</td>\n",
       "      <td>16806.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>7.000553</td>\n",
       "      <td>62.154296</td>\n",
       "      <td>32.560432</td>\n",
       "      <td>39.934071</td>\n",
       "      <td>23.438467</td>\n",
       "      <td>2.743194</td>\n",
       "      <td>36.853698</td>\n",
       "      <td>10.655362</td>\n",
       "      <td>23.494028</td>\n",
       "      <td>-1.129381</td>\n",
       "      <td>18.201614</td>\n",
       "      <td>263.741571</td>\n",
       "      <td>3.612015</td>\n",
       "      <td>11.638304</td>\n",
       "      <td>1.478575</td>\n",
       "      <td>41.122434</td>\n",
       "      <td>251.066658</td>\n",
       "      <td>0.553304</td>\n",
       "      <td>207.103660</td>\n",
       "      <td>92.981430</td>\n",
       "      <td>142.704627</td>\n",
       "      <td>81.858080</td>\n",
       "      <td>2.012246</td>\n",
       "      <td>4.153665</td>\n",
       "      <td>63.708425</td>\n",
       "      <td>7.158564</td>\n",
       "      <td>99.091557</td>\n",
       "      <td>97.668464</td>\n",
       "      <td>1.46046</td>\n",
       "      <td>106.321705</td>\n",
       "      <td>31.373145</td>\n",
       "      <td>84.525235</td>\n",
       "      <td>1.670710</td>\n",
       "      <td>7.410737</td>\n",
       "      <td>122.179033</td>\n",
       "      <td>7.369232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>4.436079</td>\n",
       "      <td>16.338174</td>\n",
       "      <td>7.656557</td>\n",
       "      <td>25.250865</td>\n",
       "      <td>19.992525</td>\n",
       "      <td>2.288238</td>\n",
       "      <td>0.875930</td>\n",
       "      <td>2.060924</td>\n",
       "      <td>4.268973</td>\n",
       "      <td>4.074558</td>\n",
       "      <td>5.099404</td>\n",
       "      <td>135.765163</td>\n",
       "      <td>1.363589</td>\n",
       "      <td>6.597006</td>\n",
       "      <td>1.833298</td>\n",
       "      <td>8.936093</td>\n",
       "      <td>912.050978</td>\n",
       "      <td>0.215522</td>\n",
       "      <td>105.003544</td>\n",
       "      <td>10.722283</td>\n",
       "      <td>58.320583</td>\n",
       "      <td>16.347929</td>\n",
       "      <td>0.436171</td>\n",
       "      <td>0.677888</td>\n",
       "      <td>13.790649</td>\n",
       "      <td>2.816791</td>\n",
       "      <td>111.488116</td>\n",
       "      <td>2.781903</td>\n",
       "      <td>2.83072</td>\n",
       "      <td>5.913163</td>\n",
       "      <td>5.759315</td>\n",
       "      <td>17.570528</td>\n",
       "      <td>3.297235</td>\n",
       "      <td>24.346532</td>\n",
       "      <td>23.057958</td>\n",
       "      <td>0.072396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.01000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>7.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>130.500000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.30000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>7.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>40.600000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.30000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.530000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>7.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>293.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>249.900000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>23.300000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>170.300000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9840.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1667.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>2121.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>21.00000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>71.700000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>46.400000</td>\n",
       "      <td>271.600000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>7.690000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            Age        EtCO2          PTT           BUN      Lactate          Temp           Hgb         HCO3    BaseExcess          RRate   Fibrinogen    Phosphate           WBC    Creatinine         PaCO2          AST          FiO2     Platelets         SaO2       Glucose           ABPm     Magnesium     Potassium           ABPd       Calcium  Alkalinephos           SpO2  Bilirubin_direct     Chloride           Hct      Heartrate  Bilirubin_total    TroponinI           ABPs            pH\n",
       "count  151968.000000  151968.000000  6528.000000  6853.000000  13448.000000  7276.000000  54155.000000  14872.000000  8422.000000  13238.000000  125299.000000  1655.000000  7686.000000  12753.000000  11805.000000  13881.000000  3908.000000  17799.000000  11873.000000  8670.000000  31072.000000  130672.000000  11827.000000  19015.000000  101309.000000  11892.000000   3861.000000  130227.000000         478.00000  9417.000000  18249.000000  133565.000000      3549.000000  2469.000000  128317.000000  16806.000000\n",
       "mean        7.000553      62.154296    32.560432    39.934071     23.438467     2.743194     36.853698     10.655362    23.494028     -1.129381      18.201614   263.741571     3.612015     11.638304      1.478575     41.122434   251.066658      0.553304    207.103660    92.981430    142.704627      81.858080      2.012246      4.153665      63.708425      7.158564     99.091557      97.668464           1.46046   106.321705     31.373145      84.525235         1.670710     7.410737     122.179033      7.369232\n",
       "std         4.436079      16.338174     7.656557    25.250865     19.992525     2.288238      0.875930      2.060924     4.268973      4.074558       5.099404   135.765163     1.363589      6.597006      1.833298      8.936093   912.050978      0.215522    105.003544    10.722283     58.320583      16.347929      0.436171      0.677888      13.790649      2.816791    111.488116       2.781903           2.83072     5.913163      5.759315      17.570528         3.297235    24.346532      23.057958      0.072396\n",
       "min         1.000000      16.000000    10.000000    18.100000      1.000000     0.300000     27.000000      2.300000     0.000000    -32.000000       1.000000    35.000000     0.300000      0.100000      0.100000     14.000000     5.000000      0.000000      5.000000    27.000000     14.000000      20.000000      0.200000      1.400000      20.000000      1.000000     11.000000      22.000000           0.01000    74.000000      9.100000      21.000000         0.100000     0.010000      28.000000      6.620000\n",
       "25%         4.000000      52.000000    28.000000    27.600000     12.000000     1.310000     36.000000      9.200000    21.000000     -3.000000      15.000000   174.000000     2.800000      7.700000      0.700000     36.000000    21.000000      0.400000    138.000000    95.000000    108.000000      70.000000      1.700000      3.700000      54.000000      7.300000     53.000000      96.000000           0.12000   103.000000     27.400000      72.000000         0.500000     0.030000     105.000000      7.330000\n",
       "50%         7.000000      64.000000    33.000000    32.000000     17.000000     2.020000     37.000000     10.500000    24.000000     -1.000000      18.000000   228.000000     3.400000     10.500000      0.920000     40.000000    36.000000      0.500000    193.000000    97.000000    130.500000      80.000000      2.000000      4.100000      62.000000      8.200000     72.000000      98.000000           0.30000   107.000000     31.000000      83.000000         0.800000     0.210000     119.000000      7.370000\n",
       "75%        10.000000      74.000000    37.000000    40.600000     27.000000     3.300000     37.000000     12.000000    26.000000      0.500000      21.000000   313.000000     4.200000     14.200000      1.380000     45.000000    88.000000      0.600000    256.000000    98.000000    160.000000      91.000000      2.200000      4.500000      71.500000      8.700000    103.000000     100.000000           1.30000   110.000000     35.000000      95.000000         1.400000     2.530000     136.000000      7.410000\n",
       "max       293.000000     100.000000   100.000000   249.900000    205.000000    23.300000     42.000000     23.800000    49.000000     36.000000      98.000000   976.000000    16.500000    170.300000     25.000000    100.000000  9840.000000     10.000000   1667.000000   100.000000    871.000000     291.000000      9.100000     10.600000     240.000000     22.200000   2121.000000     100.000000          21.00000   145.000000     71.700000     184.000000        46.400000   271.600000     281.000000      7.690000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(\"number of unique pid is {}\".format(np.size(test_features.pid.unique())))\n",
    "print(\"test_features has {} rows and {} columns\".format(test_features.shape[0],test_features.shape[1]))\n",
    "test_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first preprocessing step is to make sure the time feature is always between 1-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = train_features.copy(deep=True)\n",
    "features_train['Time'] = np.tile(np.arange(1,13),18995)\n",
    "features_test = test_features.copy(deep=True)\n",
    "features_test['Time'] = np.tile(np.arange(1,13),12664)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To suppress the outliers, we use the sigmoid standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_train.apply(lambda col: sigmoid((col - np.mean(col))/np.std(col)))\n",
    "features_test = features_test.apply(lambda col: sigmoid((col - np.mean(col))/np.std(col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main part of the preprocessing, we used the MICE approach. In summary, it fill each NA with the mean of the column, then, fits a linear model for each columns and updates one feature at a time. This process is repeated until all the features are filled and predicted using the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took the iterative imputer 36.030795097351074 seconds to impute the data.\n",
      "(227940, 36)\n",
      "(151968, 36)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"be1ec458-dcd3-44f1-b9ca-dad005d87b83\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"be1ec458-dcd3-44f1-b9ca-dad005d87b83\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "from fancyimpute import IterativeImputer\n",
    "imputer = IterativeImputer(max_iter=40, random_state=0,sample_posterior=False)\n",
    "\n",
    "# from sklearn.impute import KNNImputer\n",
    "# imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "\n",
    "features_train.iloc[:, :] = imputer.fit_transform(features_train)\n",
    "features_test.iloc[:, :] = imputer.fit_transform(features_test)\n",
    "# features_test.iloc[:, :] = imputer.transform(features_test)\n",
    "\n",
    "end = time.time()\n",
    "print('It took the iterative imputer '+str(end-start)+' seconds to impute the data.')\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have 12 observations for each patient, we need to find a way to represent them in one column. We use the groupby function to extract 4 features per patient: mean, min, max, and std, across the 12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"b74d0182-64bc-41a2-b507-33f3b276849e\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"b74d0182-64bc-41a2-b507-33f3b276849e\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# features_train = features_train.groupby(features_train.index).agg(['mean','median','min', 'max','std','skew'])\n",
    "# features_test = features_test.groupby(features_test.index).agg(['mean','median','min', 'max','std','skew'])\n",
    "\n",
    "features_train = features_train.groupby(features_train.index).agg(['mean','min', 'max','std'])\n",
    "features_test = features_test.groupby(features_test.index).agg(['mean','min', 'max','std'])\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Time</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Age</th>\n",
       "      <th colspan=\"4\" halign=\"left\">EtCO2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">PTT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">BUN</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Lactate</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Temp</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Hgb</th>\n",
       "      <th colspan=\"4\" halign=\"left\">HCO3</th>\n",
       "      <th colspan=\"4\" halign=\"left\">BaseExcess</th>\n",
       "      <th colspan=\"4\" halign=\"left\">RRate</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Fibrinogen</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Phosphate</th>\n",
       "      <th colspan=\"4\" halign=\"left\">WBC</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Creatinine</th>\n",
       "      <th colspan=\"4\" halign=\"left\">PaCO2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">AST</th>\n",
       "      <th colspan=\"4\" halign=\"left\">FiO2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Platelets</th>\n",
       "      <th colspan=\"4\" halign=\"left\">SaO2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Glucose</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ABPm</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Magnesium</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Potassium</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ABPd</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Calcium</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Alkalinephos</th>\n",
       "      <th colspan=\"4\" halign=\"left\">SpO2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Bilirubin_direct</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Chloride</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Hct</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Heartrate</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Bilirubin_total</th>\n",
       "      <th colspan=\"4\" halign=\"left\">TroponinI</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ABPs</th>\n",
       "      <th colspan=\"4\" halign=\"left\">pH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.153628</td>\n",
       "      <td>0.153628</td>\n",
       "      <td>0.153628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464818</td>\n",
       "      <td>0.272579</td>\n",
       "      <td>0.546370</td>\n",
       "      <td>0.095743</td>\n",
       "      <td>0.440218</td>\n",
       "      <td>0.409184</td>\n",
       "      <td>0.458961</td>\n",
       "      <td>0.013205</td>\n",
       "      <td>0.412475</td>\n",
       "      <td>0.363786</td>\n",
       "      <td>0.452410</td>\n",
       "      <td>0.031575</td>\n",
       "      <td>0.447518</td>\n",
       "      <td>0.376569</td>\n",
       "      <td>0.518252</td>\n",
       "      <td>0.037462</td>\n",
       "      <td>0.501891</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.787785</td>\n",
       "      <td>0.156063</td>\n",
       "      <td>0.354808</td>\n",
       "      <td>0.205992</td>\n",
       "      <td>0.474178</td>\n",
       "      <td>0.120035</td>\n",
       "      <td>0.521618</td>\n",
       "      <td>0.454448</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.061118</td>\n",
       "      <td>0.525546</td>\n",
       "      <td>0.454763</td>\n",
       "      <td>0.573364</td>\n",
       "      <td>0.043427</td>\n",
       "      <td>0.445878</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.492355</td>\n",
       "      <td>0.081339</td>\n",
       "      <td>0.485682</td>\n",
       "      <td>0.485335</td>\n",
       "      <td>0.485950</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.482964</td>\n",
       "      <td>0.376412</td>\n",
       "      <td>0.671128</td>\n",
       "      <td>0.073712</td>\n",
       "      <td>0.440520</td>\n",
       "      <td>0.332326</td>\n",
       "      <td>0.511461</td>\n",
       "      <td>0.061927</td>\n",
       "      <td>0.471507</td>\n",
       "      <td>0.371770</td>\n",
       "      <td>0.512788</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.494893</td>\n",
       "      <td>0.383303</td>\n",
       "      <td>0.607064</td>\n",
       "      <td>0.064884</td>\n",
       "      <td>0.477268</td>\n",
       "      <td>0.470184</td>\n",
       "      <td>0.487141</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.497756</td>\n",
       "      <td>0.496859</td>\n",
       "      <td>0.499804</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.460153</td>\n",
       "      <td>0.356159</td>\n",
       "      <td>0.523087</td>\n",
       "      <td>0.046816</td>\n",
       "      <td>0.523161</td>\n",
       "      <td>0.508163</td>\n",
       "      <td>0.538029</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.458605</td>\n",
       "      <td>0.326689</td>\n",
       "      <td>0.548305</td>\n",
       "      <td>0.055661</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>0.145828</td>\n",
       "      <td>0.528544</td>\n",
       "      <td>0.107002</td>\n",
       "      <td>0.453755</td>\n",
       "      <td>0.137169</td>\n",
       "      <td>0.554582</td>\n",
       "      <td>0.114038</td>\n",
       "      <td>0.473132</td>\n",
       "      <td>0.371371</td>\n",
       "      <td>0.515170</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.142213</td>\n",
       "      <td>0.472823</td>\n",
       "      <td>0.104873</td>\n",
       "      <td>0.519082</td>\n",
       "      <td>0.405560</td>\n",
       "      <td>0.614167</td>\n",
       "      <td>0.064441</td>\n",
       "      <td>0.481480</td>\n",
       "      <td>0.476152</td>\n",
       "      <td>0.483401</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476183</td>\n",
       "      <td>0.476050</td>\n",
       "      <td>0.476484</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.557425</td>\n",
       "      <td>0.459441</td>\n",
       "      <td>0.787231</td>\n",
       "      <td>0.103705</td>\n",
       "      <td>0.334398</td>\n",
       "      <td>0.171632</td>\n",
       "      <td>0.484948</td>\n",
       "      <td>0.142894</td>\n",
       "      <td>0.407229</td>\n",
       "      <td>0.181942</td>\n",
       "      <td>0.694357</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.479451</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>0.481496</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.475976</td>\n",
       "      <td>0.468003</td>\n",
       "      <td>0.480472</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.419030</td>\n",
       "      <td>0.276646</td>\n",
       "      <td>0.699190</td>\n",
       "      <td>0.122937</td>\n",
       "      <td>0.522881</td>\n",
       "      <td>0.377413</td>\n",
       "      <td>0.639912</td>\n",
       "      <td>0.080885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.810662</td>\n",
       "      <td>0.810662</td>\n",
       "      <td>0.810662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498896</td>\n",
       "      <td>0.492473</td>\n",
       "      <td>0.501837</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.503114</td>\n",
       "      <td>0.421045</td>\n",
       "      <td>0.529950</td>\n",
       "      <td>0.029538</td>\n",
       "      <td>0.530929</td>\n",
       "      <td>0.507269</td>\n",
       "      <td>0.608222</td>\n",
       "      <td>0.026451</td>\n",
       "      <td>0.438841</td>\n",
       "      <td>0.394413</td>\n",
       "      <td>0.462564</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>0.351549</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.425612</td>\n",
       "      <td>0.060649</td>\n",
       "      <td>0.525656</td>\n",
       "      <td>0.482694</td>\n",
       "      <td>0.766979</td>\n",
       "      <td>0.076679</td>\n",
       "      <td>0.541963</td>\n",
       "      <td>0.507023</td>\n",
       "      <td>0.558071</td>\n",
       "      <td>0.012792</td>\n",
       "      <td>0.520120</td>\n",
       "      <td>0.504687</td>\n",
       "      <td>0.531931</td>\n",
       "      <td>0.007498</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.761442</td>\n",
       "      <td>0.208567</td>\n",
       "      <td>0.485364</td>\n",
       "      <td>0.485044</td>\n",
       "      <td>0.486064</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.498541</td>\n",
       "      <td>0.475491</td>\n",
       "      <td>0.533804</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.454291</td>\n",
       "      <td>0.437655</td>\n",
       "      <td>0.498236</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.488836</td>\n",
       "      <td>0.471281</td>\n",
       "      <td>0.581485</td>\n",
       "      <td>0.029789</td>\n",
       "      <td>0.487479</td>\n",
       "      <td>0.460542</td>\n",
       "      <td>0.508856</td>\n",
       "      <td>0.014487</td>\n",
       "      <td>0.468481</td>\n",
       "      <td>0.462693</td>\n",
       "      <td>0.477109</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.498390</td>\n",
       "      <td>0.497629</td>\n",
       "      <td>0.499315</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.467127</td>\n",
       "      <td>0.425876</td>\n",
       "      <td>0.551029</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.489569</td>\n",
       "      <td>0.471731</td>\n",
       "      <td>0.514916</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.446066</td>\n",
       "      <td>0.289177</td>\n",
       "      <td>0.505111</td>\n",
       "      <td>0.053332</td>\n",
       "      <td>0.660011</td>\n",
       "      <td>0.480568</td>\n",
       "      <td>0.809985</td>\n",
       "      <td>0.097541</td>\n",
       "      <td>0.519734</td>\n",
       "      <td>0.469111</td>\n",
       "      <td>0.554892</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>0.462516</td>\n",
       "      <td>0.371371</td>\n",
       "      <td>0.492445</td>\n",
       "      <td>0.034203</td>\n",
       "      <td>0.589262</td>\n",
       "      <td>0.393627</td>\n",
       "      <td>0.745836</td>\n",
       "      <td>0.116805</td>\n",
       "      <td>0.546005</td>\n",
       "      <td>0.498525</td>\n",
       "      <td>0.696723</td>\n",
       "      <td>0.050287</td>\n",
       "      <td>0.485449</td>\n",
       "      <td>0.483957</td>\n",
       "      <td>0.489442</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.376077</td>\n",
       "      <td>0.211674</td>\n",
       "      <td>0.530162</td>\n",
       "      <td>0.118117</td>\n",
       "      <td>0.476094</td>\n",
       "      <td>0.475962</td>\n",
       "      <td>0.476236</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.479390</td>\n",
       "      <td>0.417144</td>\n",
       "      <td>0.535298</td>\n",
       "      <td>0.037317</td>\n",
       "      <td>0.549553</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.824235</td>\n",
       "      <td>0.087440</td>\n",
       "      <td>0.213610</td>\n",
       "      <td>0.190530</td>\n",
       "      <td>0.467490</td>\n",
       "      <td>0.079951</td>\n",
       "      <td>0.475706</td>\n",
       "      <td>0.474468</td>\n",
       "      <td>0.476290</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.473840</td>\n",
       "      <td>0.432580</td>\n",
       "      <td>0.477986</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>0.583753</td>\n",
       "      <td>0.474565</td>\n",
       "      <td>0.750493</td>\n",
       "      <td>0.084562</td>\n",
       "      <td>0.530889</td>\n",
       "      <td>0.503533</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>0.014458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.485053</td>\n",
       "      <td>0.518699</td>\n",
       "      <td>0.007318</td>\n",
       "      <td>0.472120</td>\n",
       "      <td>0.415774</td>\n",
       "      <td>0.501219</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>0.445237</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.060703</td>\n",
       "      <td>0.426810</td>\n",
       "      <td>0.309008</td>\n",
       "      <td>0.480781</td>\n",
       "      <td>0.046325</td>\n",
       "      <td>0.472332</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.542140</td>\n",
       "      <td>0.074589</td>\n",
       "      <td>0.479070</td>\n",
       "      <td>0.436696</td>\n",
       "      <td>0.544679</td>\n",
       "      <td>0.027424</td>\n",
       "      <td>0.516744</td>\n",
       "      <td>0.497099</td>\n",
       "      <td>0.555427</td>\n",
       "      <td>0.018310</td>\n",
       "      <td>0.505658</td>\n",
       "      <td>0.486997</td>\n",
       "      <td>0.529578</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.367182</td>\n",
       "      <td>0.098464</td>\n",
       "      <td>0.682121</td>\n",
       "      <td>0.192913</td>\n",
       "      <td>0.485495</td>\n",
       "      <td>0.484005</td>\n",
       "      <td>0.486049</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.462602</td>\n",
       "      <td>0.369492</td>\n",
       "      <td>0.495132</td>\n",
       "      <td>0.037450</td>\n",
       "      <td>0.470642</td>\n",
       "      <td>0.370709</td>\n",
       "      <td>0.497992</td>\n",
       "      <td>0.038633</td>\n",
       "      <td>0.460297</td>\n",
       "      <td>0.365639</td>\n",
       "      <td>0.488465</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.466494</td>\n",
       "      <td>0.368731</td>\n",
       "      <td>0.501439</td>\n",
       "      <td>0.039798</td>\n",
       "      <td>0.469613</td>\n",
       "      <td>0.441883</td>\n",
       "      <td>0.474504</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>0.498151</td>\n",
       "      <td>0.497386</td>\n",
       "      <td>0.499273</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.499049</td>\n",
       "      <td>0.445595</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>0.072383</td>\n",
       "      <td>0.539316</td>\n",
       "      <td>0.497943</td>\n",
       "      <td>0.671534</td>\n",
       "      <td>0.058414</td>\n",
       "      <td>0.435629</td>\n",
       "      <td>0.234943</td>\n",
       "      <td>0.514031</td>\n",
       "      <td>0.080165</td>\n",
       "      <td>0.433396</td>\n",
       "      <td>0.170008</td>\n",
       "      <td>0.993112</td>\n",
       "      <td>0.245171</td>\n",
       "      <td>0.518619</td>\n",
       "      <td>0.385358</td>\n",
       "      <td>0.924568</td>\n",
       "      <td>0.139232</td>\n",
       "      <td>0.453033</td>\n",
       "      <td>0.218841</td>\n",
       "      <td>0.528101</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>0.344407</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.911955</td>\n",
       "      <td>0.223064</td>\n",
       "      <td>0.525956</td>\n",
       "      <td>0.494467</td>\n",
       "      <td>0.608393</td>\n",
       "      <td>0.032708</td>\n",
       "      <td>0.490650</td>\n",
       "      <td>0.482949</td>\n",
       "      <td>0.565208</td>\n",
       "      <td>0.023490</td>\n",
       "      <td>0.628158</td>\n",
       "      <td>0.355022</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.119467</td>\n",
       "      <td>0.468680</td>\n",
       "      <td>0.386394</td>\n",
       "      <td>0.476281</td>\n",
       "      <td>0.025914</td>\n",
       "      <td>0.527433</td>\n",
       "      <td>0.467542</td>\n",
       "      <td>0.601095</td>\n",
       "      <td>0.038538</td>\n",
       "      <td>0.502668</td>\n",
       "      <td>0.463292</td>\n",
       "      <td>0.659591</td>\n",
       "      <td>0.054158</td>\n",
       "      <td>0.351526</td>\n",
       "      <td>0.281616</td>\n",
       "      <td>0.503742</td>\n",
       "      <td>0.062954</td>\n",
       "      <td>0.472170</td>\n",
       "      <td>0.420457</td>\n",
       "      <td>0.478117</td>\n",
       "      <td>0.016314</td>\n",
       "      <td>0.474083</td>\n",
       "      <td>0.428488</td>\n",
       "      <td>0.479640</td>\n",
       "      <td>0.014369</td>\n",
       "      <td>0.517766</td>\n",
       "      <td>0.173260</td>\n",
       "      <td>0.876596</td>\n",
       "      <td>0.264833</td>\n",
       "      <td>0.520186</td>\n",
       "      <td>0.500316</td>\n",
       "      <td>0.544056</td>\n",
       "      <td>0.012715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484403</td>\n",
       "      <td>0.170738</td>\n",
       "      <td>0.547809</td>\n",
       "      <td>0.101038</td>\n",
       "      <td>0.467940</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.628684</td>\n",
       "      <td>0.059121</td>\n",
       "      <td>0.490997</td>\n",
       "      <td>0.423022</td>\n",
       "      <td>0.608222</td>\n",
       "      <td>0.057685</td>\n",
       "      <td>0.422622</td>\n",
       "      <td>0.392597</td>\n",
       "      <td>0.451252</td>\n",
       "      <td>0.020263</td>\n",
       "      <td>0.590662</td>\n",
       "      <td>0.107516</td>\n",
       "      <td>0.787785</td>\n",
       "      <td>0.276957</td>\n",
       "      <td>0.462837</td>\n",
       "      <td>0.323759</td>\n",
       "      <td>0.523357</td>\n",
       "      <td>0.054193</td>\n",
       "      <td>0.437764</td>\n",
       "      <td>0.265186</td>\n",
       "      <td>0.522670</td>\n",
       "      <td>0.079799</td>\n",
       "      <td>0.442174</td>\n",
       "      <td>0.289668</td>\n",
       "      <td>0.516604</td>\n",
       "      <td>0.073043</td>\n",
       "      <td>0.398830</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.682121</td>\n",
       "      <td>0.206985</td>\n",
       "      <td>0.485656</td>\n",
       "      <td>0.485097</td>\n",
       "      <td>0.486428</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.479139</td>\n",
       "      <td>0.408490</td>\n",
       "      <td>0.565855</td>\n",
       "      <td>0.046375</td>\n",
       "      <td>0.548511</td>\n",
       "      <td>0.488275</td>\n",
       "      <td>0.941120</td>\n",
       "      <td>0.125394</td>\n",
       "      <td>0.473798</td>\n",
       "      <td>0.457259</td>\n",
       "      <td>0.488714</td>\n",
       "      <td>0.010838</td>\n",
       "      <td>0.464833</td>\n",
       "      <td>0.360571</td>\n",
       "      <td>0.538627</td>\n",
       "      <td>0.057525</td>\n",
       "      <td>0.480085</td>\n",
       "      <td>0.458646</td>\n",
       "      <td>0.494647</td>\n",
       "      <td>0.012098</td>\n",
       "      <td>0.498298</td>\n",
       "      <td>0.496925</td>\n",
       "      <td>0.503042</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.462477</td>\n",
       "      <td>0.277496</td>\n",
       "      <td>0.515157</td>\n",
       "      <td>0.063461</td>\n",
       "      <td>0.527223</td>\n",
       "      <td>0.201206</td>\n",
       "      <td>0.612611</td>\n",
       "      <td>0.120999</td>\n",
       "      <td>0.423775</td>\n",
       "      <td>0.196376</td>\n",
       "      <td>0.630208</td>\n",
       "      <td>0.131139</td>\n",
       "      <td>0.273306</td>\n",
       "      <td>0.207061</td>\n",
       "      <td>0.365051</td>\n",
       "      <td>0.054893</td>\n",
       "      <td>0.541320</td>\n",
       "      <td>0.470101</td>\n",
       "      <td>0.924568</td>\n",
       "      <td>0.123020</td>\n",
       "      <td>0.600600</td>\n",
       "      <td>0.443707</td>\n",
       "      <td>0.779764</td>\n",
       "      <td>0.104159</td>\n",
       "      <td>0.254133</td>\n",
       "      <td>0.170582</td>\n",
       "      <td>0.351675</td>\n",
       "      <td>0.046488</td>\n",
       "      <td>0.468186</td>\n",
       "      <td>0.324610</td>\n",
       "      <td>0.737910</td>\n",
       "      <td>0.112124</td>\n",
       "      <td>0.482998</td>\n",
       "      <td>0.476219</td>\n",
       "      <td>0.488149</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.641577</td>\n",
       "      <td>0.355022</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.111098</td>\n",
       "      <td>0.476189</td>\n",
       "      <td>0.476014</td>\n",
       "      <td>0.476669</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.583532</td>\n",
       "      <td>0.469282</td>\n",
       "      <td>0.838402</td>\n",
       "      <td>0.109787</td>\n",
       "      <td>0.461981</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>0.531020</td>\n",
       "      <td>0.049448</td>\n",
       "      <td>0.539535</td>\n",
       "      <td>0.464320</td>\n",
       "      <td>0.576998</td>\n",
       "      <td>0.047056</td>\n",
       "      <td>0.477716</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>0.479601</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.480426</td>\n",
       "      <td>0.477392</td>\n",
       "      <td>0.486785</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.284826</td>\n",
       "      <td>0.228119</td>\n",
       "      <td>0.370168</td>\n",
       "      <td>0.051299</td>\n",
       "      <td>0.472226</td>\n",
       "      <td>0.377413</td>\n",
       "      <td>0.575933</td>\n",
       "      <td>0.060024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.227909</td>\n",
       "      <td>0.227909</td>\n",
       "      <td>0.227909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500768</td>\n",
       "      <td>0.469843</td>\n",
       "      <td>0.504768</td>\n",
       "      <td>0.009763</td>\n",
       "      <td>0.453319</td>\n",
       "      <td>0.434423</td>\n",
       "      <td>0.488565</td>\n",
       "      <td>0.014075</td>\n",
       "      <td>0.423985</td>\n",
       "      <td>0.407995</td>\n",
       "      <td>0.445689</td>\n",
       "      <td>0.013476</td>\n",
       "      <td>0.456992</td>\n",
       "      <td>0.435333</td>\n",
       "      <td>0.492165</td>\n",
       "      <td>0.017132</td>\n",
       "      <td>0.425754</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.529462</td>\n",
       "      <td>0.104438</td>\n",
       "      <td>0.513763</td>\n",
       "      <td>0.492853</td>\n",
       "      <td>0.529744</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.514118</td>\n",
       "      <td>0.489425</td>\n",
       "      <td>0.536491</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>0.519696</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.531981</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.454382</td>\n",
       "      <td>0.348379</td>\n",
       "      <td>0.590608</td>\n",
       "      <td>0.071208</td>\n",
       "      <td>0.485535</td>\n",
       "      <td>0.485090</td>\n",
       "      <td>0.487022</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.474137</td>\n",
       "      <td>0.443803</td>\n",
       "      <td>0.500839</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.458956</td>\n",
       "      <td>0.429330</td>\n",
       "      <td>0.497671</td>\n",
       "      <td>0.017412</td>\n",
       "      <td>0.531348</td>\n",
       "      <td>0.479659</td>\n",
       "      <td>0.931847</td>\n",
       "      <td>0.126290</td>\n",
       "      <td>0.485870</td>\n",
       "      <td>0.465642</td>\n",
       "      <td>0.524451</td>\n",
       "      <td>0.017752</td>\n",
       "      <td>0.480296</td>\n",
       "      <td>0.473217</td>\n",
       "      <td>0.496829</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.498126</td>\n",
       "      <td>0.497135</td>\n",
       "      <td>0.499294</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.481968</td>\n",
       "      <td>0.433959</td>\n",
       "      <td>0.507755</td>\n",
       "      <td>0.019849</td>\n",
       "      <td>0.490897</td>\n",
       "      <td>0.460684</td>\n",
       "      <td>0.508265</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>0.487178</td>\n",
       "      <td>0.395365</td>\n",
       "      <td>0.522517</td>\n",
       "      <td>0.031638</td>\n",
       "      <td>0.934465</td>\n",
       "      <td>0.506804</td>\n",
       "      <td>1.029744</td>\n",
       "      <td>0.138651</td>\n",
       "      <td>0.467829</td>\n",
       "      <td>0.433625</td>\n",
       "      <td>0.554582</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.463321</td>\n",
       "      <td>0.409133</td>\n",
       "      <td>0.554719</td>\n",
       "      <td>0.038684</td>\n",
       "      <td>0.913880</td>\n",
       "      <td>0.531919</td>\n",
       "      <td>0.996324</td>\n",
       "      <td>0.125135</td>\n",
       "      <td>0.543814</td>\n",
       "      <td>0.506987</td>\n",
       "      <td>0.565312</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>0.482733</td>\n",
       "      <td>0.481643</td>\n",
       "      <td>0.486369</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.518426</td>\n",
       "      <td>0.211674</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.145144</td>\n",
       "      <td>0.475989</td>\n",
       "      <td>0.475675</td>\n",
       "      <td>0.476138</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.464081</td>\n",
       "      <td>0.326015</td>\n",
       "      <td>0.494605</td>\n",
       "      <td>0.046915</td>\n",
       "      <td>0.526579</td>\n",
       "      <td>0.497586</td>\n",
       "      <td>0.551578</td>\n",
       "      <td>0.014774</td>\n",
       "      <td>0.463252</td>\n",
       "      <td>0.293223</td>\n",
       "      <td>0.631156</td>\n",
       "      <td>0.108623</td>\n",
       "      <td>0.477341</td>\n",
       "      <td>0.474939</td>\n",
       "      <td>0.478745</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.473673</td>\n",
       "      <td>0.429072</td>\n",
       "      <td>0.478710</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.909817</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.993381</td>\n",
       "      <td>0.151726</td>\n",
       "      <td>0.522482</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.545138</td>\n",
       "      <td>0.018274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31653</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.351530</td>\n",
       "      <td>0.351530</td>\n",
       "      <td>0.351530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530564</td>\n",
       "      <td>0.502928</td>\n",
       "      <td>0.613011</td>\n",
       "      <td>0.039493</td>\n",
       "      <td>0.458933</td>\n",
       "      <td>0.366107</td>\n",
       "      <td>0.507467</td>\n",
       "      <td>0.043116</td>\n",
       "      <td>0.437160</td>\n",
       "      <td>0.352308</td>\n",
       "      <td>0.490429</td>\n",
       "      <td>0.054018</td>\n",
       "      <td>0.433319</td>\n",
       "      <td>0.344722</td>\n",
       "      <td>0.504392</td>\n",
       "      <td>0.057241</td>\n",
       "      <td>0.537929</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.697537</td>\n",
       "      <td>0.140801</td>\n",
       "      <td>0.461726</td>\n",
       "      <td>0.323759</td>\n",
       "      <td>0.547185</td>\n",
       "      <td>0.068979</td>\n",
       "      <td>0.496185</td>\n",
       "      <td>0.416313</td>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.045485</td>\n",
       "      <td>0.507326</td>\n",
       "      <td>0.466716</td>\n",
       "      <td>0.573364</td>\n",
       "      <td>0.034512</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.227626</td>\n",
       "      <td>0.795618</td>\n",
       "      <td>0.179597</td>\n",
       "      <td>0.523398</td>\n",
       "      <td>0.485062</td>\n",
       "      <td>0.941907</td>\n",
       "      <td>0.131796</td>\n",
       "      <td>0.513891</td>\n",
       "      <td>0.454548</td>\n",
       "      <td>0.638492</td>\n",
       "      <td>0.059938</td>\n",
       "      <td>0.539952</td>\n",
       "      <td>0.496564</td>\n",
       "      <td>0.786604</td>\n",
       "      <td>0.079029</td>\n",
       "      <td>0.463358</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>0.505789</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.525053</td>\n",
       "      <td>0.436910</td>\n",
       "      <td>0.707430</td>\n",
       "      <td>0.079338</td>\n",
       "      <td>0.474659</td>\n",
       "      <td>0.457682</td>\n",
       "      <td>0.486257</td>\n",
       "      <td>0.007370</td>\n",
       "      <td>0.498732</td>\n",
       "      <td>0.497915</td>\n",
       "      <td>0.499724</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.554457</td>\n",
       "      <td>0.450446</td>\n",
       "      <td>0.938771</td>\n",
       "      <td>0.140247</td>\n",
       "      <td>0.519445</td>\n",
       "      <td>0.472718</td>\n",
       "      <td>0.687629</td>\n",
       "      <td>0.057429</td>\n",
       "      <td>0.491875</td>\n",
       "      <td>0.458854</td>\n",
       "      <td>0.538351</td>\n",
       "      <td>0.026696</td>\n",
       "      <td>0.300015</td>\n",
       "      <td>0.145828</td>\n",
       "      <td>0.602968</td>\n",
       "      <td>0.140922</td>\n",
       "      <td>0.514424</td>\n",
       "      <td>0.385358</td>\n",
       "      <td>0.712032</td>\n",
       "      <td>0.098729</td>\n",
       "      <td>0.570294</td>\n",
       "      <td>0.476778</td>\n",
       "      <td>0.753073</td>\n",
       "      <td>0.106782</td>\n",
       "      <td>0.316607</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.623411</td>\n",
       "      <td>0.133395</td>\n",
       "      <td>0.540039</td>\n",
       "      <td>0.490282</td>\n",
       "      <td>0.707674</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.485016</td>\n",
       "      <td>0.482372</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.391467</td>\n",
       "      <td>0.157921</td>\n",
       "      <td>0.548748</td>\n",
       "      <td>0.107234</td>\n",
       "      <td>0.476051</td>\n",
       "      <td>0.475322</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.497446</td>\n",
       "      <td>0.426529</td>\n",
       "      <td>0.523965</td>\n",
       "      <td>0.026280</td>\n",
       "      <td>0.497266</td>\n",
       "      <td>0.423356</td>\n",
       "      <td>0.573841</td>\n",
       "      <td>0.051356</td>\n",
       "      <td>0.747019</td>\n",
       "      <td>0.533727</td>\n",
       "      <td>0.973346</td>\n",
       "      <td>0.114468</td>\n",
       "      <td>0.478778</td>\n",
       "      <td>0.474899</td>\n",
       "      <td>0.481133</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.479966</td>\n",
       "      <td>0.477994</td>\n",
       "      <td>0.483273</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.223517</td>\n",
       "      <td>0.103717</td>\n",
       "      <td>0.563547</td>\n",
       "      <td>0.147142</td>\n",
       "      <td>0.462176</td>\n",
       "      <td>0.377413</td>\n",
       "      <td>0.508241</td>\n",
       "      <td>0.049878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31654</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.559380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498741</td>\n",
       "      <td>0.475186</td>\n",
       "      <td>0.503526</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.471615</td>\n",
       "      <td>0.430151</td>\n",
       "      <td>0.515868</td>\n",
       "      <td>0.023350</td>\n",
       "      <td>0.498309</td>\n",
       "      <td>0.447263</td>\n",
       "      <td>0.631748</td>\n",
       "      <td>0.058129</td>\n",
       "      <td>0.455345</td>\n",
       "      <td>0.435512</td>\n",
       "      <td>0.473628</td>\n",
       "      <td>0.010473</td>\n",
       "      <td>0.492397</td>\n",
       "      <td>0.326609</td>\n",
       "      <td>0.787785</td>\n",
       "      <td>0.122049</td>\n",
       "      <td>0.516397</td>\n",
       "      <td>0.487074</td>\n",
       "      <td>0.568464</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>0.521803</td>\n",
       "      <td>0.496321</td>\n",
       "      <td>0.549620</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.520649</td>\n",
       "      <td>0.504951</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>0.008779</td>\n",
       "      <td>0.445079</td>\n",
       "      <td>0.264398</td>\n",
       "      <td>0.590608</td>\n",
       "      <td>0.108426</td>\n",
       "      <td>0.485350</td>\n",
       "      <td>0.484981</td>\n",
       "      <td>0.486091</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.524292</td>\n",
       "      <td>0.439255</td>\n",
       "      <td>0.818743</td>\n",
       "      <td>0.125079</td>\n",
       "      <td>0.457749</td>\n",
       "      <td>0.275816</td>\n",
       "      <td>0.499840</td>\n",
       "      <td>0.060409</td>\n",
       "      <td>0.567819</td>\n",
       "      <td>0.476411</td>\n",
       "      <td>0.990908</td>\n",
       "      <td>0.197510</td>\n",
       "      <td>0.501331</td>\n",
       "      <td>0.475067</td>\n",
       "      <td>0.549755</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.481625</td>\n",
       "      <td>0.472596</td>\n",
       "      <td>0.497956</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>0.498258</td>\n",
       "      <td>0.496810</td>\n",
       "      <td>0.499273</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.464727</td>\n",
       "      <td>0.315507</td>\n",
       "      <td>0.499090</td>\n",
       "      <td>0.048810</td>\n",
       "      <td>0.483750</td>\n",
       "      <td>0.439239</td>\n",
       "      <td>0.516446</td>\n",
       "      <td>0.020247</td>\n",
       "      <td>0.465746</td>\n",
       "      <td>0.207708</td>\n",
       "      <td>0.522186</td>\n",
       "      <td>0.091948</td>\n",
       "      <td>0.876559</td>\n",
       "      <td>0.491881</td>\n",
       "      <td>0.963428</td>\n",
       "      <td>0.127826</td>\n",
       "      <td>0.477389</td>\n",
       "      <td>0.438466</td>\n",
       "      <td>0.558303</td>\n",
       "      <td>0.042348</td>\n",
       "      <td>0.549791</td>\n",
       "      <td>0.443159</td>\n",
       "      <td>0.921148</td>\n",
       "      <td>0.169257</td>\n",
       "      <td>0.853622</td>\n",
       "      <td>0.477301</td>\n",
       "      <td>0.948261</td>\n",
       "      <td>0.128991</td>\n",
       "      <td>0.542529</td>\n",
       "      <td>0.502174</td>\n",
       "      <td>0.625201</td>\n",
       "      <td>0.036556</td>\n",
       "      <td>0.483616</td>\n",
       "      <td>0.482254</td>\n",
       "      <td>0.485905</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.384348</td>\n",
       "      <td>0.211674</td>\n",
       "      <td>0.617679</td>\n",
       "      <td>0.131758</td>\n",
       "      <td>0.476067</td>\n",
       "      <td>0.475802</td>\n",
       "      <td>0.476188</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.439093</td>\n",
       "      <td>0.260592</td>\n",
       "      <td>0.496122</td>\n",
       "      <td>0.072281</td>\n",
       "      <td>0.540481</td>\n",
       "      <td>0.497378</td>\n",
       "      <td>0.639875</td>\n",
       "      <td>0.036153</td>\n",
       "      <td>0.595159</td>\n",
       "      <td>0.381536</td>\n",
       "      <td>0.849083</td>\n",
       "      <td>0.138362</td>\n",
       "      <td>0.476321</td>\n",
       "      <td>0.475132</td>\n",
       "      <td>0.477345</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.477929</td>\n",
       "      <td>0.477314</td>\n",
       "      <td>0.479937</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.823465</td>\n",
       "      <td>0.494520</td>\n",
       "      <td>0.941418</td>\n",
       "      <td>0.115341</td>\n",
       "      <td>0.509145</td>\n",
       "      <td>0.424130</td>\n",
       "      <td>0.539925</td>\n",
       "      <td>0.038428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31656</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.506378</td>\n",
       "      <td>0.445989</td>\n",
       "      <td>0.598757</td>\n",
       "      <td>0.033602</td>\n",
       "      <td>0.473883</td>\n",
       "      <td>0.435279</td>\n",
       "      <td>0.508438</td>\n",
       "      <td>0.021035</td>\n",
       "      <td>0.441624</td>\n",
       "      <td>0.399117</td>\n",
       "      <td>0.461569</td>\n",
       "      <td>0.018704</td>\n",
       "      <td>0.480386</td>\n",
       "      <td>0.437717</td>\n",
       "      <td>0.526415</td>\n",
       "      <td>0.028049</td>\n",
       "      <td>0.604201</td>\n",
       "      <td>0.436698</td>\n",
       "      <td>0.787785</td>\n",
       "      <td>0.125688</td>\n",
       "      <td>0.502576</td>\n",
       "      <td>0.469908</td>\n",
       "      <td>0.701396</td>\n",
       "      <td>0.063194</td>\n",
       "      <td>0.476061</td>\n",
       "      <td>0.410209</td>\n",
       "      <td>0.529029</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.454097</td>\n",
       "      <td>0.289668</td>\n",
       "      <td>0.512165</td>\n",
       "      <td>0.078931</td>\n",
       "      <td>0.736322</td>\n",
       "      <td>0.471896</td>\n",
       "      <td>0.875960</td>\n",
       "      <td>0.131350</td>\n",
       "      <td>0.485853</td>\n",
       "      <td>0.485480</td>\n",
       "      <td>0.486149</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.478891</td>\n",
       "      <td>0.430263</td>\n",
       "      <td>0.544313</td>\n",
       "      <td>0.038891</td>\n",
       "      <td>0.525747</td>\n",
       "      <td>0.473790</td>\n",
       "      <td>0.771247</td>\n",
       "      <td>0.079346</td>\n",
       "      <td>0.473791</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>0.498930</td>\n",
       "      <td>0.026132</td>\n",
       "      <td>0.520463</td>\n",
       "      <td>0.449419</td>\n",
       "      <td>0.707430</td>\n",
       "      <td>0.077320</td>\n",
       "      <td>0.484434</td>\n",
       "      <td>0.472940</td>\n",
       "      <td>0.491337</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.497907</td>\n",
       "      <td>0.496925</td>\n",
       "      <td>0.499358</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.557975</td>\n",
       "      <td>0.481715</td>\n",
       "      <td>0.793700</td>\n",
       "      <td>0.082082</td>\n",
       "      <td>0.513280</td>\n",
       "      <td>0.496444</td>\n",
       "      <td>0.524183</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.477545</td>\n",
       "      <td>0.382832</td>\n",
       "      <td>0.524185</td>\n",
       "      <td>0.037189</td>\n",
       "      <td>0.458287</td>\n",
       "      <td>0.310808</td>\n",
       "      <td>0.659415</td>\n",
       "      <td>0.115575</td>\n",
       "      <td>0.471997</td>\n",
       "      <td>0.424918</td>\n",
       "      <td>0.513364</td>\n",
       "      <td>0.029779</td>\n",
       "      <td>0.525274</td>\n",
       "      <td>0.443270</td>\n",
       "      <td>0.565854</td>\n",
       "      <td>0.037611</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.343529</td>\n",
       "      <td>0.745836</td>\n",
       "      <td>0.125276</td>\n",
       "      <td>0.485679</td>\n",
       "      <td>0.295909</td>\n",
       "      <td>0.539289</td>\n",
       "      <td>0.070027</td>\n",
       "      <td>0.483917</td>\n",
       "      <td>0.482298</td>\n",
       "      <td>0.488520</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.669620</td>\n",
       "      <td>0.500204</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.067650</td>\n",
       "      <td>0.475983</td>\n",
       "      <td>0.475639</td>\n",
       "      <td>0.476083</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.466061</td>\n",
       "      <td>0.172886</td>\n",
       "      <td>0.537049</td>\n",
       "      <td>0.094162</td>\n",
       "      <td>0.514455</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.718848</td>\n",
       "      <td>0.065139</td>\n",
       "      <td>0.663620</td>\n",
       "      <td>0.519245</td>\n",
       "      <td>0.833975</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>0.477598</td>\n",
       "      <td>0.472998</td>\n",
       "      <td>0.480590</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.480586</td>\n",
       "      <td>0.478291</td>\n",
       "      <td>0.489595</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.318955</td>\n",
       "      <td>0.185917</td>\n",
       "      <td>0.476114</td>\n",
       "      <td>0.115935</td>\n",
       "      <td>0.445156</td>\n",
       "      <td>0.171353</td>\n",
       "      <td>0.542945</td>\n",
       "      <td>0.124002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31657</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.618169</td>\n",
       "      <td>0.618169</td>\n",
       "      <td>0.618169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499284</td>\n",
       "      <td>0.497286</td>\n",
       "      <td>0.502805</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.473134</td>\n",
       "      <td>0.445363</td>\n",
       "      <td>0.490015</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.496487</td>\n",
       "      <td>0.487685</td>\n",
       "      <td>0.504213</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.438443</td>\n",
       "      <td>0.415819</td>\n",
       "      <td>0.463473</td>\n",
       "      <td>0.012886</td>\n",
       "      <td>0.424928</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.542140</td>\n",
       "      <td>0.075437</td>\n",
       "      <td>0.489251</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.499698</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.506467</td>\n",
       "      <td>0.496752</td>\n",
       "      <td>0.515640</td>\n",
       "      <td>0.006141</td>\n",
       "      <td>0.502226</td>\n",
       "      <td>0.496823</td>\n",
       "      <td>0.509247</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.410873</td>\n",
       "      <td>0.304766</td>\n",
       "      <td>0.499192</td>\n",
       "      <td>0.066366</td>\n",
       "      <td>0.485364</td>\n",
       "      <td>0.485187</td>\n",
       "      <td>0.485457</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.490679</td>\n",
       "      <td>0.474672</td>\n",
       "      <td>0.501996</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.465658</td>\n",
       "      <td>0.498049</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.472978</td>\n",
       "      <td>0.469605</td>\n",
       "      <td>0.477590</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>0.476912</td>\n",
       "      <td>0.460283</td>\n",
       "      <td>0.500859</td>\n",
       "      <td>0.011275</td>\n",
       "      <td>0.472536</td>\n",
       "      <td>0.469386</td>\n",
       "      <td>0.475197</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.498190</td>\n",
       "      <td>0.497281</td>\n",
       "      <td>0.499283</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.461802</td>\n",
       "      <td>0.447647</td>\n",
       "      <td>0.488877</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>0.514661</td>\n",
       "      <td>0.503955</td>\n",
       "      <td>0.523242</td>\n",
       "      <td>0.006709</td>\n",
       "      <td>0.457051</td>\n",
       "      <td>0.426340</td>\n",
       "      <td>0.505689</td>\n",
       "      <td>0.023820</td>\n",
       "      <td>0.394676</td>\n",
       "      <td>0.297955</td>\n",
       "      <td>0.498220</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>0.514431</td>\n",
       "      <td>0.459952</td>\n",
       "      <td>0.555934</td>\n",
       "      <td>0.029277</td>\n",
       "      <td>0.501544</td>\n",
       "      <td>0.477826</td>\n",
       "      <td>0.531418</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.477613</td>\n",
       "      <td>0.359907</td>\n",
       "      <td>0.622886</td>\n",
       "      <td>0.080820</td>\n",
       "      <td>0.527149</td>\n",
       "      <td>0.501320</td>\n",
       "      <td>0.544239</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>0.483524</td>\n",
       "      <td>0.483039</td>\n",
       "      <td>0.484223</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.606716</td>\n",
       "      <td>0.501051</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.069766</td>\n",
       "      <td>0.476157</td>\n",
       "      <td>0.476123</td>\n",
       "      <td>0.476207</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.523686</td>\n",
       "      <td>0.496022</td>\n",
       "      <td>0.547397</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.496898</td>\n",
       "      <td>0.485169</td>\n",
       "      <td>0.507035</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.283805</td>\n",
       "      <td>0.208629</td>\n",
       "      <td>0.495261</td>\n",
       "      <td>0.099118</td>\n",
       "      <td>0.476044</td>\n",
       "      <td>0.475271</td>\n",
       "      <td>0.476956</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.477672</td>\n",
       "      <td>0.477342</td>\n",
       "      <td>0.478093</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.285326</td>\n",
       "      <td>0.517503</td>\n",
       "      <td>0.079141</td>\n",
       "      <td>0.520004</td>\n",
       "      <td>0.501069</td>\n",
       "      <td>0.532965</td>\n",
       "      <td>0.008492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31658</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.168926</td>\n",
       "      <td>0.831074</td>\n",
       "      <td>0.229294</td>\n",
       "      <td>0.468528</td>\n",
       "      <td>0.468528</td>\n",
       "      <td>0.468528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500728</td>\n",
       "      <td>0.490447</td>\n",
       "      <td>0.503558</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.480007</td>\n",
       "      <td>0.459758</td>\n",
       "      <td>0.511557</td>\n",
       "      <td>0.016446</td>\n",
       "      <td>0.461518</td>\n",
       "      <td>0.375421</td>\n",
       "      <td>0.483828</td>\n",
       "      <td>0.028717</td>\n",
       "      <td>0.449522</td>\n",
       "      <td>0.425075</td>\n",
       "      <td>0.464179</td>\n",
       "      <td>0.012561</td>\n",
       "      <td>0.455602</td>\n",
       "      <td>0.274142</td>\n",
       "      <td>0.548527</td>\n",
       "      <td>0.094636</td>\n",
       "      <td>0.530084</td>\n",
       "      <td>0.484684</td>\n",
       "      <td>0.860313</td>\n",
       "      <td>0.104748</td>\n",
       "      <td>0.523122</td>\n",
       "      <td>0.496691</td>\n",
       "      <td>0.554867</td>\n",
       "      <td>0.019843</td>\n",
       "      <td>0.517831</td>\n",
       "      <td>0.503671</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.017115</td>\n",
       "      <td>0.487449</td>\n",
       "      <td>0.457343</td>\n",
       "      <td>0.515717</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.485406</td>\n",
       "      <td>0.485235</td>\n",
       "      <td>0.485647</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.476892</td>\n",
       "      <td>0.408092</td>\n",
       "      <td>0.501402</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.460498</td>\n",
       "      <td>0.330130</td>\n",
       "      <td>0.497904</td>\n",
       "      <td>0.045633</td>\n",
       "      <td>0.474484</td>\n",
       "      <td>0.397956</td>\n",
       "      <td>0.489831</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.494047</td>\n",
       "      <td>0.476384</td>\n",
       "      <td>0.523471</td>\n",
       "      <td>0.014618</td>\n",
       "      <td>0.475676</td>\n",
       "      <td>0.468441</td>\n",
       "      <td>0.483463</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.498338</td>\n",
       "      <td>0.497486</td>\n",
       "      <td>0.499257</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.473970</td>\n",
       "      <td>0.403610</td>\n",
       "      <td>0.496116</td>\n",
       "      <td>0.023739</td>\n",
       "      <td>0.494183</td>\n",
       "      <td>0.476025</td>\n",
       "      <td>0.508390</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>0.453649</td>\n",
       "      <td>0.315108</td>\n",
       "      <td>0.506138</td>\n",
       "      <td>0.057606</td>\n",
       "      <td>0.600489</td>\n",
       "      <td>0.493274</td>\n",
       "      <td>0.827974</td>\n",
       "      <td>0.126829</td>\n",
       "      <td>0.487608</td>\n",
       "      <td>0.440735</td>\n",
       "      <td>0.525500</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>0.467285</td>\n",
       "      <td>0.304751</td>\n",
       "      <td>0.504237</td>\n",
       "      <td>0.055772</td>\n",
       "      <td>0.582471</td>\n",
       "      <td>0.482382</td>\n",
       "      <td>0.772099</td>\n",
       "      <td>0.108316</td>\n",
       "      <td>0.540054</td>\n",
       "      <td>0.503493</td>\n",
       "      <td>0.641713</td>\n",
       "      <td>0.035379</td>\n",
       "      <td>0.483670</td>\n",
       "      <td>0.483173</td>\n",
       "      <td>0.484119</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.433838</td>\n",
       "      <td>0.355022</td>\n",
       "      <td>0.543286</td>\n",
       "      <td>0.084295</td>\n",
       "      <td>0.476064</td>\n",
       "      <td>0.475992</td>\n",
       "      <td>0.476171</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.487231</td>\n",
       "      <td>0.467144</td>\n",
       "      <td>0.519119</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.538715</td>\n",
       "      <td>0.494463</td>\n",
       "      <td>0.824235</td>\n",
       "      <td>0.091311</td>\n",
       "      <td>0.385811</td>\n",
       "      <td>0.218141</td>\n",
       "      <td>0.516849</td>\n",
       "      <td>0.115281</td>\n",
       "      <td>0.477284</td>\n",
       "      <td>0.476901</td>\n",
       "      <td>0.477686</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.478117</td>\n",
       "      <td>0.477678</td>\n",
       "      <td>0.478812</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.550551</td>\n",
       "      <td>0.488565</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.071542</td>\n",
       "      <td>0.523901</td>\n",
       "      <td>0.499153</td>\n",
       "      <td>0.566698</td>\n",
       "      <td>0.020814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18995 rows × 144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Time                                     Age                              EtCO2                                     PTT                                     BUN                                 Lactate                                    Temp                                     Hgb                                    HCO3                               BaseExcess                                   RRate                               Fibrinogen                               Phosphate                                     WBC                               Creatinine                                   PaCO2                                     AST                                    FiO2                               Platelets                                    SaO2                                 Glucose                                    ABPm                               Magnesium                               Potassium                                    ABPd                      \\\n",
       "      mean       min       max       std      mean       min       max  std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std       mean       min       max       std      mean       min       max       std       mean       min       max       std      mean       min       max       std      mean       min       max       std       mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max   \n",
       "pid                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1      0.5  0.168926  0.831074  0.229294  0.153628  0.153628  0.153628  0.0  0.464818  0.272579  0.546370  0.095743  0.440218  0.409184  0.458961  0.013205  0.412475  0.363786  0.452410  0.031575  0.447518  0.376569  0.518252  0.037462  0.501891  0.274142  0.787785  0.156063  0.354808  0.205992  0.474178  0.120035  0.521618  0.454448  0.638860  0.061118   0.525546  0.454763  0.573364  0.043427  0.445878  0.227626  0.492355  0.081339   0.485682  0.485335  0.485950  0.000143  0.482964  0.376412  0.671128  0.073712  0.440520  0.332326  0.511461  0.061927   0.471507  0.371770  0.512788  0.038164  0.494893  0.383303  0.607064  0.064884  0.477268  0.470184  0.487141  0.005355  0.497756  0.496859  0.499804  0.000881  0.460153  0.356159  0.523087  0.046816  0.523161  0.508163  0.538029  0.010930  0.458605  0.326689  0.548305  0.055661  0.310796  0.145828  0.528544  0.107002  0.453755  0.137169  0.554582  0.114038  0.473132  0.371371  0.515170  0.037873  0.280900  0.142213  0.472823   \n",
       "2      0.5  0.168926  0.831074  0.229294  0.810662  0.810662  0.810662  0.0  0.498896  0.492473  0.501837  0.002396  0.503114  0.421045  0.529950  0.029538  0.530929  0.507269  0.608222  0.026451  0.438841  0.394413  0.462564  0.019310  0.351549  0.274142  0.425612  0.060649  0.525656  0.482694  0.766979  0.076679  0.541963  0.507023  0.558071  0.012792   0.520120  0.504687  0.531931  0.007498  0.496372  0.227626  0.761442  0.208567   0.485364  0.485044  0.486064  0.000280  0.498541  0.475491  0.533804  0.016508  0.454291  0.437655  0.498236  0.016766   0.488836  0.471281  0.581485  0.029789  0.487479  0.460542  0.508856  0.014487  0.468481  0.462693  0.477109  0.003643  0.498390  0.497629  0.499315  0.000676  0.467127  0.425876  0.551029  0.032943  0.489569  0.471731  0.514916  0.012186  0.446066  0.289177  0.505111  0.053332  0.660011  0.480568  0.809985  0.097541  0.519734  0.469111  0.554892  0.030512  0.462516  0.371371  0.492445  0.034203  0.589262  0.393627  0.745836   \n",
       "4      0.5  0.168926  0.831074  0.229294  0.559380  0.559380  0.559380  0.0  0.501100  0.485053  0.518699  0.007318  0.472120  0.415774  0.501219  0.025274  0.445237  0.318922  0.490347  0.060703  0.426810  0.309008  0.480781  0.046325  0.472332  0.274142  0.542140  0.074589  0.479070  0.436696  0.544679  0.027424  0.516744  0.497099  0.555427  0.018310   0.505658  0.486997  0.529578  0.011761  0.367182  0.098464  0.682121  0.192913   0.485495  0.484005  0.486049  0.000522  0.462602  0.369492  0.495132  0.037450  0.470642  0.370709  0.497992  0.038633   0.460297  0.365639  0.488465  0.040650  0.466494  0.368731  0.501439  0.039798  0.469613  0.441883  0.474504  0.009178  0.498151  0.497386  0.499273  0.000542  0.499049  0.445595  0.671217  0.072383  0.539316  0.497943  0.671534  0.058414  0.435629  0.234943  0.514031  0.080165  0.433396  0.170008  0.993112  0.245171  0.518619  0.385358  0.924568  0.139232  0.453033  0.218841  0.528101  0.090066  0.344407  0.151204  0.911955   \n",
       "6      0.5  0.168926  0.831074  0.229294  0.559380  0.559380  0.559380  0.0  0.484403  0.170738  0.547809  0.101038  0.467940  0.386897  0.628684  0.059121  0.490997  0.423022  0.608222  0.057685  0.422622  0.392597  0.451252  0.020263  0.590662  0.107516  0.787785  0.276957  0.462837  0.323759  0.523357  0.054193  0.437764  0.265186  0.522670  0.079799   0.442174  0.289668  0.516604  0.073043  0.398830  0.227626  0.682121  0.206985   0.485656  0.485097  0.486428  0.000347  0.479139  0.408490  0.565855  0.046375  0.548511  0.488275  0.941120  0.125394   0.473798  0.457259  0.488714  0.010838  0.464833  0.360571  0.538627  0.057525  0.480085  0.458646  0.494647  0.012098  0.498298  0.496925  0.503042  0.001650  0.462477  0.277496  0.515157  0.063461  0.527223  0.201206  0.612611  0.120999  0.423775  0.196376  0.630208  0.131139  0.273306  0.207061  0.365051  0.054893  0.541320  0.470101  0.924568  0.123020  0.600600  0.443707  0.779764  0.104159  0.254133  0.170582  0.351675   \n",
       "8      0.5  0.168926  0.831074  0.229294  0.227909  0.227909  0.227909  0.0  0.500768  0.469843  0.504768  0.009763  0.453319  0.434423  0.488565  0.014075  0.423985  0.407995  0.445689  0.013476  0.456992  0.435333  0.492165  0.017132  0.425754  0.274142  0.529462  0.104438  0.513763  0.492853  0.529744  0.009960  0.514118  0.489425  0.536491  0.015540   0.519696  0.505300  0.531981  0.008322  0.454382  0.348379  0.590608  0.071208   0.485535  0.485090  0.487022  0.000495  0.474137  0.443803  0.500839  0.015197  0.458956  0.429330  0.497671  0.017412   0.531348  0.479659  0.931847  0.126290  0.485870  0.465642  0.524451  0.017752  0.480296  0.473217  0.496829  0.006400  0.498126  0.497135  0.499294  0.000705  0.481968  0.433959  0.507755  0.019849  0.490897  0.460684  0.508265  0.015432  0.487178  0.395365  0.522517  0.031638  0.934465  0.506804  1.029744  0.138651  0.467829  0.433625  0.554582  0.034667  0.463321  0.409133  0.554719  0.038684  0.913880  0.531919  0.996324   \n",
       "...    ...       ...       ...       ...       ...       ...       ...  ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...        ...       ...       ...       ...       ...       ...       ...       ...        ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...        ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...   \n",
       "31653  0.5  0.168926  0.831074  0.229294  0.351530  0.351530  0.351530  0.0  0.530564  0.502928  0.613011  0.039493  0.458933  0.366107  0.507467  0.043116  0.437160  0.352308  0.490429  0.054018  0.433319  0.344722  0.504392  0.057241  0.537929  0.274142  0.697537  0.140801  0.461726  0.323759  0.547185  0.068979  0.496185  0.416313  0.592421  0.045485   0.507326  0.466716  0.573364  0.034512  0.388600  0.227626  0.795618  0.179597   0.523398  0.485062  0.941907  0.131796  0.513891  0.454548  0.638492  0.059938  0.539952  0.496564  0.786604  0.079029   0.463358  0.396694  0.505789  0.026470  0.525053  0.436910  0.707430  0.079338  0.474659  0.457682  0.486257  0.007370  0.498732  0.497915  0.499724  0.000665  0.554457  0.450446  0.938771  0.140247  0.519445  0.472718  0.687629  0.057429  0.491875  0.458854  0.538351  0.026696  0.300015  0.145828  0.602968  0.140922  0.514424  0.385358  0.712032  0.098729  0.570294  0.476778  0.753073  0.106782  0.316607  0.151204  0.623411   \n",
       "31654  0.5  0.168926  0.831074  0.229294  0.559380  0.559380  0.559380  0.0  0.498741  0.475186  0.503526  0.009549  0.471615  0.430151  0.515868  0.023350  0.498309  0.447263  0.631748  0.058129  0.455345  0.435512  0.473628  0.010473  0.492397  0.326609  0.787785  0.122049  0.516397  0.487074  0.568464  0.020361  0.521803  0.496321  0.549620  0.017319   0.520649  0.504951  0.536160  0.008779  0.445079  0.264398  0.590608  0.108426   0.485350  0.484981  0.486091  0.000312  0.524292  0.439255  0.818743  0.125079  0.457749  0.275816  0.499840  0.060409   0.567819  0.476411  0.990908  0.197510  0.501331  0.475067  0.549755  0.021270  0.481625  0.472596  0.497956  0.008275  0.498258  0.496810  0.499273  0.000786  0.464727  0.315507  0.499090  0.048810  0.483750  0.439239  0.516446  0.020247  0.465746  0.207708  0.522186  0.091948  0.876559  0.491881  0.963428  0.127826  0.477389  0.438466  0.558303  0.042348  0.549791  0.443159  0.921148  0.169257  0.853622  0.477301  0.948261   \n",
       "31656  0.5  0.168926  0.831074  0.229294  0.250004  0.250004  0.250004  0.0  0.506378  0.445989  0.598757  0.033602  0.473883  0.435279  0.508438  0.021035  0.441624  0.399117  0.461569  0.018704  0.480386  0.437717  0.526415  0.028049  0.604201  0.436698  0.787785  0.125688  0.502576  0.469908  0.701396  0.063194  0.476061  0.410209  0.529029  0.030928   0.454097  0.289668  0.512165  0.078931  0.736322  0.471896  0.875960  0.131350   0.485853  0.485480  0.486149  0.000195  0.478891  0.430263  0.544313  0.038891  0.525747  0.473790  0.771247  0.079346   0.473791  0.396694  0.498930  0.026132  0.520463  0.449419  0.707430  0.077320  0.484434  0.472940  0.491337  0.005445  0.497907  0.496925  0.499358  0.000872  0.557975  0.481715  0.793700  0.082082  0.513280  0.496444  0.524183  0.009021  0.477545  0.382832  0.524185  0.037189  0.458287  0.310808  0.659415  0.115575  0.471997  0.424918  0.513364  0.029779  0.525274  0.443270  0.565854  0.037611  0.500600  0.343529  0.745836   \n",
       "31657  0.5  0.168926  0.831074  0.229294  0.618169  0.618169  0.618169  0.0  0.499284  0.497286  0.502805  0.001583  0.473134  0.445363  0.490015  0.012419  0.496487  0.487685  0.504213  0.004702  0.438443  0.415819  0.463473  0.012886  0.424928  0.274142  0.542140  0.075437  0.489251  0.480000  0.499698  0.006729  0.506467  0.496752  0.515640  0.006141   0.502226  0.496823  0.509247  0.003096  0.410873  0.304766  0.499192  0.066366   0.485364  0.485187  0.485457  0.000081  0.490679  0.474672  0.501996  0.008609  0.474300  0.465658  0.498049  0.009840   0.472978  0.469605  0.477590  0.002372  0.476912  0.460283  0.500859  0.011275  0.472536  0.469386  0.475197  0.001694  0.498190  0.497281  0.499283  0.000638  0.461802  0.447647  0.488877  0.011720  0.514661  0.503955  0.523242  0.006709  0.457051  0.426340  0.505689  0.023820  0.394676  0.297955  0.498220  0.080257  0.514431  0.459952  0.555934  0.029277  0.501544  0.477826  0.531418  0.017143  0.477613  0.359907  0.622886   \n",
       "31658  0.5  0.168926  0.831074  0.229294  0.468528  0.468528  0.468528  0.0  0.500728  0.490447  0.503558  0.003409  0.480007  0.459758  0.511557  0.016446  0.461518  0.375421  0.483828  0.028717  0.449522  0.425075  0.464179  0.012561  0.455602  0.274142  0.548527  0.094636  0.530084  0.484684  0.860313  0.104748  0.523122  0.496691  0.554867  0.019843   0.517831  0.503671  0.564800  0.017115  0.487449  0.457343  0.515717  0.019792   0.485406  0.485235  0.485647  0.000118  0.476892  0.408092  0.501402  0.023734  0.460498  0.330130  0.497904  0.045633   0.474484  0.397956  0.489831  0.024560  0.494047  0.476384  0.523471  0.014618  0.475676  0.468441  0.483463  0.004076  0.498338  0.497486  0.499257  0.000631  0.473970  0.403610  0.496116  0.023739  0.494183  0.476025  0.508390  0.011523  0.453649  0.315108  0.506138  0.057606  0.600489  0.493274  0.827974  0.126829  0.487608  0.440735  0.525500  0.027035  0.467285  0.304751  0.504237  0.055772  0.582471  0.482382  0.772099   \n",
       "\n",
       "                  Calcium                               Alkalinephos                                    SpO2                               Bilirubin_direct                                Chloride                                     Hct                               Heartrate                               Bilirubin_total                               TroponinI                                    ABPs                                      pH                                \n",
       "            std      mean       min       max       std         mean       min       max       std      mean       min       max       std             mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std            mean       min       max       std      mean       min       max       std      mean       min       max       std      mean       min       max       std  \n",
       "pid                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "1      0.104873  0.519082  0.405560  0.614167  0.064441     0.481480  0.476152  0.483401  0.002080  0.698175  0.698175  0.698175  0.000000         0.476183  0.476050  0.476484  0.000148  0.557425  0.459441  0.787231  0.103705  0.334398  0.171632  0.484948  0.142894  0.407229  0.181942  0.694357  0.179800        0.479451  0.475400  0.481496  0.002011  0.475976  0.468003  0.480472  0.003587  0.419030  0.276646  0.699190  0.122937  0.522881  0.377413  0.639912  0.080885  \n",
       "2      0.116805  0.546005  0.498525  0.696723  0.050287     0.485449  0.483957  0.489442  0.001563  0.376077  0.211674  0.530162  0.118117         0.476094  0.475962  0.476236  0.000085  0.479390  0.417144  0.535298  0.037317  0.549553  0.497219  0.824235  0.087440  0.213610  0.190530  0.467490  0.079951        0.475706  0.474468  0.476290  0.000548  0.473840  0.432580  0.477986  0.012996  0.583753  0.474565  0.750493  0.084562  0.530889  0.503533  0.547600  0.014458  \n",
       "4      0.223064  0.525956  0.494467  0.608393  0.032708     0.490650  0.482949  0.565208  0.023490  0.628158  0.355022  0.698175  0.119467         0.468680  0.386394  0.476281  0.025914  0.527433  0.467542  0.601095  0.038538  0.502668  0.463292  0.659591  0.054158  0.351526  0.281616  0.503742  0.062954        0.472170  0.420457  0.478117  0.016314  0.474083  0.428488  0.479640  0.014369  0.517766  0.173260  0.876596  0.264833  0.520186  0.500316  0.544056  0.012715  \n",
       "6      0.046488  0.468186  0.324610  0.737910  0.112124     0.482998  0.476219  0.488149  0.002765  0.641577  0.355022  0.698175  0.111098         0.476189  0.476014  0.476669  0.000170  0.583532  0.469282  0.838402  0.109787  0.461981  0.337838  0.531020  0.049448  0.539535  0.464320  0.576998  0.047056        0.477716  0.476238  0.479601  0.001053  0.480426  0.477392  0.486785  0.003266  0.284826  0.228119  0.370168  0.051299  0.472226  0.377413  0.575933  0.060024  \n",
       "8      0.125135  0.543814  0.506987  0.565312  0.015172     0.482733  0.481643  0.486369  0.001267  0.518426  0.211674  0.698175  0.145144         0.475989  0.475675  0.476138  0.000119  0.464081  0.326015  0.494605  0.046915  0.526579  0.497586  0.551578  0.014774  0.463252  0.293223  0.631156  0.108623        0.477341  0.474939  0.478745  0.000979  0.473673  0.429072  0.478710  0.014050  0.909817  0.474800  0.993381  0.151726  0.522482  0.483871  0.545138  0.018274  \n",
       "...         ...       ...       ...       ...       ...          ...       ...       ...       ...       ...       ...       ...       ...              ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...             ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...       ...  \n",
       "31653  0.133395  0.540039  0.490282  0.707674  0.060028     0.485016  0.482372  0.491228  0.002721  0.391467  0.157921  0.548748  0.107234         0.476051  0.475322  0.476201  0.000235  0.497446  0.426529  0.523965  0.026280  0.497266  0.423356  0.573841  0.051356  0.747019  0.533727  0.973346  0.114468        0.478778  0.474899  0.481133  0.001670  0.479966  0.477994  0.483273  0.001488  0.223517  0.103717  0.563547  0.147142  0.462176  0.377413  0.508241  0.049878  \n",
       "31654  0.128991  0.542529  0.502174  0.625201  0.036556     0.483616  0.482254  0.485905  0.000982  0.384348  0.211674  0.617679  0.131758         0.476067  0.475802  0.476188  0.000104  0.439093  0.260592  0.496122  0.072281  0.540481  0.497378  0.639875  0.036153  0.595159  0.381536  0.849083  0.138362        0.476321  0.475132  0.477345  0.000567  0.477929  0.477314  0.479937  0.000887  0.823465  0.494520  0.941418  0.115341  0.509145  0.424130  0.539925  0.038428  \n",
       "31656  0.125276  0.485679  0.295909  0.539289  0.070027     0.483917  0.482298  0.488520  0.001726  0.669620  0.500204  0.698175  0.067650         0.475983  0.475639  0.476083  0.000119  0.466061  0.172886  0.537049  0.094162  0.514455  0.478529  0.718848  0.065139  0.663620  0.519245  0.833975  0.093310        0.477598  0.472998  0.480590  0.001865  0.480586  0.478291  0.489595  0.003352  0.318955  0.185917  0.476114  0.115935  0.445156  0.171353  0.542945  0.124002  \n",
       "31657  0.080820  0.527149  0.501320  0.544239  0.012171     0.483524  0.483039  0.484223  0.000294  0.606716  0.501051  0.698175  0.069766         0.476157  0.476123  0.476207  0.000026  0.523686  0.496022  0.547397  0.014710  0.496898  0.485169  0.507035  0.006955  0.283805  0.208629  0.495261  0.099118        0.476044  0.475271  0.476956  0.000502  0.477672  0.477342  0.478093  0.000211  0.397321  0.285326  0.517503  0.079141  0.520004  0.501069  0.532965  0.008492  \n",
       "31658  0.108316  0.540054  0.503493  0.641713  0.035379     0.483670  0.483173  0.484119  0.000336  0.433838  0.355022  0.543286  0.084295         0.476064  0.475992  0.476171  0.000062  0.487231  0.467144  0.519119  0.015152  0.538715  0.494463  0.824235  0.091311  0.385811  0.218141  0.516849  0.115281        0.477284  0.476901  0.477686  0.000232  0.478117  0.477678  0.478812  0.000298  0.550551  0.488565  0.652174  0.071542  0.523901  0.499153  0.566698  0.020814  \n",
       "\n",
       "[18995 rows x 144 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time, and age features it does not make sense to extract the 4 aforementioned features, so they are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 133)\n",
      "(12664, 133)\n"
     ]
    }
   ],
   "source": [
    "# features_train.columns = features_train.columns.droplevel()\n",
    "cols = [1,2,3,4,5,6,8,9,10,11,12]\n",
    "features_train.drop(features_train.columns[cols],inplace=True,axis=1)\n",
    "print(features_train.shape)\n",
    "features_test.drop(features_test.columns[cols],inplace=True,axis=1)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = features_train\n",
    "temp_test = features_test\n",
    "\n",
    "# features_train = temp_train\n",
    "# features_test = temp_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there might be features that are highly correlated with each other, we only keep one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PTT', 'std'), ('BUN', 'std'), ('BaseExcess', 'mean'), ('WBC', 'std'), ('Creatinine', 'max'), ('Creatinine', 'std'), ('FiO2', 'max'), ('FiO2', 'std'), ('SaO2', 'std'), ('Glucose', 'std'), ('ABPm', 'min'), ('ABPm', 'max'), ('ABPd', 'mean'), ('ABPd', 'min'), ('ABPd', 'max'), ('ABPd', 'std'), ('Alkalinephos', 'max'), ('Alkalinephos', 'std'), ('SpO2', 'min'), ('SpO2', 'std'), ('Bilirubin_direct', 'max'), ('Bilirubin_direct', 'std'), ('Hct', 'mean'), ('Hct', 'min'), ('Hct', 'max'), ('Hct', 'std'), ('Heartrate', 'min'), ('Heartrate', 'max'), ('Bilirubin_total', 'max'), ('Bilirubin_total', 'std'), ('TroponinI', 'max'), ('TroponinI', 'std'), ('ABPs', 'mean'), ('ABPs', 'min'), ('ABPs', 'max')]\n",
      "(18995, 98)\n",
      "(12664, 98)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"ab14f6c0-5a7d-4653-96c9-e72f4e03f6d8\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"ab14f6c0-5a7d-4653-96c9-e72f4e03f6d8\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix = features_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "print(to_drop)\n",
    "\n",
    "features_train.drop(features_train[to_drop], axis=1,inplace=True)\n",
    "features_test.drop(features_test[to_drop], axis=1,inplace=True)\n",
    "\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took the iterative imputer 5.412101745605469e-05 seconds to concat the data.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"c13524a0-bba2-4739-a4f9-629aee42f4c6\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"c13524a0-bba2-4739-a4f9-629aee42f4c6\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to skip concat, uncomment these\n",
    "fullTrain_concat = features_train\n",
    "fullTest_concat = features_test\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# fullTrain_concat = concat_pid_rows(features_train)\n",
    "# fullTest_concat = concat_pid_rows(features_test)\n",
    "\n",
    "# ## for skipping PCA, uncomment these:\n",
    "fullTrain = fullTrain_concat\n",
    "fullTest = fullTest_concat\n",
    "\n",
    "end = time.time()\n",
    "print('It took the iterative imputer '+str(end-start)+' seconds to concat the data.')\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our attempts, the PCA did not improve the performance a lot, rather it decreased it. Our guess is it is because of the highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = 0.75,random_state=0)\n",
    "\n",
    "# fullTrain_pca = pca.fit_transform(fullTrain_concat)\n",
    "# fullTrain = pd.DataFrame(fullTrain_pca,fullTrain_concat.index)\n",
    "\n",
    "# fullTest_pca = pca.transform(fullTest_concat)\n",
    "# fullTest = pd.DataFrame(fullTest_pca,fullTest_concat.index)\n",
    "# %notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullTrain.to_pickle('./pickled_data/fullTrain_Farzam')\n",
    "fullTest.to_pickle('./pickled_data/fullTest_Farzam')\n",
    "\n",
    "# fullTrain = pd.read_pickle('./pickled_data/fullTrain_Farzam')\n",
    "# fullTest = pd.read_pickle('./pickled_data/fullTest_Farzam')\n",
    "\n",
    "# fullTrain = pd.read_pickle('./pickled_data/features_train_scaled_impiter_concat_pca.pkl')\n",
    "# fullTest = pd.read_pickle('./pickled_data/features_test_scaled_impiter_concat_pca.pkl')\n",
    "\n",
    "# fullTrain = pd.read_pickle('./pickled_data/features_train_scaled_impknn_concat_pca')\n",
    "# fullTest = pd.read_pickle('./pickled_data/features_test_scaled_impknn_concat_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullTrain.sort_index(ascending=True,inplace=True)\n",
    "trains_labels.sort_index(ascending=True,inplace=True)\n",
    "predictions = pd.DataFrame(index=fullTest.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_task1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', 'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2', 'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "labels_task2 = ['LABEL_Sepsis']\n",
    "labels_task3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this subtask, we use logistic regression, and cross validate different parameters, with roc_auc for scoring.\n",
    "We also utilized random over sampler to handle the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_BaseExcess\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   36.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2048, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.8814621511870055\n",
      "LABEL_Fibrinogen\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=32, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.8019448979591837\n",
      "LABEL_AST\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   33.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2048, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.7262586916307487\n",
      "LABEL_Alkalinephos\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   31.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2048, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.7339883751037901\n",
      "LABEL_Bilirubin_total\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   32.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2048, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.7315576325479175\n",
      "LABEL_Lactate\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   20.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=16, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.7833829514031473\n",
      "LABEL_TroponinI\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   11.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2048, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.8478309117870245\n",
      "LABEL_SaO2\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   34.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.7985582744358573\n",
      "LABEL_Bilirubin_direct\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2048, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.8178769144708923\n",
      "LABEL_EtCO2\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=128, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.8997329756899135\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"1367900a-04d2-4cb0-a3ef-67594729b3cf\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"1367900a-04d2-4cb0-a3ef-67594729b3cf\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# runs faster\n",
    "predictions_1 = pd.DataFrame(columns=labels_task1)\n",
    "n_estimators = 10\n",
    "X_test_ = fullTest\n",
    "\n",
    "for i in labels_task1:\n",
    "    \n",
    "        print(i)\n",
    "        X = fullTrain.iloc[:,:]\n",
    "        y = trains_labels.loc[:,i]\n",
    "        X, y = RandomUnderSampler(random_state=0).fit_resample(X, y)\n",
    "\n",
    "        params = {'C':[2**-6,2**-5,2**-4,2**-3,2**-2,2**-1,2**1,2**2,2**3,2**4,2**5,2**7,2**9,2**11]}\n",
    "        est = LogisticRegression(solver='lbfgs',max_iter=5000,verbose=0,fit_intercept=True,intercept_scaling=1,\n",
    "                         multi_class='ovr')\n",
    "\n",
    "        clf = GridSearchCV(est,params,cv=5,n_jobs=-1,verbose=1,scoring='roc_auc')\n",
    "        clf.fit(X,y)\n",
    "        predictions_1.loc[:,i] = sigmoid(clf.decision_function(X_test_))\n",
    "        print(clf.best_estimator_)\n",
    "        print(\"train ROC score: \",roc_auc_score(y, sigmoid(clf.decision_function(X))))\n",
    "        \n",
    "        \n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_1.to_pickle(\"./pickled_data/task1CrossValidated\")\n",
    "\n",
    "# predictions_1 = pd.read_pickle(\"./pickled_data/task1CrossValidated\")\n",
    "# predictions_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small scale tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fullTrain.iloc[:,:]\n",
    "y = trains_labels.iloc[:,0]\n",
    "\n",
    "X, y = SMOTE().fit_resample(X, y)\n",
    "# X, y = ADASYN().fit_resample(X, y)\n",
    "# X, y = BorderlineSMOTE().fit_resample(X, y)\n",
    "# smote_nc = SMOTENC(categorical_features=[0, 1], random_state=0)\n",
    "# X, y = smote_nc.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=BaggingClassifier(base_estimator=SVC(C=32,\n",
       "                                                                   break_ties=False,\n",
       "                                                                   cache_size=200,\n",
       "                                                                   class_weight='balanced',\n",
       "                                                                   coef0=0.0,\n",
       "                                                                   decision_function_shape='ovr',\n",
       "                                                                   degree=5,\n",
       "                                                                   gamma='scale',\n",
       "                                                                   kernel='rbf',\n",
       "                                                                   max_iter=-1,\n",
       "                                                                   probability=True,\n",
       "                                                                   random_state=None,\n",
       "                                                                   shrinking=True,\n",
       "                                                                   tol=0.001,\n",
       "                                                                   verbose=False),\n",
       "                                                bootstrap=False,\n",
       "                                                bootstrap_features=False,\n",
       "                                                max_features=1.0,\n",
       "                                                max_samples=0.1,\n",
       "                                                n_estimators=10, n_jobs=-1,\n",
       "                                                oob_score=False,\n",
       "                                                random_state=None, verbose=0,\n",
       "                                                warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = 10\n",
    "clf_linear = OneVsRestClassifier(BaggingClassifier(SVC(kernel='rbf',C=2**5,degree=5,gamma='scale',shrinking=True,\n",
    "                                    probability=True,class_weight='balanced'), \n",
    "                                    max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=-1,bootstrap = False))\n",
    "clf_linear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ROC score:  0.9060435927100545\n",
      "test ROC score:  0.8986237271184149\n",
      "test ROC score (with decision function):  0.8984396960458944\n"
     ]
    }
   ],
   "source": [
    "print(\"train ROC score: \",roc_auc_score(y_train, pd.DataFrame(clf_linear.predict_proba(X_train)).iloc[:,1]))\n",
    "print(\"test ROC score: \",roc_auc_score(y_test, pd.DataFrame(clf_linear.predict_proba(X_test)).iloc[:,1]))\n",
    "print(\"test ROC score (with decision function): \",roc_auc_score(y_test, sigmoid(clf_linear.decision_function(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Best estimator: LogisticRegression(C=128, class_weight='balanced', dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.8778488278119425\n",
      "test ROC score:  0.8781711711571802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farzamf/Anaconda3/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "grid = {\"C\":[2**-2,2**-1, 2**1, 2**3, 2**5, 2**7, 2**9, 2**11,2**13, 2**15,2**17]}\n",
    "clf = LogisticRegression(class_weight='balanced',max_iter=1000)\n",
    "model = GridSearchCV(clf,grid,cv=5,n_jobs=-1)\n",
    "model.fit(X,y)\n",
    "print(' > Best estimator: {}'.format(model.best_estimator_))\n",
    "\n",
    "print(\"train ROC score: \",roc_auc_score(y_train, sigmoid(model.decision_function(X_train))))\n",
    "print(\"test ROC score: \",roc_auc_score(y_test, sigmoid(model.decision_function(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18995, 98)\n",
      "(18995, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "projected = pca.fit_transform(fullTrain_concat)\n",
    "print(fullTrain_concat.shape)\n",
    "print(projected.shape)\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=trains_labels.loc[:,'LABEL_Sepsis'], edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('summer', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 11 candidates, totalling 33 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator for sepsis: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=2000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "train ROC score:  0.7213581714889682\n",
      "test ROC score:  0.711110370994725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "predictions_2 = pd.DataFrame(columns=labels_task2)\n",
    "\n",
    "X = fullTrain.iloc[:,:]\n",
    "y = trains_labels.loc[:,'LABEL_Sepsis']\n",
    "\n",
    "X, testX, y, testY = train_test_split(X, y,stratify=y,test_size=0.2,random_state=0)\n",
    "\n",
    "X, y = RandomUnderSampler(random_state=0).fit_resample(X, y)\n",
    "# X, y = NearMiss(version=1).fit_resample(X, y)\n",
    "# X, y = SMOTE().fit_resample(X, y)\n",
    "X_test_ = fullTest\n",
    "\n",
    "params = {'C':[2**-6,2**-5,2**-4,2**-3,2**-2,2**-1,2**1,2**2,2**3,2**4,2**5]}\n",
    "est = LogisticRegression(solver='lbfgs',max_iter=2000,verbose=0,fit_intercept=True,intercept_scaling=1,\n",
    "                         multi_class='ovr')\n",
    "\n",
    "clf_sepsis = GridSearchCV(est,params,cv=3,n_jobs=-1,verbose=1,scoring='roc_auc')\n",
    "clf_sepsis.fit(X, y)\n",
    "\n",
    "predictions_2['LABEL_Sepsis'] = sigmoid(clf_sepsis.decision_function(X_test_))\n",
    "\n",
    "print('Best estimator for sepsis: {}'.format(clf_sepsis.best_estimator_))\n",
    "print(\"train ROC score: \",roc_auc_score(y, sigmoid(clf_sepsis.decision_function(X))))\n",
    "print(\"test ROC score: \",roc_auc_score(testY, sigmoid(clf_sepsis.decision_function(testX))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"2cb6f670-cb17-4817-b2ed-84a53bcb58c9\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"2cb6f670-cb17-4817-b2ed-84a53bcb58c9\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def auc( y_true, y_pred ) :\n",
    "    score = tf.py_function( lambda y_true, y_pred : roc_auc_score( y_true, y_pred, average='macro', sample_weight=None).astype('float32'),\n",
    "                        [y_true, y_pred],\n",
    "                        'float32',\n",
    "#                         stateful=False,\n",
    "                        name='sklearnAUC' )\n",
    "    return score\n",
    "\n",
    "predictions_2 = pd.DataFrame(columns=labels_task2)\n",
    "X = fullTrain.iloc[:,:]\n",
    "y = trains_labels.loc[:,'LABEL_Sepsis']\n",
    "X_test_ = fullTest\n",
    "\n",
    "X, testX, y, testY = train_test_split(X, y,stratify=y,test_size=0.15,random_state=0)\n",
    "\n",
    "# X, y = RandomOverSampler(random_state=0).fit_resample(X, y)\n",
    "X, y = RandomUnderSampler(random_state=0).fit_resample(X, y)\n",
    "# X, y = SMOTE().fit_resample(X, y)\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"5f4b8c47-a818-432a-9a09-0b74300bd857\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"5f4b8c47-a818-432a-9a09-0b74300bd857\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.constraints import unit_norm\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X.shape[1], kernel_initializer='he_uniform',activation='relu'))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-3,l2=1e-3)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-1,l2=1e-5)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5,l2=1e-1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "usualCallback = EarlyStopping()\n",
    "overfitCallback = EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=100)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.keras import BalancedBatchGenerator\n",
    "# from imblearn.under_sampling import NearMiss\n",
    "# from imblearn.combine import SMOTEENN\n",
    "# from imblearn.under_sampling import AllKNN\n",
    "\n",
    "# training_generator = BalancedBatchGenerator(X, to_categorical(y), sampler=NearMiss(), batch_size=128, random_state=0)\n",
    "# # validation_generator = BalancedBatchGenerator(testX, to_categorical(testY), sampler=NearMiss(), batch_size=128, random_state=0)\n",
    "\n",
    "# %notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1850 samples, validate on 2850 samples\n",
      "Epoch 1/1000\n",
      "1850/1850 [==============================] - 0s 253us/step - loss: 383.4000 - auc: 0.5055 - val_loss: 376.2031 - val_auc: 0.6129\n",
      "Epoch 2/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 376.1877 - auc: 0.5096 - val_loss: 369.0631 - val_auc: 0.6196\n",
      "Epoch 3/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 369.0468 - auc: 0.4919 - val_loss: 361.9986 - val_auc: 0.6231\n",
      "Epoch 4/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 361.9811 - auc: 0.4935 - val_loss: 355.0224 - val_auc: 0.6239\n",
      "Epoch 5/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 355.0033 - auc: 0.5120 - val_loss: 348.1189 - val_auc: 0.6159\n",
      "Epoch 6/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 348.1012 - auc: 0.5268 - val_loss: 341.2938 - val_auc: 0.6250\n",
      "Epoch 7/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 341.2814 - auc: 0.5097 - val_loss: 334.5479 - val_auc: 0.6254\n",
      "Epoch 8/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 334.5395 - auc: 0.5158 - val_loss: 327.8818 - val_auc: 0.6324\n",
      "Epoch 9/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 327.8766 - auc: 0.5154 - val_loss: 321.3006 - val_auc: 0.6419\n",
      "Epoch 10/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 321.2956 - auc: 0.5297 - val_loss: 314.7936 - val_auc: 0.6532\n",
      "Epoch 11/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 314.7874 - auc: 0.5285 - val_loss: 308.3669 - val_auc: 0.6611\n",
      "Epoch 12/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 308.3604 - auc: 0.5336 - val_loss: 302.0123 - val_auc: 0.6670\n",
      "Epoch 13/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 302.0049 - auc: 0.5504 - val_loss: 295.7390 - val_auc: 0.6698\n",
      "Epoch 14/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 295.7318 - auc: 0.5472 - val_loss: 289.5382 - val_auc: 0.6710\n",
      "Epoch 15/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 289.5300 - auc: 0.5542 - val_loss: 283.4087 - val_auc: 0.6738\n",
      "Epoch 16/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 283.3991 - auc: 0.5844 - val_loss: 277.3465 - val_auc: 0.6761\n",
      "Epoch 17/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 277.3373 - auc: 0.5737 - val_loss: 271.3325 - val_auc: 0.6765\n",
      "Epoch 18/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 271.3243 - auc: 0.5757 - val_loss: 265.4052 - val_auc: 0.6765\n",
      "Epoch 19/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 265.4024 - auc: 0.5660 - val_loss: 259.5460 - val_auc: 0.6770\n",
      "Epoch 20/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 259.5509 - auc: 0.6057 - val_loss: 253.7542 - val_auc: 0.6772\n",
      "Epoch 21/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 253.7693 - auc: 0.6021 - val_loss: 248.0437 - val_auc: 0.6783\n",
      "Epoch 22/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 248.0645 - auc: 0.5828 - val_loss: 242.3951 - val_auc: 0.6790\n",
      "Epoch 23/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 242.4137 - auc: 0.6118 - val_loss: 236.8328 - val_auc: 0.6790\n",
      "Epoch 24/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 236.8458 - auc: 0.5969 - val_loss: 231.3486 - val_auc: 0.6795\n",
      "Epoch 25/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 231.3474 - auc: 0.6167 - val_loss: 225.9300 - val_auc: 0.6805\n",
      "Epoch 26/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 225.9255 - auc: 0.6126 - val_loss: 220.5681 - val_auc: 0.6815\n",
      "Epoch 27/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 220.5741 - auc: 0.6034 - val_loss: 215.2624 - val_auc: 0.6825\n",
      "Epoch 28/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 215.2838 - auc: 0.6387 - val_loss: 210.0304 - val_auc: 0.6830\n",
      "Epoch 29/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 210.0558 - auc: 0.6436 - val_loss: 204.8883 - val_auc: 0.6832\n",
      "Epoch 30/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 204.9006 - auc: 0.6286 - val_loss: 199.8302 - val_auc: 0.6833\n",
      "Epoch 31/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 199.8165 - auc: 0.6267 - val_loss: 194.8009 - val_auc: 0.6838\n",
      "Epoch 32/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 194.7854 - auc: 0.6295 - val_loss: 189.8187 - val_auc: 0.6843\n",
      "Epoch 33/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 189.8329 - auc: 0.6363 - val_loss: 184.9190 - val_auc: 0.6848\n",
      "Epoch 34/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 184.9526 - auc: 0.6525 - val_loss: 180.1372 - val_auc: 0.6851\n",
      "Epoch 35/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 180.1414 - auc: 0.6394 - val_loss: 175.3939 - val_auc: 0.6856\n",
      "Epoch 36/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 175.3880 - auc: 0.6498 - val_loss: 170.6921 - val_auc: 0.6862\n",
      "Epoch 37/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 170.7130 - auc: 0.6649 - val_loss: 166.0907 - val_auc: 0.6865\n",
      "Epoch 38/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 166.1072 - auc: 0.6575 - val_loss: 161.5794 - val_auc: 0.6868\n",
      "Epoch 39/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 161.5582 - auc: 0.6544 - val_loss: 157.0701 - val_auc: 0.6875\n",
      "Epoch 40/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 157.0771 - auc: 0.6548 - val_loss: 152.6283 - val_auc: 0.6886\n",
      "Epoch 41/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 152.6693 - auc: 0.6527 - val_loss: 148.3552 - val_auc: 0.6888\n",
      "Epoch 42/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 148.3218 - auc: 0.6534 - val_loss: 144.0467 - val_auc: 0.6895\n",
      "Epoch 43/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 144.0337 - auc: 0.6525 - val_loss: 139.7854 - val_auc: 0.6905\n",
      "Epoch 44/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 139.8196 - auc: 0.6593 - val_loss: 135.7015 - val_auc: 0.6903\n",
      "Epoch 45/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 135.6777 - auc: 0.6643 - val_loss: 131.6192 - val_auc: 0.6908\n",
      "Epoch 46/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 131.6029 - auc: 0.6683 - val_loss: 127.5838 - val_auc: 0.6913\n",
      "Epoch 47/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 127.6060 - auc: 0.6645 - val_loss: 123.6535 - val_auc: 0.6912\n",
      "Epoch 48/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 123.6607 - auc: 0.6664 - val_loss: 119.8125 - val_auc: 0.6917\n",
      "Epoch 49/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 119.7783 - auc: 0.6676 - val_loss: 115.9545 - val_auc: 0.6921\n",
      "Epoch 50/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 115.9707 - auc: 0.6727 - val_loss: 112.2301 - val_auc: 0.6924\n",
      "Epoch 51/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 112.2569 - auc: 0.6676 - val_loss: 108.6259 - val_auc: 0.6929\n",
      "Epoch 52/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 108.6007 - auc: 0.6651 - val_loss: 105.0076 - val_auc: 0.6931\n",
      "Epoch 53/1000\n",
      "1850/1850 [==============================] - 0s 28us/step - loss: 104.9857 - auc: 0.6723 - val_loss: 101.4272 - val_auc: 0.6930\n",
      "Epoch 54/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 101.4374 - auc: 0.6749 - val_loss: 97.9304 - val_auc: 0.6938\n",
      "Epoch 55/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 97.9522 - auc: 0.6694 - val_loss: 94.5436 - val_auc: 0.6942\n",
      "Epoch 56/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 94.5169 - auc: 0.6708 - val_loss: 91.1363 - val_auc: 0.6943\n",
      "Epoch 57/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 91.1509 - auc: 0.6715 - val_loss: 87.8458 - val_auc: 0.6950\n",
      "Epoch 58/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 87.8474 - auc: 0.6764 - val_loss: 84.6420 - val_auc: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 84.6178 - auc: 0.6761 - val_loss: 81.4638 - val_auc: 0.6957\n",
      "Epoch 60/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 81.4667 - auc: 0.6738 - val_loss: 78.3512 - val_auc: 0.6960\n",
      "Epoch 61/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 78.3776 - auc: 0.6754 - val_loss: 75.3821 - val_auc: 0.6966\n",
      "Epoch 62/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 75.3443 - auc: 0.6785 - val_loss: 72.3975 - val_auc: 0.6970\n",
      "Epoch 63/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 72.3785 - auc: 0.6819 - val_loss: 69.4551 - val_auc: 0.6971\n",
      "Epoch 64/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 69.4960 - auc: 0.6778 - val_loss: 66.6859 - val_auc: 0.6979\n",
      "Epoch 65/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 66.6675 - auc: 0.6805 - val_loss: 63.9419 - val_auc: 0.6982\n",
      "Epoch 66/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 63.9026 - auc: 0.6779 - val_loss: 61.1839 - val_auc: 0.6988\n",
      "Epoch 67/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 61.1973 - auc: 0.6833 - val_loss: 58.5419 - val_auc: 0.6993\n",
      "Epoch 68/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 58.5512 - auc: 0.6746 - val_loss: 56.0114 - val_auc: 0.6993\n",
      "Epoch 69/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 55.9751 - auc: 0.6845 - val_loss: 53.4833 - val_auc: 0.6993\n",
      "Epoch 70/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 53.4666 - auc: 0.6851 - val_loss: 50.9948 - val_auc: 0.6992\n",
      "Epoch 71/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 51.0197 - auc: 0.6826 - val_loss: 48.6538 - val_auc: 0.6996\n",
      "Epoch 72/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 48.6379 - auc: 0.6730 - val_loss: 46.3349 - val_auc: 0.6995\n",
      "Epoch 73/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 46.3090 - auc: 0.6836 - val_loss: 44.0080 - val_auc: 0.6998\n",
      "Epoch 74/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 44.0483 - auc: 0.6737 - val_loss: 41.8280 - val_auc: 0.6998\n",
      "Epoch 75/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 41.8400 - auc: 0.6768 - val_loss: 39.7248 - val_auc: 0.7000\n",
      "Epoch 76/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 39.6977 - auc: 0.6840 - val_loss: 37.6314 - val_auc: 0.7003\n",
      "Epoch 77/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 37.6197 - auc: 0.6815 - val_loss: 35.5890 - val_auc: 0.7004\n",
      "Epoch 78/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 35.6097 - auc: 0.6808 - val_loss: 33.6681 - val_auc: 0.7010\n",
      "Epoch 79/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 33.6682 - auc: 0.6807 - val_loss: 31.8156 - val_auc: 0.7016\n",
      "Epoch 80/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 31.7793 - auc: 0.6834 - val_loss: 29.9379 - val_auc: 0.7017\n",
      "Epoch 81/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 29.9530 - auc: 0.6793 - val_loss: 28.1838 - val_auc: 0.7018\n",
      "Epoch 82/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 28.1974 - auc: 0.6764 - val_loss: 26.5377 - val_auc: 0.7015\n",
      "Epoch 83/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 26.5080 - auc: 0.6819 - val_loss: 24.8730 - val_auc: 0.7019\n",
      "Epoch 84/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 24.8889 - auc: 0.6671 - val_loss: 23.3094 - val_auc: 0.7022\n",
      "Epoch 85/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 23.3294 - auc: 0.6788 - val_loss: 21.8494 - val_auc: 0.7022\n",
      "Epoch 86/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 21.8327 - auc: 0.6822 - val_loss: 20.4209 - val_auc: 0.7022\n",
      "Epoch 87/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 20.4062 - auc: 0.6837 - val_loss: 19.0224 - val_auc: 0.7022\n",
      "Epoch 88/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 19.0432 - auc: 0.6752 - val_loss: 17.7380 - val_auc: 0.7024\n",
      "Epoch 89/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 17.7330 - auc: 0.6859 - val_loss: 16.5104 - val_auc: 0.7026\n",
      "Epoch 90/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 16.4989 - auc: 0.6721 - val_loss: 15.3035 - val_auc: 0.7026\n",
      "Epoch 91/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 15.3155 - auc: 0.6863 - val_loss: 14.1948 - val_auc: 0.7030\n",
      "Epoch 92/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 14.1938 - auc: 0.6739 - val_loss: 13.1326 - val_auc: 0.7033\n",
      "Epoch 93/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 13.1317 - auc: 0.6672 - val_loss: 12.1348 - val_auc: 0.7035\n",
      "Epoch 94/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 12.1342 - auc: 0.6770 - val_loss: 11.1880 - val_auc: 0.7036\n",
      "Epoch 95/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 11.2025 - auc: 0.6589 - val_loss: 10.3257 - val_auc: 0.7039\n",
      "Epoch 96/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 10.3252 - auc: 0.6692 - val_loss: 9.5183 - val_auc: 0.7040\n",
      "Epoch 97/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 9.5190 - auc: 0.6497 - val_loss: 8.7469 - val_auc: 0.7042\n",
      "Epoch 98/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 8.7554 - auc: 0.6849 - val_loss: 8.0770 - val_auc: 0.7040\n",
      "Epoch 99/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 8.0830 - auc: 0.6522 - val_loss: 7.4658 - val_auc: 0.7040\n",
      "Epoch 100/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 7.4614 - auc: 0.6488 - val_loss: 6.8958 - val_auc: 0.7038\n",
      "Epoch 101/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 6.8974 - auc: 0.6388 - val_loss: 6.3701 - val_auc: 0.7037\n",
      "Epoch 102/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 6.3779 - auc: 0.6503 - val_loss: 5.9286 - val_auc: 0.7036\n",
      "Epoch 103/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 5.9453 - auc: 0.6003 - val_loss: 5.5568 - val_auc: 0.7037\n",
      "Epoch 104/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 5.5549 - auc: 0.6215 - val_loss: 5.2334 - val_auc: 0.7035\n",
      "Epoch 105/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 5.2359 - auc: 0.6041 - val_loss: 4.9546 - val_auc: 0.7033\n",
      "Epoch 106/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 4.9731 - auc: 0.6007 - val_loss: 4.7712 - val_auc: 0.7032\n",
      "Epoch 107/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 4.7774 - auc: 0.5878 - val_loss: 4.6704 - val_auc: 0.7031\n",
      "Epoch 108/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 4.6512 - auc: 0.5724 - val_loss: 4.5839 - val_auc: 0.7030\n",
      "Epoch 109/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 4.5938 - auc: 0.5671 - val_loss: 4.5327 - val_auc: 0.7029\n",
      "Epoch 110/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 4.5511 - auc: 0.5903 - val_loss: 4.4686 - val_auc: 0.7030\n",
      "Epoch 111/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 4.4745 - auc: 0.5696 - val_loss: 4.3632 - val_auc: 0.7031\n",
      "Epoch 112/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 4.3541 - auc: 0.5840 - val_loss: 4.2000 - val_auc: 0.7032\n",
      "Epoch 113/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 4.2024 - auc: 0.6168 - val_loss: 4.0183 - val_auc: 0.7030\n",
      "Epoch 114/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 4.0322 - auc: 0.5970 - val_loss: 3.8496 - val_auc: 0.7032\n",
      "Epoch 115/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 3.8444 - auc: 0.6285 - val_loss: 3.6673 - val_auc: 0.7033\n",
      "Epoch 116/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 3.6636 - auc: 0.6005 - val_loss: 3.4697 - val_auc: 0.7033\n",
      "Epoch 117/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 3.4703 - auc: 0.6319 - val_loss: 3.2745 - val_auc: 0.7032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 3.2840 - auc: 0.6313 - val_loss: 3.1145 - val_auc: 0.7031\n",
      "Epoch 119/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 3.1167 - auc: 0.6119 - val_loss: 2.9674 - val_auc: 0.7032\n",
      "Epoch 120/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 2.9631 - auc: 0.6415 - val_loss: 2.8359 - val_auc: 0.7030\n",
      "Epoch 121/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 2.8398 - auc: 0.6450 - val_loss: 2.7319 - val_auc: 0.7031\n",
      "Epoch 122/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.7417 - auc: 0.6347 - val_loss: 2.6607 - val_auc: 0.7031\n",
      "Epoch 123/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 2.6722 - auc: 0.6420 - val_loss: 2.6309 - val_auc: 0.7031\n",
      "Epoch 124/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 2.6230 - auc: 0.6542 - val_loss: 2.5890 - val_auc: 0.7031\n",
      "Epoch 125/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 2.5897 - auc: 0.6363 - val_loss: 2.5275 - val_auc: 0.7031\n",
      "Epoch 126/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.5488 - auc: 0.6365 - val_loss: 2.4936 - val_auc: 0.7031\n",
      "Epoch 127/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.4915 - auc: 0.6536 - val_loss: 2.4250 - val_auc: 0.7032\n",
      "Epoch 128/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.4200 - auc: 0.6622 - val_loss: 2.3430 - val_auc: 0.7031\n",
      "Epoch 129/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 2.3485 - auc: 0.6381 - val_loss: 2.2673 - val_auc: 0.7031\n",
      "Epoch 130/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 2.2680 - auc: 0.6313 - val_loss: 2.1971 - val_auc: 0.7031\n",
      "Epoch 131/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.1991 - auc: 0.6477 - val_loss: 2.1368 - val_auc: 0.7032\n",
      "Epoch 132/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 2.1461 - auc: 0.6566 - val_loss: 2.1214 - val_auc: 0.7032\n",
      "Epoch 133/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.1116 - auc: 0.6374 - val_loss: 2.0758 - val_auc: 0.7031\n",
      "Epoch 134/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 2.0793 - auc: 0.6480 - val_loss: 2.0290 - val_auc: 0.7032\n",
      "Epoch 135/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 2.0493 - auc: 0.6400 - val_loss: 2.0325 - val_auc: 0.7034\n",
      "Epoch 136/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 2.0162 - auc: 0.6450 - val_loss: 1.9784 - val_auc: 0.7034\n",
      "Epoch 137/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.9862 - auc: 0.6417 - val_loss: 1.9172 - val_auc: 0.7035\n",
      "Epoch 138/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.9513 - auc: 0.6573 - val_loss: 1.9761 - val_auc: 0.7035\n",
      "Epoch 139/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.9241 - auc: 0.6510 - val_loss: 1.8084 - val_auc: 0.7035\n",
      "Epoch 140/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.8909 - auc: 0.6506 - val_loss: 1.9233 - val_auc: 0.7036\n",
      "Epoch 141/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.8642 - auc: 0.6464 - val_loss: 1.8117 - val_auc: 0.7036\n",
      "Epoch 142/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.8414 - auc: 0.6459 - val_loss: 1.7870 - val_auc: 0.7036\n",
      "Epoch 143/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.8201 - auc: 0.6587 - val_loss: 1.8831 - val_auc: 0.7038\n",
      "Epoch 144/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.8123 - auc: 0.6440 - val_loss: 1.7229 - val_auc: 0.7037\n",
      "Epoch 145/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.7843 - auc: 0.6554 - val_loss: 1.7686 - val_auc: 0.7037\n",
      "Epoch 146/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.7626 - auc: 0.6530 - val_loss: 1.7904 - val_auc: 0.7038\n",
      "Epoch 147/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.7449 - auc: 0.6613 - val_loss: 1.6561 - val_auc: 0.7037\n",
      "Epoch 148/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.7301 - auc: 0.6679 - val_loss: 1.7694 - val_auc: 0.7038\n",
      "Epoch 149/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.7222 - auc: 0.6508 - val_loss: 1.7046 - val_auc: 0.7039\n",
      "Epoch 150/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.7108 - auc: 0.6483 - val_loss: 1.6460 - val_auc: 0.7037\n",
      "Epoch 151/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.7065 - auc: 0.6449 - val_loss: 1.7543 - val_auc: 0.7038\n",
      "Epoch 152/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.6852 - auc: 0.6657 - val_loss: 1.6203 - val_auc: 0.7037\n",
      "Epoch 153/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.6721 - auc: 0.6557 - val_loss: 1.6661 - val_auc: 0.7036\n",
      "Epoch 154/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.6527 - auc: 0.6679 - val_loss: 1.6830 - val_auc: 0.7036\n",
      "Epoch 155/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.6499 - auc: 0.6663 - val_loss: 1.5771 - val_auc: 0.7036\n",
      "Epoch 156/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.6509 - auc: 0.6604 - val_loss: 1.7084 - val_auc: 0.7039\n",
      "Epoch 157/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.6400 - auc: 0.6665 - val_loss: 1.5523 - val_auc: 0.7037\n",
      "Epoch 158/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.6316 - auc: 0.6566 - val_loss: 1.6794 - val_auc: 0.7039\n",
      "Epoch 159/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.6240 - auc: 0.6574 - val_loss: 1.5817 - val_auc: 0.7037\n",
      "Epoch 160/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.6152 - auc: 0.6511 - val_loss: 1.5944 - val_auc: 0.7038\n",
      "Epoch 161/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.6039 - auc: 0.6617 - val_loss: 1.6421 - val_auc: 0.7039\n",
      "Epoch 162/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5989 - auc: 0.6664 - val_loss: 1.5326 - val_auc: 0.7039\n",
      "Epoch 163/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5984 - auc: 0.6567 - val_loss: 1.6764 - val_auc: 0.7040\n",
      "Epoch 164/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5946 - auc: 0.6673 - val_loss: 1.4927 - val_auc: 0.7039\n",
      "Epoch 165/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5918 - auc: 0.6647 - val_loss: 1.6875 - val_auc: 0.7040\n",
      "Epoch 166/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5866 - auc: 0.6729 - val_loss: 1.4992 - val_auc: 0.7040\n",
      "Epoch 167/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5797 - auc: 0.6662 - val_loss: 1.6162 - val_auc: 0.7039\n",
      "Epoch 168/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5726 - auc: 0.6713 - val_loss: 1.5725 - val_auc: 0.7040\n",
      "Epoch 169/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.5672 - auc: 0.6776 - val_loss: 1.5183 - val_auc: 0.7041\n",
      "Epoch 170/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5707 - auc: 0.6573 - val_loss: 1.6378 - val_auc: 0.7042\n",
      "Epoch 171/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5695 - auc: 0.6731 - val_loss: 1.4964 - val_auc: 0.7042\n",
      "Epoch 172/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5687 - auc: 0.6575 - val_loss: 1.6333 - val_auc: 0.7043\n",
      "Epoch 173/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5626 - auc: 0.6684 - val_loss: 1.4928 - val_auc: 0.7043\n",
      "Epoch 174/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5644 - auc: 0.6699 - val_loss: 1.6248 - val_auc: 0.7043\n",
      "Epoch 175/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5609 - auc: 0.6694 - val_loss: 1.4983 - val_auc: 0.7043\n",
      "Epoch 176/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5540 - auc: 0.6692 - val_loss: 1.5838 - val_auc: 0.7044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5565 - auc: 0.6547 - val_loss: 1.5539 - val_auc: 0.7044\n",
      "Epoch 178/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5511 - auc: 0.6576 - val_loss: 1.5075 - val_auc: 0.7045\n",
      "Epoch 179/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5466 - auc: 0.6612 - val_loss: 1.6155 - val_auc: 0.7044\n",
      "Epoch 180/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5483 - auc: 0.6686 - val_loss: 1.4437 - val_auc: 0.7046\n",
      "Epoch 181/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5570 - auc: 0.6553 - val_loss: 1.6643 - val_auc: 0.7045\n",
      "Epoch 182/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5488 - auc: 0.6684 - val_loss: 1.4227 - val_auc: 0.7046\n",
      "Epoch 183/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5508 - auc: 0.6576 - val_loss: 1.6159 - val_auc: 0.7047\n",
      "Epoch 184/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5427 - auc: 0.6678 - val_loss: 1.5480 - val_auc: 0.7048\n",
      "Epoch 185/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5345 - auc: 0.6683 - val_loss: 1.4526 - val_auc: 0.7048\n",
      "Epoch 186/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5408 - auc: 0.6603 - val_loss: 1.6227 - val_auc: 0.7047\n",
      "Epoch 187/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5398 - auc: 0.6613 - val_loss: 1.5187 - val_auc: 0.7048\n",
      "Epoch 188/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5326 - auc: 0.6487 - val_loss: 1.4844 - val_auc: 0.7049\n",
      "Epoch 189/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5296 - auc: 0.6698 - val_loss: 1.6304 - val_auc: 0.7048\n",
      "Epoch 190/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5376 - auc: 0.6674 - val_loss: 1.4546 - val_auc: 0.7050\n",
      "Epoch 191/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5359 - auc: 0.6727 - val_loss: 1.5430 - val_auc: 0.7050\n",
      "Epoch 192/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5305 - auc: 0.6696 - val_loss: 1.5914 - val_auc: 0.7050\n",
      "Epoch 193/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5301 - auc: 0.6750 - val_loss: 1.4439 - val_auc: 0.7052\n",
      "Epoch 194/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5317 - auc: 0.6565 - val_loss: 1.6051 - val_auc: 0.7051\n",
      "Epoch 195/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5317 - auc: 0.6642 - val_loss: 1.4917 - val_auc: 0.7052\n",
      "Epoch 196/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.5236 - auc: 0.6693 - val_loss: 1.5044 - val_auc: 0.7052\n",
      "Epoch 197/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5213 - auc: 0.6690 - val_loss: 1.5867 - val_auc: 0.7051\n",
      "Epoch 198/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5182 - auc: 0.6769 - val_loss: 1.4234 - val_auc: 0.7054\n",
      "Epoch 199/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5311 - auc: 0.6657 - val_loss: 1.6390 - val_auc: 0.7052\n",
      "Epoch 200/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5289 - auc: 0.6721 - val_loss: 1.4612 - val_auc: 0.7055\n",
      "Epoch 201/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5179 - auc: 0.6757 - val_loss: 1.5221 - val_auc: 0.7054\n",
      "Epoch 202/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5104 - auc: 0.6737 - val_loss: 1.5478 - val_auc: 0.7054\n",
      "Epoch 203/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5177 - auc: 0.6620 - val_loss: 1.4521 - val_auc: 0.7054\n",
      "Epoch 204/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5187 - auc: 0.6683 - val_loss: 1.6213 - val_auc: 0.7053\n",
      "Epoch 205/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5249 - auc: 0.6690 - val_loss: 1.4151 - val_auc: 0.7054\n",
      "Epoch 206/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5253 - auc: 0.6733 - val_loss: 1.6736 - val_auc: 0.7053\n",
      "Epoch 207/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5290 - auc: 0.6696 - val_loss: 1.4144 - val_auc: 0.7056\n",
      "Epoch 208/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5223 - auc: 0.6733 - val_loss: 1.5466 - val_auc: 0.7057\n",
      "Epoch 209/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5144 - auc: 0.6712 - val_loss: 1.5615 - val_auc: 0.7057\n",
      "Epoch 210/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5061 - auc: 0.6859 - val_loss: 1.4451 - val_auc: 0.7058\n",
      "Epoch 211/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5183 - auc: 0.6683 - val_loss: 1.5651 - val_auc: 0.7057\n",
      "Epoch 212/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5157 - auc: 0.6688 - val_loss: 1.5096 - val_auc: 0.7057\n",
      "Epoch 213/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5151 - auc: 0.6627 - val_loss: 1.4852 - val_auc: 0.7058\n",
      "Epoch 214/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5073 - auc: 0.6753 - val_loss: 1.5625 - val_auc: 0.7057\n",
      "Epoch 215/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5137 - auc: 0.6689 - val_loss: 1.4592 - val_auc: 0.7058\n",
      "Epoch 216/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5113 - auc: 0.6789 - val_loss: 1.5838 - val_auc: 0.7057\n",
      "Epoch 217/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5109 - auc: 0.6745 - val_loss: 1.4180 - val_auc: 0.7059\n",
      "Epoch 218/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5114 - auc: 0.6743 - val_loss: 1.6161 - val_auc: 0.7060\n",
      "Epoch 219/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5163 - auc: 0.6708 - val_loss: 1.4160 - val_auc: 0.7060\n",
      "Epoch 220/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5168 - auc: 0.6692 - val_loss: 1.5838 - val_auc: 0.7062\n",
      "Epoch 221/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5164 - auc: 0.6651 - val_loss: 1.4543 - val_auc: 0.7062\n",
      "Epoch 222/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5086 - auc: 0.6696 - val_loss: 1.5446 - val_auc: 0.7062\n",
      "Epoch 223/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5049 - auc: 0.6785 - val_loss: 1.5060 - val_auc: 0.7063\n",
      "Epoch 224/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5071 - auc: 0.6712 - val_loss: 1.4574 - val_auc: 0.7063\n",
      "Epoch 225/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5056 - auc: 0.6725 - val_loss: 1.6287 - val_auc: 0.7062\n",
      "Epoch 226/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5109 - auc: 0.6771 - val_loss: 1.3622 - val_auc: 0.7064\n",
      "Epoch 227/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5242 - auc: 0.6653 - val_loss: 1.7155 - val_auc: 0.7064\n",
      "Epoch 228/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5298 - auc: 0.6835 - val_loss: 1.4510 - val_auc: 0.7068\n",
      "Epoch 229/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5182 - auc: 0.6754 - val_loss: 1.4308 - val_auc: 0.7066\n",
      "Epoch 230/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5206 - auc: 0.6605 - val_loss: 1.6337 - val_auc: 0.7069\n",
      "Epoch 231/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5190 - auc: 0.6651 - val_loss: 1.4934 - val_auc: 0.7069\n",
      "Epoch 232/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4987 - auc: 0.6718 - val_loss: 1.4311 - val_auc: 0.7067\n",
      "Epoch 233/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5033 - auc: 0.6793 - val_loss: 1.6291 - val_auc: 0.7062\n",
      "Epoch 234/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5162 - auc: 0.6830 - val_loss: 1.4646 - val_auc: 0.7066\n",
      "Epoch 235/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5125 - auc: 0.6772 - val_loss: 1.4818 - val_auc: 0.7067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5030 - auc: 0.6766 - val_loss: 1.5784 - val_auc: 0.7066\n",
      "Epoch 237/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5045 - auc: 0.6724 - val_loss: 1.4370 - val_auc: 0.7068\n",
      "Epoch 238/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5010 - auc: 0.6761 - val_loss: 1.5339 - val_auc: 0.7068\n",
      "Epoch 239/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4998 - auc: 0.6764 - val_loss: 1.4707 - val_auc: 0.7070\n",
      "Epoch 240/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5031 - auc: 0.6786 - val_loss: 1.5015 - val_auc: 0.7070\n",
      "Epoch 241/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5075 - auc: 0.6740 - val_loss: 1.5263 - val_auc: 0.7069\n",
      "Epoch 242/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5003 - auc: 0.6720 - val_loss: 1.4495 - val_auc: 0.7070\n",
      "Epoch 243/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4975 - auc: 0.6724 - val_loss: 1.6277 - val_auc: 0.7067\n",
      "Epoch 244/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5076 - auc: 0.6759 - val_loss: 1.3384 - val_auc: 0.7074\n",
      "Epoch 245/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5300 - auc: 0.6690 - val_loss: 1.8347 - val_auc: 0.7070\n",
      "Epoch 246/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5599 - auc: 0.6800 - val_loss: 1.4683 - val_auc: 0.7076\n",
      "Epoch 247/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5146 - auc: 0.6818 - val_loss: 1.3653 - val_auc: 0.7073\n",
      "Epoch 248/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5356 - auc: 0.6631 - val_loss: 1.6272 - val_auc: 0.6819\n",
      "Epoch 249/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5257 - auc: 0.6556 - val_loss: 1.5847 - val_auc: 0.7077\n",
      "Epoch 250/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5059 - auc: 0.6753 - val_loss: 1.4511 - val_auc: 0.7076\n",
      "Epoch 251/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5015 - auc: 0.6891 - val_loss: 1.5049 - val_auc: 0.7073\n",
      "Epoch 252/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5160 - auc: 0.6764 - val_loss: 1.5803 - val_auc: 0.7070\n",
      "Epoch 253/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5238 - auc: 0.6746 - val_loss: 1.4960 - val_auc: 0.7072\n",
      "Epoch 254/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5133 - auc: 0.6769 - val_loss: 1.4912 - val_auc: 0.7071\n",
      "Epoch 255/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5101 - auc: 0.6537 - val_loss: 1.5736 - val_auc: 0.7071\n",
      "Epoch 256/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5064 - auc: 0.6657 - val_loss: 1.4580 - val_auc: 0.7073\n",
      "Epoch 257/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5014 - auc: 0.6708 - val_loss: 1.4904 - val_auc: 0.7075\n",
      "Epoch 258/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5024 - auc: 0.6780 - val_loss: 1.5229 - val_auc: 0.7075\n",
      "Epoch 259/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5076 - auc: 0.6806 - val_loss: 1.4748 - val_auc: 0.7075\n",
      "Epoch 260/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5052 - auc: 0.6730 - val_loss: 1.4937 - val_auc: 0.7074\n",
      "Epoch 261/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4992 - auc: 0.6725 - val_loss: 1.5446 - val_auc: 0.7074\n",
      "Epoch 262/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4936 - auc: 0.6757 - val_loss: 1.3662 - val_auc: 0.7077\n",
      "Epoch 263/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5139 - auc: 0.6662 - val_loss: 1.9055 - val_auc: 0.7072\n",
      "Epoch 264/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5859 - auc: 0.6746 - val_loss: 1.4491 - val_auc: 0.7080\n",
      "Epoch 265/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5178 - auc: 0.6861 - val_loss: 1.3152 - val_auc: 0.7080\n",
      "Epoch 266/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5496 - auc: 0.6645 - val_loss: 1.7143 - val_auc: 0.6979\n",
      "Epoch 267/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5380 - auc: 0.6667 - val_loss: 1.6265 - val_auc: 0.7081\n",
      "Epoch 268/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5140 - auc: 0.6694 - val_loss: 1.4088 - val_auc: 0.7080\n",
      "Epoch 269/1000\n",
      "1850/1850 [==============================] - 0s 31us/step - loss: 1.5145 - auc: 0.6829 - val_loss: 1.4438 - val_auc: 0.7079\n",
      "Epoch 270/1000\n",
      "1850/1850 [==============================] - 0s 29us/step - loss: 1.5179 - auc: 0.6733 - val_loss: 1.5852 - val_auc: 0.7079\n",
      "Epoch 271/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5238 - auc: 0.6707 - val_loss: 1.5977 - val_auc: 0.7078\n",
      "Epoch 272/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5179 - auc: 0.6711 - val_loss: 1.4789 - val_auc: 0.7078\n",
      "Epoch 273/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5105 - auc: 0.6649 - val_loss: 1.4672 - val_auc: 0.7077\n",
      "Epoch 274/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5003 - auc: 0.6669 - val_loss: 1.5358 - val_auc: 0.7078\n",
      "Epoch 275/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4930 - auc: 0.6807 - val_loss: 1.5295 - val_auc: 0.7078\n",
      "Epoch 276/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5013 - auc: 0.6728 - val_loss: 1.4377 - val_auc: 0.7080\n",
      "Epoch 277/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5018 - auc: 0.6802 - val_loss: 1.4488 - val_auc: 0.7079\n",
      "Epoch 278/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4989 - auc: 0.6768 - val_loss: 1.5506 - val_auc: 0.7079\n",
      "Epoch 279/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5000 - auc: 0.6676 - val_loss: 1.5097 - val_auc: 0.7080\n",
      "Epoch 280/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4891 - auc: 0.6733 - val_loss: 1.4025 - val_auc: 0.7079\n",
      "Epoch 281/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4906 - auc: 0.6748 - val_loss: 1.6678 - val_auc: 0.7078\n",
      "Epoch 282/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5069 - auc: 0.6795 - val_loss: 1.4524 - val_auc: 0.7081\n",
      "Epoch 283/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4973 - auc: 0.6781 - val_loss: 1.3713 - val_auc: 0.7083\n",
      "Epoch 284/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5071 - auc: 0.6724 - val_loss: 1.7594 - val_auc: 0.7083\n",
      "Epoch 285/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5344 - auc: 0.6749 - val_loss: 1.4907 - val_auc: 0.7086\n",
      "Epoch 286/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4900 - auc: 0.6830 - val_loss: 1.3210 - val_auc: 0.7086\n",
      "Epoch 287/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5262 - auc: 0.6628 - val_loss: 1.6195 - val_auc: 0.7088\n",
      "Epoch 288/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5059 - auc: 0.6757 - val_loss: 1.6068 - val_auc: 0.7088\n",
      "Epoch 289/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5049 - auc: 0.6743 - val_loss: 1.4027 - val_auc: 0.7088\n",
      "Epoch 290/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5012 - auc: 0.6812 - val_loss: 1.4426 - val_auc: 0.7087\n",
      "Epoch 291/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4942 - auc: 0.6770 - val_loss: 1.6002 - val_auc: 0.7086\n",
      "Epoch 292/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4971 - auc: 0.6795 - val_loss: 1.5549 - val_auc: 0.7086\n",
      "Epoch 293/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4953 - auc: 0.6723 - val_loss: 1.4209 - val_auc: 0.7086\n",
      "Epoch 294/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4974 - auc: 0.6732 - val_loss: 1.5083 - val_auc: 0.7086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4884 - auc: 0.6782 - val_loss: 1.5416 - val_auc: 0.7087\n",
      "Epoch 296/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4952 - auc: 0.6722 - val_loss: 1.4679 - val_auc: 0.7087\n",
      "Epoch 297/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4930 - auc: 0.6685 - val_loss: 1.4268 - val_auc: 0.7088\n",
      "Epoch 298/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4837 - auc: 0.6792 - val_loss: 1.5380 - val_auc: 0.7085\n",
      "Epoch 299/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4831 - auc: 0.6783 - val_loss: 1.4927 - val_auc: 0.7085\n",
      "Epoch 300/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4805 - auc: 0.6820 - val_loss: 1.4223 - val_auc: 0.7086\n",
      "Epoch 301/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4883 - auc: 0.6742 - val_loss: 1.5299 - val_auc: 0.7085\n",
      "Epoch 302/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4809 - auc: 0.6829 - val_loss: 1.4850 - val_auc: 0.7086\n",
      "Epoch 303/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4832 - auc: 0.6717 - val_loss: 1.4368 - val_auc: 0.7087\n",
      "Epoch 304/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4853 - auc: 0.6643 - val_loss: 1.5974 - val_auc: 0.7089\n",
      "Epoch 305/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4814 - auc: 0.6866 - val_loss: 1.3505 - val_auc: 0.7089\n",
      "Epoch 306/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4930 - auc: 0.6717 - val_loss: 1.6417 - val_auc: 0.7088\n",
      "Epoch 307/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5050 - auc: 0.6801 - val_loss: 1.5117 - val_auc: 0.7092\n",
      "Epoch 308/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4926 - auc: 0.6802 - val_loss: 1.3564 - val_auc: 0.7095\n",
      "Epoch 309/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5067 - auc: 0.6750 - val_loss: 1.5790 - val_auc: 0.6982\n",
      "Epoch 310/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4954 - auc: 0.6679 - val_loss: 1.5183 - val_auc: 0.7093\n",
      "Epoch 311/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4826 - auc: 0.6737 - val_loss: 1.4085 - val_auc: 0.7091\n",
      "Epoch 312/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4881 - auc: 0.6819 - val_loss: 1.5143 - val_auc: 0.7092\n",
      "Epoch 313/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4839 - auc: 0.6843 - val_loss: 1.5458 - val_auc: 0.7092\n",
      "Epoch 314/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4852 - auc: 0.6813 - val_loss: 1.4407 - val_auc: 0.7092\n",
      "Epoch 315/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4878 - auc: 0.6859 - val_loss: 1.6054 - val_auc: 0.7093\n",
      "Epoch 316/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4920 - auc: 0.6791 - val_loss: 1.4582 - val_auc: 0.7093\n",
      "Epoch 317/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4821 - auc: 0.6782 - val_loss: 1.3931 - val_auc: 0.7094\n",
      "Epoch 318/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4882 - auc: 0.6772 - val_loss: 1.5772 - val_auc: 0.7093\n",
      "Epoch 319/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4908 - auc: 0.6772 - val_loss: 1.5073 - val_auc: 0.7095\n",
      "Epoch 320/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4836 - auc: 0.6822 - val_loss: 1.3779 - val_auc: 0.7096\n",
      "Epoch 321/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4981 - auc: 0.6711 - val_loss: 1.5392 - val_auc: 0.7095\n",
      "Epoch 322/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4830 - auc: 0.6795 - val_loss: 1.5202 - val_auc: 0.7095\n",
      "Epoch 323/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4799 - auc: 0.6773 - val_loss: 1.3972 - val_auc: 0.7096\n",
      "Epoch 324/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4875 - auc: 0.6716 - val_loss: 1.5709 - val_auc: 0.7095\n",
      "Epoch 325/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4859 - auc: 0.6806 - val_loss: 1.4748 - val_auc: 0.7095\n",
      "Epoch 326/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4711 - auc: 0.6915 - val_loss: 1.4025 - val_auc: 0.7096\n",
      "Epoch 327/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4850 - auc: 0.6750 - val_loss: 1.6228 - val_auc: 0.7096\n",
      "Epoch 328/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4942 - auc: 0.6754 - val_loss: 1.4671 - val_auc: 0.7097\n",
      "Epoch 329/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4796 - auc: 0.6790 - val_loss: 1.3864 - val_auc: 0.7098\n",
      "Epoch 330/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4912 - auc: 0.6686 - val_loss: 1.6303 - val_auc: 0.7097\n",
      "Epoch 331/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4953 - auc: 0.6811 - val_loss: 1.4863 - val_auc: 0.7101\n",
      "Epoch 332/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4842 - auc: 0.6751 - val_loss: 1.3696 - val_auc: 0.7099\n",
      "Epoch 333/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4920 - auc: 0.6784 - val_loss: 1.5881 - val_auc: 0.7099\n",
      "Epoch 334/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4855 - auc: 0.6808 - val_loss: 1.5108 - val_auc: 0.7099\n",
      "Epoch 335/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4752 - auc: 0.6813 - val_loss: 1.3700 - val_auc: 0.7098\n",
      "Epoch 336/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4896 - auc: 0.6722 - val_loss: 1.5653 - val_auc: 0.7099\n",
      "Epoch 337/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4879 - auc: 0.6761 - val_loss: 1.5444 - val_auc: 0.7100\n",
      "Epoch 338/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4797 - auc: 0.6813 - val_loss: 1.3802 - val_auc: 0.7098\n",
      "Epoch 339/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4854 - auc: 0.6833 - val_loss: 1.5703 - val_auc: 0.7100\n",
      "Epoch 340/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4767 - auc: 0.6905 - val_loss: 1.5027 - val_auc: 0.7101\n",
      "Epoch 341/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4788 - auc: 0.6794 - val_loss: 1.3756 - val_auc: 0.7100\n",
      "Epoch 342/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4877 - auc: 0.6777 - val_loss: 1.5379 - val_auc: 0.7103\n",
      "Epoch 343/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4796 - auc: 0.6798 - val_loss: 1.5310 - val_auc: 0.7102\n",
      "Epoch 344/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4731 - auc: 0.6860 - val_loss: 1.3735 - val_auc: 0.7102\n",
      "Epoch 345/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4791 - auc: 0.6855 - val_loss: 1.5649 - val_auc: 0.7102\n",
      "Epoch 346/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4794 - auc: 0.6853 - val_loss: 1.5113 - val_auc: 0.7103\n",
      "Epoch 347/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4767 - auc: 0.6797 - val_loss: 1.3709 - val_auc: 0.7104\n",
      "Epoch 348/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4859 - auc: 0.6704 - val_loss: 1.6002 - val_auc: 0.7103\n",
      "Epoch 349/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4795 - auc: 0.6918 - val_loss: 1.4800 - val_auc: 0.7103\n",
      "Epoch 350/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4755 - auc: 0.6780 - val_loss: 1.3638 - val_auc: 0.7104\n",
      "Epoch 351/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4891 - auc: 0.6829 - val_loss: 1.5733 - val_auc: 0.7104\n",
      "Epoch 352/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4784 - auc: 0.6879 - val_loss: 1.5262 - val_auc: 0.7104\n",
      "Epoch 353/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4733 - auc: 0.6793 - val_loss: 1.3552 - val_auc: 0.7104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4896 - auc: 0.6678 - val_loss: 1.6373 - val_auc: 0.7105\n",
      "Epoch 355/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4928 - auc: 0.6809 - val_loss: 1.5345 - val_auc: 0.7105\n",
      "Epoch 356/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4819 - auc: 0.6799 - val_loss: 1.3537 - val_auc: 0.7107\n",
      "Epoch 357/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4965 - auc: 0.6781 - val_loss: 1.5284 - val_auc: 0.7108\n",
      "Epoch 358/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4808 - auc: 0.6767 - val_loss: 1.5531 - val_auc: 0.7107\n",
      "Epoch 359/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4793 - auc: 0.6742 - val_loss: 1.4072 - val_auc: 0.7107\n",
      "Epoch 360/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4737 - auc: 0.6884 - val_loss: 1.4482 - val_auc: 0.7108\n",
      "Epoch 361/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4744 - auc: 0.6848 - val_loss: 1.5553 - val_auc: 0.7109\n",
      "Epoch 362/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4775 - auc: 0.6772 - val_loss: 1.4562 - val_auc: 0.7109\n",
      "Epoch 363/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4713 - auc: 0.6852 - val_loss: 1.4877 - val_auc: 0.7108\n",
      "Epoch 364/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4665 - auc: 0.6841 - val_loss: 1.4957 - val_auc: 0.7108\n",
      "Epoch 365/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4683 - auc: 0.6854 - val_loss: 1.4246 - val_auc: 0.7109\n",
      "Epoch 366/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4693 - auc: 0.6832 - val_loss: 1.5076 - val_auc: 0.7109\n",
      "Epoch 367/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4694 - auc: 0.6838 - val_loss: 1.4428 - val_auc: 0.7110\n",
      "Epoch 368/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4679 - auc: 0.6836 - val_loss: 1.4976 - val_auc: 0.7112\n",
      "Epoch 369/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4668 - auc: 0.6856 - val_loss: 1.4187 - val_auc: 0.7112\n",
      "Epoch 370/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4710 - auc: 0.6782 - val_loss: 1.5390 - val_auc: 0.7112\n",
      "Epoch 371/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4698 - auc: 0.6853 - val_loss: 1.3981 - val_auc: 0.7112\n",
      "Epoch 372/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4683 - auc: 0.6868 - val_loss: 1.6242 - val_auc: 0.7112\n",
      "Epoch 373/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4882 - auc: 0.6770 - val_loss: 1.3408 - val_auc: 0.7112\n",
      "Epoch 374/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4887 - auc: 0.6821 - val_loss: 1.5850 - val_auc: 0.7114\n",
      "Epoch 375/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4842 - auc: 0.6825 - val_loss: 1.5100 - val_auc: 0.7114\n",
      "Epoch 376/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4708 - auc: 0.6916 - val_loss: 1.3505 - val_auc: 0.7116\n",
      "Epoch 377/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4838 - auc: 0.6718 - val_loss: 1.6189 - val_auc: 0.7114\n",
      "Epoch 378/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4846 - auc: 0.6776 - val_loss: 1.4662 - val_auc: 0.7115\n",
      "Epoch 379/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4799 - auc: 0.6847 - val_loss: 1.4043 - val_auc: 0.7114\n",
      "Epoch 380/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4898 - auc: 0.6756 - val_loss: 1.5466 - val_auc: 0.7112\n",
      "Epoch 381/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4811 - auc: 0.6782 - val_loss: 1.5354 - val_auc: 0.7113\n",
      "Epoch 382/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4693 - auc: 0.6833 - val_loss: 1.3946 - val_auc: 0.7111\n",
      "Epoch 383/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4726 - auc: 0.6823 - val_loss: 1.5676 - val_auc: 0.7113\n",
      "Epoch 384/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4797 - auc: 0.6875 - val_loss: 1.5106 - val_auc: 0.7112\n",
      "Epoch 385/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4802 - auc: 0.6824 - val_loss: 1.3876 - val_auc: 0.7113\n",
      "Epoch 386/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4805 - auc: 0.6844 - val_loss: 1.5180 - val_auc: 0.7113\n",
      "Epoch 387/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4760 - auc: 0.6750 - val_loss: 1.5151 - val_auc: 0.7114\n",
      "Epoch 388/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4658 - auc: 0.6849 - val_loss: 1.3706 - val_auc: 0.7114\n",
      "Epoch 389/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4768 - auc: 0.6790 - val_loss: 1.5307 - val_auc: 0.7115\n",
      "Epoch 390/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4747 - auc: 0.6868 - val_loss: 1.5009 - val_auc: 0.7115\n",
      "Epoch 391/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4731 - auc: 0.6830 - val_loss: 1.3891 - val_auc: 0.7116\n",
      "Epoch 392/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4753 - auc: 0.6817 - val_loss: 1.6608 - val_auc: 0.7115\n",
      "Epoch 393/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4892 - auc: 0.6811 - val_loss: 1.4148 - val_auc: 0.7117\n",
      "Epoch 394/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4626 - auc: 0.6896 - val_loss: 1.3766 - val_auc: 0.7117\n",
      "Epoch 395/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4709 - auc: 0.6925 - val_loss: 1.6294 - val_auc: 0.7116\n",
      "Epoch 396/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4862 - auc: 0.6814 - val_loss: 1.4713 - val_auc: 0.7118\n",
      "Epoch 397/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4681 - auc: 0.6811 - val_loss: 1.3394 - val_auc: 0.7118\n",
      "Epoch 398/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4830 - auc: 0.6849 - val_loss: 1.6641 - val_auc: 0.7117\n",
      "Epoch 399/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4965 - auc: 0.6809 - val_loss: 1.5492 - val_auc: 0.7119\n",
      "Epoch 400/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4820 - auc: 0.6838 - val_loss: 1.3303 - val_auc: 0.7122\n",
      "Epoch 401/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4922 - auc: 0.6784 - val_loss: 1.4638 - val_auc: 0.7122\n",
      "Epoch 402/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4699 - auc: 0.6759 - val_loss: 1.5931 - val_auc: 0.7122\n",
      "Epoch 403/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4810 - auc: 0.6775 - val_loss: 1.4438 - val_auc: 0.7119\n",
      "Epoch 404/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4718 - auc: 0.6821 - val_loss: 1.4093 - val_auc: 0.7120\n",
      "Epoch 405/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4816 - auc: 0.6785 - val_loss: 1.5407 - val_auc: 0.7119\n",
      "Epoch 406/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4788 - auc: 0.6765 - val_loss: 1.5361 - val_auc: 0.7119\n",
      "Epoch 407/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4719 - auc: 0.6854 - val_loss: 1.3799 - val_auc: 0.7120\n",
      "Epoch 408/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4862 - auc: 0.6699 - val_loss: 1.6504 - val_auc: 0.7121\n",
      "Epoch 409/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4947 - auc: 0.6866 - val_loss: 1.5183 - val_auc: 0.7121\n",
      "Epoch 410/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4873 - auc: 0.6864 - val_loss: 1.3683 - val_auc: 0.7123\n",
      "Epoch 411/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5011 - auc: 0.6739 - val_loss: 1.4223 - val_auc: 0.7126\n",
      "Epoch 412/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4770 - auc: 0.6898 - val_loss: 1.6162 - val_auc: 0.7005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4884 - auc: 0.6686 - val_loss: 1.4073 - val_auc: 0.7121\n",
      "Epoch 414/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4700 - auc: 0.6790 - val_loss: 1.4232 - val_auc: 0.7121\n",
      "Epoch 415/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4743 - auc: 0.6852 - val_loss: 1.5866 - val_auc: 0.7118\n",
      "Epoch 416/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4846 - auc: 0.6835 - val_loss: 1.4329 - val_auc: 0.7118\n",
      "Epoch 417/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4758 - auc: 0.6731 - val_loss: 1.5500 - val_auc: 0.7119\n",
      "Epoch 418/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4642 - auc: 0.6840 - val_loss: 1.4032 - val_auc: 0.7120\n",
      "Epoch 419/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4660 - auc: 0.6774 - val_loss: 1.5493 - val_auc: 0.7120\n",
      "Epoch 420/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4710 - auc: 0.6874 - val_loss: 1.4371 - val_auc: 0.7122\n",
      "Epoch 421/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4739 - auc: 0.6820 - val_loss: 1.3824 - val_auc: 0.7124\n",
      "Epoch 422/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4758 - auc: 0.6770 - val_loss: 1.5872 - val_auc: 0.7119\n",
      "Epoch 423/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4784 - auc: 0.6785 - val_loss: 1.4368 - val_auc: 0.7126\n",
      "Epoch 424/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4651 - auc: 0.6822 - val_loss: 1.3766 - val_auc: 0.7127\n",
      "Epoch 425/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4703 - auc: 0.6845 - val_loss: 1.5901 - val_auc: 0.7126\n",
      "Epoch 426/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4726 - auc: 0.6915 - val_loss: 1.4703 - val_auc: 0.7127\n",
      "Epoch 427/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4654 - auc: 0.6839 - val_loss: 1.3525 - val_auc: 0.7127\n",
      "Epoch 428/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4740 - auc: 0.6752 - val_loss: 1.7360 - val_auc: 0.7126\n",
      "Epoch 429/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5096 - auc: 0.6849 - val_loss: 1.5274 - val_auc: 0.7131\n",
      "Epoch 430/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4856 - auc: 0.6872 - val_loss: 1.3474 - val_auc: 0.7132\n",
      "Epoch 431/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.5027 - auc: 0.6806 - val_loss: 1.4286 - val_auc: 0.7130\n",
      "Epoch 432/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4804 - auc: 0.6834 - val_loss: 1.6301 - val_auc: 0.6938\n",
      "Epoch 433/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4915 - auc: 0.6671 - val_loss: 1.4780 - val_auc: 0.7128\n",
      "Epoch 434/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4600 - auc: 0.6883 - val_loss: 1.4072 - val_auc: 0.7125\n",
      "Epoch 435/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4807 - auc: 0.6855 - val_loss: 1.5247 - val_auc: 0.7124\n",
      "Epoch 436/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4848 - auc: 0.6890 - val_loss: 1.5419 - val_auc: 0.7125\n",
      "Epoch 437/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4816 - auc: 0.6865 - val_loss: 1.4035 - val_auc: 0.7125\n",
      "Epoch 438/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4826 - auc: 0.6696 - val_loss: 1.6281 - val_auc: 0.7125\n",
      "Epoch 439/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4868 - auc: 0.6818 - val_loss: 1.4832 - val_auc: 0.7126\n",
      "Epoch 440/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4696 - auc: 0.6871 - val_loss: 1.3193 - val_auc: 0.7128\n",
      "Epoch 441/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4928 - auc: 0.6871 - val_loss: 1.5306 - val_auc: 0.7127\n",
      "Epoch 442/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4774 - auc: 0.6872 - val_loss: 1.5745 - val_auc: 0.7127\n",
      "Epoch 443/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4861 - auc: 0.6909 - val_loss: 1.4126 - val_auc: 0.7130\n",
      "Epoch 444/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4828 - auc: 0.6804 - val_loss: 1.3741 - val_auc: 0.7132\n",
      "Epoch 445/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4772 - auc: 0.6839 - val_loss: 1.6133 - val_auc: 0.7128\n",
      "Epoch 446/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4802 - auc: 0.6822 - val_loss: 1.4998 - val_auc: 0.7129\n",
      "Epoch 447/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4653 - auc: 0.6887 - val_loss: 1.3346 - val_auc: 0.7131\n",
      "Epoch 448/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4877 - auc: 0.6841 - val_loss: 1.6352 - val_auc: 0.7128\n",
      "Epoch 449/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4899 - auc: 0.6916 - val_loss: 1.6155 - val_auc: 0.7130\n",
      "Epoch 450/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4918 - auc: 0.6827 - val_loss: 1.3998 - val_auc: 0.7134\n",
      "Epoch 451/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4804 - auc: 0.6827 - val_loss: 1.3725 - val_auc: 0.7134\n",
      "Epoch 452/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4813 - auc: 0.6774 - val_loss: 1.6443 - val_auc: 0.7134\n",
      "Epoch 453/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4898 - auc: 0.6747 - val_loss: 1.4941 - val_auc: 0.7133\n",
      "Epoch 454/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4723 - auc: 0.6880 - val_loss: 1.4013 - val_auc: 0.7133\n",
      "Epoch 455/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4889 - auc: 0.6818 - val_loss: 1.4309 - val_auc: 0.7131\n",
      "Epoch 456/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4783 - auc: 0.6810 - val_loss: 1.5586 - val_auc: 0.7129\n",
      "Epoch 457/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4713 - auc: 0.6887 - val_loss: 1.5013 - val_auc: 0.7127\n",
      "Epoch 458/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4664 - auc: 0.6789 - val_loss: 1.4551 - val_auc: 0.7127\n",
      "Epoch 459/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4589 - auc: 0.6839 - val_loss: 1.5102 - val_auc: 0.7126\n",
      "Epoch 460/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4662 - auc: 0.6854 - val_loss: 1.4381 - val_auc: 0.7128\n",
      "Epoch 461/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4676 - auc: 0.6935 - val_loss: 1.4352 - val_auc: 0.7129\n",
      "Epoch 462/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4661 - auc: 0.6823 - val_loss: 1.5235 - val_auc: 0.7128\n",
      "Epoch 463/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4626 - auc: 0.6858 - val_loss: 1.3961 - val_auc: 0.7130\n",
      "Epoch 464/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4590 - auc: 0.6849 - val_loss: 1.5353 - val_auc: 0.7131\n",
      "Epoch 465/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4615 - auc: 0.6878 - val_loss: 1.3968 - val_auc: 0.7132\n",
      "Epoch 466/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4628 - auc: 0.6785 - val_loss: 1.4801 - val_auc: 0.7131\n",
      "Epoch 467/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4569 - auc: 0.6906 - val_loss: 1.4477 - val_auc: 0.7132\n",
      "Epoch 468/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4515 - auc: 0.6914 - val_loss: 1.4305 - val_auc: 0.7131\n",
      "Epoch 469/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4571 - auc: 0.6825 - val_loss: 1.5359 - val_auc: 0.7131\n",
      "Epoch 470/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4545 - auc: 0.6889 - val_loss: 1.3164 - val_auc: 0.7134\n",
      "Epoch 471/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4688 - auc: 0.6826 - val_loss: 1.7804 - val_auc: 0.7131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5248 - auc: 0.6860 - val_loss: 1.6006 - val_auc: 0.7137\n",
      "Epoch 473/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5042 - auc: 0.6848 - val_loss: 1.3832 - val_auc: 0.7139\n",
      "Epoch 474/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4987 - auc: 0.6847 - val_loss: 1.4131 - val_auc: 0.7138\n",
      "Epoch 475/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4957 - auc: 0.6509 - val_loss: 1.6021 - val_auc: 0.6728\n",
      "Epoch 476/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4985 - auc: 0.6344 - val_loss: 1.4396 - val_auc: 0.7139\n",
      "Epoch 477/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4691 - auc: 0.6764 - val_loss: 1.4437 - val_auc: 0.7139\n",
      "Epoch 478/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4762 - auc: 0.6877 - val_loss: 1.5161 - val_auc: 0.7134\n",
      "Epoch 479/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4879 - auc: 0.6865 - val_loss: 1.5525 - val_auc: 0.7129\n",
      "Epoch 480/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4884 - auc: 0.6814 - val_loss: 1.4786 - val_auc: 0.7128\n",
      "Epoch 481/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4772 - auc: 0.6816 - val_loss: 1.5116 - val_auc: 0.7128\n",
      "Epoch 482/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4603 - auc: 0.6864 - val_loss: 1.4648 - val_auc: 0.7127\n",
      "Epoch 483/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4579 - auc: 0.6884 - val_loss: 1.4220 - val_auc: 0.7127\n",
      "Epoch 484/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4627 - auc: 0.6873 - val_loss: 1.5001 - val_auc: 0.7127\n",
      "Epoch 485/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4625 - auc: 0.6899 - val_loss: 1.4544 - val_auc: 0.7127\n",
      "Epoch 486/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4582 - auc: 0.6901 - val_loss: 1.3743 - val_auc: 0.7127\n",
      "Epoch 487/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4659 - auc: 0.6781 - val_loss: 1.6406 - val_auc: 0.7111\n",
      "Epoch 488/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4737 - auc: 0.6980 - val_loss: 1.4316 - val_auc: 0.7129\n",
      "Epoch 489/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4688 - auc: 0.6883 - val_loss: 1.3175 - val_auc: 0.7130\n",
      "Epoch 490/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4908 - auc: 0.6776 - val_loss: 1.6403 - val_auc: 0.7131\n",
      "Epoch 491/1000\n",
      "1850/1850 [==============================] - 0s 35us/step - loss: 1.4911 - auc: 0.6818 - val_loss: 1.5435 - val_auc: 0.7132\n",
      "Epoch 492/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4654 - auc: 0.6892 - val_loss: 1.2729 - val_auc: 0.7135\n",
      "Epoch 493/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5136 - auc: 0.6664 - val_loss: 1.7048 - val_auc: 0.7133\n",
      "Epoch 494/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5180 - auc: 0.6906 - val_loss: 1.6835 - val_auc: 0.7131\n",
      "Epoch 495/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.5459 - auc: 0.6911 - val_loss: 1.5165 - val_auc: 0.7134\n",
      "Epoch 496/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5449 - auc: 0.6810 - val_loss: 1.4457 - val_auc: 0.7140\n",
      "Epoch 497/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5327 - auc: 0.6741 - val_loss: 1.5228 - val_auc: 0.7142\n",
      "Epoch 498/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.5121 - auc: 0.6697 - val_loss: 1.4971 - val_auc: 0.7141\n",
      "Epoch 499/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5096 - auc: 0.6391 - val_loss: 1.4544 - val_auc: 0.7140\n",
      "Epoch 500/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4869 - auc: 0.6801 - val_loss: 1.4933 - val_auc: 0.7137\n",
      "Epoch 501/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4949 - auc: 0.6793 - val_loss: 1.5399 - val_auc: 0.7134\n",
      "Epoch 502/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4997 - auc: 0.6860 - val_loss: 1.5425 - val_auc: 0.7132\n",
      "Epoch 503/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4939 - auc: 0.6897 - val_loss: 1.4970 - val_auc: 0.7131\n",
      "Epoch 504/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4829 - auc: 0.6913 - val_loss: 1.4321 - val_auc: 0.7131\n",
      "Epoch 505/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4743 - auc: 0.6872 - val_loss: 1.5168 - val_auc: 0.7131\n",
      "Epoch 506/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4642 - auc: 0.6919 - val_loss: 1.4915 - val_auc: 0.7131\n",
      "Epoch 507/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4656 - auc: 0.6891 - val_loss: 1.4427 - val_auc: 0.7133\n",
      "Epoch 508/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4666 - auc: 0.6883 - val_loss: 1.4420 - val_auc: 0.7134\n",
      "Epoch 509/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4643 - auc: 0.6833 - val_loss: 1.5290 - val_auc: 0.7132\n",
      "Epoch 510/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4643 - auc: 0.6825 - val_loss: 1.3794 - val_auc: 0.7133\n",
      "Epoch 511/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4687 - auc: 0.6848 - val_loss: 1.5629 - val_auc: 0.7133\n",
      "Epoch 512/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4731 - auc: 0.6887 - val_loss: 1.4461 - val_auc: 0.7133\n",
      "Epoch 513/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4658 - auc: 0.6873 - val_loss: 1.3756 - val_auc: 0.7135\n",
      "Epoch 514/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4657 - auc: 0.6859 - val_loss: 1.8003 - val_auc: 0.7112\n",
      "Epoch 515/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5283 - auc: 0.6801 - val_loss: 1.4858 - val_auc: 0.7136\n",
      "Epoch 516/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4920 - auc: 0.6802 - val_loss: 1.3154 - val_auc: 0.7140\n",
      "Epoch 517/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5223 - auc: 0.6881 - val_loss: 1.4449 - val_auc: 0.7137\n",
      "Epoch 518/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4832 - auc: 0.6930 - val_loss: 1.6309 - val_auc: 0.7140\n",
      "Epoch 519/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4880 - auc: 0.6916 - val_loss: 1.4991 - val_auc: 0.7140\n",
      "Epoch 520/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4729 - auc: 0.6784 - val_loss: 1.3583 - val_auc: 0.7139\n",
      "Epoch 521/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4817 - auc: 0.6662 - val_loss: 1.6052 - val_auc: 0.7139\n",
      "Epoch 522/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4862 - auc: 0.6906 - val_loss: 1.5955 - val_auc: 0.7139\n",
      "Epoch 523/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5063 - auc: 0.6903 - val_loss: 1.4632 - val_auc: 0.7139\n",
      "Epoch 524/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5009 - auc: 0.6843 - val_loss: 1.3765 - val_auc: 0.7141\n",
      "Epoch 525/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4979 - auc: 0.6772 - val_loss: 1.4282 - val_auc: 0.7141\n",
      "Epoch 526/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4779 - auc: 0.6783 - val_loss: 1.5339 - val_auc: 0.7141\n",
      "Epoch 527/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4716 - auc: 0.6816 - val_loss: 1.5069 - val_auc: 0.7139\n",
      "Epoch 528/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4659 - auc: 0.6820 - val_loss: 1.4481 - val_auc: 0.7140\n",
      "Epoch 529/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4657 - auc: 0.6995 - val_loss: 1.4281 - val_auc: 0.7141\n",
      "Epoch 530/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4732 - auc: 0.6852 - val_loss: 1.5063 - val_auc: 0.7141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4705 - auc: 0.6800 - val_loss: 1.5181 - val_auc: 0.7140\n",
      "Epoch 532/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4655 - auc: 0.6773 - val_loss: 1.4081 - val_auc: 0.7141\n",
      "Epoch 533/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4616 - auc: 0.6793 - val_loss: 1.5944 - val_auc: 0.7140\n",
      "Epoch 534/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4684 - auc: 0.6948 - val_loss: 1.4937 - val_auc: 0.7141\n",
      "Epoch 535/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4724 - auc: 0.6944 - val_loss: 1.3857 - val_auc: 0.7142\n",
      "Epoch 536/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4784 - auc: 0.6911 - val_loss: 1.4774 - val_auc: 0.7020\n",
      "Epoch 537/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4657 - auc: 0.6829 - val_loss: 1.5042 - val_auc: 0.7027\n",
      "Epoch 538/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4523 - auc: 0.6944 - val_loss: 1.3431 - val_auc: 0.7142\n",
      "Epoch 539/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4690 - auc: 0.6789 - val_loss: 1.5486 - val_auc: 0.7141\n",
      "Epoch 540/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4763 - auc: 0.6888 - val_loss: 1.5610 - val_auc: 0.7141\n",
      "Epoch 541/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4799 - auc: 0.6855 - val_loss: 1.3424 - val_auc: 0.7141\n",
      "Epoch 542/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4830 - auc: 0.6851 - val_loss: 1.7455 - val_auc: 0.7140\n",
      "Epoch 543/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5131 - auc: 0.6907 - val_loss: 1.5960 - val_auc: 0.7142\n",
      "Epoch 544/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4922 - auc: 0.6919 - val_loss: 1.3802 - val_auc: 0.7144\n",
      "Epoch 545/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4970 - auc: 0.6855 - val_loss: 1.3247 - val_auc: 0.7146\n",
      "Epoch 546/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5017 - auc: 0.6827 - val_loss: 1.4927 - val_auc: 0.6957\n",
      "Epoch 547/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4813 - auc: 0.6698 - val_loss: 1.5882 - val_auc: 0.6993\n",
      "Epoch 548/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4838 - auc: 0.6714 - val_loss: 1.4918 - val_auc: 0.7144\n",
      "Epoch 549/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4688 - auc: 0.6888 - val_loss: 1.4276 - val_auc: 0.7142\n",
      "Epoch 550/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4768 - auc: 0.6918 - val_loss: 1.4451 - val_auc: 0.7141\n",
      "Epoch 551/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4755 - auc: 0.6957 - val_loss: 1.5245 - val_auc: 0.7141\n",
      "Epoch 552/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4709 - auc: 0.6857 - val_loss: 1.5333 - val_auc: 0.7140\n",
      "Epoch 553/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4614 - auc: 0.6917 - val_loss: 1.3990 - val_auc: 0.7142\n",
      "Epoch 554/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4601 - auc: 0.6741 - val_loss: 1.5526 - val_auc: 0.7141\n",
      "Epoch 555/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4560 - auc: 0.6937 - val_loss: 1.4914 - val_auc: 0.7142\n",
      "Epoch 556/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4586 - auc: 0.6911 - val_loss: 1.3633 - val_auc: 0.7143\n",
      "Epoch 557/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4630 - auc: 0.6905 - val_loss: 1.4173 - val_auc: 0.7143\n",
      "Epoch 558/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4501 - auc: 0.6947 - val_loss: 1.5390 - val_auc: 0.7113\n",
      "Epoch 559/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4580 - auc: 0.6878 - val_loss: 1.4106 - val_auc: 0.7142\n",
      "Epoch 560/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4497 - auc: 0.6932 - val_loss: 1.4057 - val_auc: 0.7141\n",
      "Epoch 561/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4513 - auc: 0.6887 - val_loss: 1.5004 - val_auc: 0.7141\n",
      "Epoch 562/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4539 - auc: 0.6789 - val_loss: 1.4234 - val_auc: 0.7142\n",
      "Epoch 563/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4494 - auc: 0.6888 - val_loss: 1.4841 - val_auc: 0.7143\n",
      "Epoch 564/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4458 - auc: 0.6934 - val_loss: 1.4189 - val_auc: 0.7143\n",
      "Epoch 565/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4470 - auc: 0.6872 - val_loss: 1.5880 - val_auc: 0.7144\n",
      "Epoch 566/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4566 - auc: 0.6937 - val_loss: 1.3728 - val_auc: 0.7143\n",
      "Epoch 567/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4530 - auc: 0.6922 - val_loss: 1.4670 - val_auc: 0.7144\n",
      "Epoch 568/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4530 - auc: 0.6873 - val_loss: 1.4922 - val_auc: 0.7146\n",
      "Epoch 569/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4531 - auc: 0.6895 - val_loss: 1.4224 - val_auc: 0.7147\n",
      "Epoch 570/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4509 - auc: 0.6842 - val_loss: 1.4321 - val_auc: 0.7147\n",
      "Epoch 571/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4441 - auc: 0.6900 - val_loss: 1.4854 - val_auc: 0.7148\n",
      "Epoch 572/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4447 - auc: 0.6960 - val_loss: 1.4402 - val_auc: 0.7148\n",
      "Epoch 573/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4486 - auc: 0.6877 - val_loss: 1.3935 - val_auc: 0.7148\n",
      "Epoch 574/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4458 - auc: 0.6931 - val_loss: 1.5652 - val_auc: 0.7148\n",
      "Epoch 575/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4531 - auc: 0.6908 - val_loss: 1.4350 - val_auc: 0.7148\n",
      "Epoch 576/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4465 - auc: 0.6910 - val_loss: 1.3905 - val_auc: 0.7149\n",
      "Epoch 577/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4443 - auc: 0.6918 - val_loss: 1.5437 - val_auc: 0.7149\n",
      "Epoch 578/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4418 - auc: 0.6970 - val_loss: 1.4338 - val_auc: 0.7149\n",
      "Epoch 579/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4408 - auc: 0.6946 - val_loss: 1.3630 - val_auc: 0.7148\n",
      "Epoch 580/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4580 - auc: 0.6770 - val_loss: 1.6259 - val_auc: 0.7149\n",
      "Epoch 581/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4669 - auc: 0.6951 - val_loss: 1.4976 - val_auc: 0.7147\n",
      "Epoch 582/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4525 - auc: 0.6949 - val_loss: 1.3294 - val_auc: 0.7148\n",
      "Epoch 583/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4626 - auc: 0.6942 - val_loss: 1.4379 - val_auc: 0.7149\n",
      "Epoch 584/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4504 - auc: 0.6908 - val_loss: 1.5843 - val_auc: 0.7114\n",
      "Epoch 585/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4585 - auc: 0.6972 - val_loss: 1.4107 - val_auc: 0.7149\n",
      "Epoch 586/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4439 - auc: 0.6896 - val_loss: 1.3956 - val_auc: 0.7150\n",
      "Epoch 587/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4459 - auc: 0.6945 - val_loss: 1.5320 - val_auc: 0.7148\n",
      "Epoch 588/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4511 - auc: 0.6907 - val_loss: 1.4914 - val_auc: 0.7147\n",
      "Epoch 589/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4540 - auc: 0.6843 - val_loss: 1.3226 - val_auc: 0.7149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4729 - auc: 0.6859 - val_loss: 1.7689 - val_auc: 0.7147\n",
      "Epoch 591/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5211 - auc: 0.6888 - val_loss: 1.6451 - val_auc: 0.7148\n",
      "Epoch 592/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5135 - auc: 0.6979 - val_loss: 1.4305 - val_auc: 0.7146\n",
      "Epoch 593/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5189 - auc: 0.6790 - val_loss: 1.3543 - val_auc: 0.7148\n",
      "Epoch 594/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5164 - auc: 0.6773 - val_loss: 1.4805 - val_auc: 0.6890\n",
      "Epoch 595/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4992 - auc: 0.6508 - val_loss: 1.5380 - val_auc: 0.6768\n",
      "Epoch 596/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4847 - auc: 0.6467 - val_loss: 1.4483 - val_auc: 0.7147\n",
      "Epoch 597/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4636 - auc: 0.6887 - val_loss: 1.4667 - val_auc: 0.7148\n",
      "Epoch 598/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4746 - auc: 0.6884 - val_loss: 1.5262 - val_auc: 0.7142\n",
      "Epoch 599/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4823 - auc: 0.6924 - val_loss: 1.5251 - val_auc: 0.7141\n",
      "Epoch 600/1000\n",
      "1850/1850 [==============================] - 0s 28us/step - loss: 1.4803 - auc: 0.6899 - val_loss: 1.4303 - val_auc: 0.7139\n",
      "Epoch 601/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4770 - auc: 0.6801 - val_loss: 1.6344 - val_auc: 0.7138\n",
      "Epoch 602/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4801 - auc: 0.6877 - val_loss: 1.5016 - val_auc: 0.7141\n",
      "Epoch 603/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4617 - auc: 0.6936 - val_loss: 1.2999 - val_auc: 0.7144\n",
      "Epoch 604/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4888 - auc: 0.6894 - val_loss: 1.5783 - val_auc: 0.7143\n",
      "Epoch 605/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4812 - auc: 0.6997 - val_loss: 1.6153 - val_auc: 0.7146\n",
      "Epoch 606/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5030 - auc: 0.6908 - val_loss: 1.4577 - val_auc: 0.7148\n",
      "Epoch 607/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4944 - auc: 0.6979 - val_loss: 1.3677 - val_auc: 0.7146\n",
      "Epoch 608/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4971 - auc: 0.6747 - val_loss: 1.4783 - val_auc: 0.6743\n",
      "Epoch 609/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4841 - auc: 0.6592 - val_loss: 1.4653 - val_auc: 0.7012\n",
      "Epoch 610/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4621 - auc: 0.6727 - val_loss: 1.4387 - val_auc: 0.7148\n",
      "Epoch 611/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4560 - auc: 0.6956 - val_loss: 1.4925 - val_auc: 0.7147\n",
      "Epoch 612/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4643 - auc: 0.6911 - val_loss: 1.4884 - val_auc: 0.7149\n",
      "Epoch 613/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4594 - auc: 0.6930 - val_loss: 1.4387 - val_auc: 0.7148\n",
      "Epoch 614/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4661 - auc: 0.6748 - val_loss: 1.6795 - val_auc: 0.7148\n",
      "Epoch 615/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4821 - auc: 0.6899 - val_loss: 1.4878 - val_auc: 0.7148\n",
      "Epoch 616/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4601 - auc: 0.6886 - val_loss: 1.2967 - val_auc: 0.7147\n",
      "Epoch 617/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4957 - auc: 0.6815 - val_loss: 1.6321 - val_auc: 0.7149\n",
      "Epoch 618/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4924 - auc: 0.6955 - val_loss: 1.6483 - val_auc: 0.7146\n",
      "Epoch 619/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5216 - auc: 0.6900 - val_loss: 1.4900 - val_auc: 0.7147\n",
      "Epoch 620/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5133 - auc: 0.6906 - val_loss: 1.4115 - val_auc: 0.7150\n",
      "Epoch 621/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.5079 - auc: 0.6751 - val_loss: 1.4762 - val_auc: 0.6719\n",
      "Epoch 622/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4973 - auc: 0.6491 - val_loss: 1.4542 - val_auc: 0.6925\n",
      "Epoch 623/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4901 - auc: 0.6458 - val_loss: 1.4114 - val_auc: 0.7150\n",
      "Epoch 624/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4799 - auc: 0.6819 - val_loss: 1.4866 - val_auc: 0.7149\n",
      "Epoch 625/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4707 - auc: 0.6960 - val_loss: 1.5634 - val_auc: 0.7146\n",
      "Epoch 626/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4755 - auc: 0.6985 - val_loss: 1.5358 - val_auc: 0.7144\n",
      "Epoch 627/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4739 - auc: 0.6892 - val_loss: 1.4088 - val_auc: 0.7144\n",
      "Epoch 628/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4735 - auc: 0.6839 - val_loss: 1.5618 - val_auc: 0.7141\n",
      "Epoch 629/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4639 - auc: 0.6933 - val_loss: 1.5108 - val_auc: 0.7141\n",
      "Epoch 630/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4576 - auc: 0.6993 - val_loss: 1.3593 - val_auc: 0.7142\n",
      "Epoch 631/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4759 - auc: 0.6786 - val_loss: 1.5811 - val_auc: 0.7141\n",
      "Epoch 632/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4714 - auc: 0.6916 - val_loss: 1.5196 - val_auc: 0.7144\n",
      "Epoch 633/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4624 - auc: 0.6930 - val_loss: 1.3913 - val_auc: 0.7149\n",
      "Epoch 634/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4596 - auc: 0.6939 - val_loss: 1.3937 - val_auc: 0.7148\n",
      "Epoch 635/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4593 - auc: 0.6824 - val_loss: 1.6008 - val_auc: 0.7024\n",
      "Epoch 636/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4721 - auc: 0.6792 - val_loss: 1.3480 - val_auc: 0.7151\n",
      "Epoch 637/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4659 - auc: 0.6937 - val_loss: 1.4192 - val_auc: 0.7151\n",
      "Epoch 638/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4662 - auc: 0.7009 - val_loss: 1.5370 - val_auc: 0.7151\n",
      "Epoch 639/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4701 - auc: 0.6986 - val_loss: 1.4665 - val_auc: 0.7148\n",
      "Epoch 640/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4598 - auc: 0.6858 - val_loss: 1.3562 - val_auc: 0.7146\n",
      "Epoch 641/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4718 - auc: 0.6694 - val_loss: 1.8980 - val_auc: 0.7146\n",
      "Epoch 642/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5687 - auc: 0.6873 - val_loss: 1.8079 - val_auc: 0.7149\n",
      "Epoch 643/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5620 - auc: 0.6954 - val_loss: 1.4754 - val_auc: 0.7144\n",
      "Epoch 644/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5188 - auc: 0.6972 - val_loss: 1.3180 - val_auc: 0.7144\n",
      "Epoch 645/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5552 - auc: 0.6823 - val_loss: 1.2981 - val_auc: 0.7143\n",
      "Epoch 646/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5374 - auc: 0.6779 - val_loss: 1.4331 - val_auc: 0.6895\n",
      "Epoch 647/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4907 - auc: 0.6561 - val_loss: 1.6293 - val_auc: 0.6844\n",
      "Epoch 648/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5048 - auc: 0.6449 - val_loss: 1.5601 - val_auc: 0.7141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4792 - auc: 0.6927 - val_loss: 1.4963 - val_auc: 0.7141\n",
      "Epoch 650/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4904 - auc: 0.6758 - val_loss: 1.4671 - val_auc: 0.7141\n",
      "Epoch 651/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5050 - auc: 0.6632 - val_loss: 1.4752 - val_auc: 0.7142\n",
      "Epoch 652/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4842 - auc: 0.6878 - val_loss: 1.5435 - val_auc: 0.7143\n",
      "Epoch 653/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4722 - auc: 0.6917 - val_loss: 1.5459 - val_auc: 0.7145\n",
      "Epoch 654/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4648 - auc: 0.6899 - val_loss: 1.3467 - val_auc: 0.7143\n",
      "Epoch 655/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4775 - auc: 0.6870 - val_loss: 1.7629 - val_auc: 0.7143\n",
      "Epoch 656/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5418 - auc: 0.6909 - val_loss: 1.7494 - val_auc: 0.7145\n",
      "Epoch 657/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5654 - auc: 0.6987 - val_loss: 1.5501 - val_auc: 0.7141\n",
      "Epoch 658/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5511 - auc: 0.6901 - val_loss: 1.4185 - val_auc: 0.7142\n",
      "Epoch 659/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5511 - auc: 0.6688 - val_loss: 1.4396 - val_auc: 0.6459\n",
      "Epoch 660/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5273 - auc: 0.6447 - val_loss: 1.6116 - val_auc: 0.6040\n",
      "Epoch 661/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5432 - auc: 0.5605 - val_loss: 1.3123 - val_auc: 0.7151\n",
      "Epoch 662/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5170 - auc: 0.6652 - val_loss: 1.3870 - val_auc: 0.7149\n",
      "Epoch 663/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4966 - auc: 0.6919 - val_loss: 1.5276 - val_auc: 0.7145\n",
      "Epoch 664/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4999 - auc: 0.6932 - val_loss: 1.6244 - val_auc: 0.7150\n",
      "Epoch 665/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5110 - auc: 0.6818 - val_loss: 1.5830 - val_auc: 0.7147\n",
      "Epoch 666/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4915 - auc: 0.6855 - val_loss: 1.4270 - val_auc: 0.7150\n",
      "Epoch 667/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4924 - auc: 0.6688 - val_loss: 1.5078 - val_auc: 0.7148\n",
      "Epoch 668/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4741 - auc: 0.6848 - val_loss: 1.5266 - val_auc: 0.7149\n",
      "Epoch 669/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4745 - auc: 0.6945 - val_loss: 1.4836 - val_auc: 0.7149\n",
      "Epoch 670/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4705 - auc: 0.6923 - val_loss: 1.4246 - val_auc: 0.7149\n",
      "Epoch 671/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4729 - auc: 0.6928 - val_loss: 1.4265 - val_auc: 0.7150\n",
      "Epoch 672/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4676 - auc: 0.6874 - val_loss: 1.4681 - val_auc: 0.7150\n",
      "Epoch 673/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4555 - auc: 0.6883 - val_loss: 1.4547 - val_auc: 0.7150\n",
      "Epoch 674/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4508 - auc: 0.6900 - val_loss: 1.4340 - val_auc: 0.7151\n",
      "Epoch 675/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4532 - auc: 0.6980 - val_loss: 1.4471 - val_auc: 0.7154\n",
      "Epoch 676/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4546 - auc: 0.6863 - val_loss: 1.4642 - val_auc: 0.7154\n",
      "Epoch 677/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4499 - auc: 0.6841 - val_loss: 1.4295 - val_auc: 0.7154\n",
      "Epoch 678/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4454 - auc: 0.6899 - val_loss: 1.4433 - val_auc: 0.7154\n",
      "Epoch 679/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4407 - auc: 0.6945 - val_loss: 1.4682 - val_auc: 0.7154\n",
      "Epoch 680/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4445 - auc: 0.6912 - val_loss: 1.4227 - val_auc: 0.7154\n",
      "Epoch 681/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4419 - auc: 0.6958 - val_loss: 1.5253 - val_auc: 0.7154\n",
      "Epoch 682/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4429 - auc: 0.6915 - val_loss: 1.3489 - val_auc: 0.7154\n",
      "Epoch 683/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4450 - auc: 0.6902 - val_loss: 1.6380 - val_auc: 0.7154\n",
      "Epoch 684/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4736 - auc: 0.6808 - val_loss: 1.5010 - val_auc: 0.7155\n",
      "Epoch 685/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4613 - auc: 0.6851 - val_loss: 1.3220 - val_auc: 0.7152\n",
      "Epoch 686/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4680 - auc: 0.6887 - val_loss: 1.5253 - val_auc: 0.7150\n",
      "Epoch 687/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4540 - auc: 0.6870 - val_loss: 1.5089 - val_auc: 0.7152\n",
      "Epoch 688/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4471 - auc: 0.6947 - val_loss: 1.3825 - val_auc: 0.7154\n",
      "Epoch 689/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4471 - auc: 0.6979 - val_loss: 1.4106 - val_auc: 0.7153\n",
      "Epoch 690/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4501 - auc: 0.6815 - val_loss: 1.5342 - val_auc: 0.7153\n",
      "Epoch 691/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4507 - auc: 0.6846 - val_loss: 1.4579 - val_auc: 0.7155\n",
      "Epoch 692/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4381 - auc: 0.6945 - val_loss: 1.3931 - val_auc: 0.7156\n",
      "Epoch 693/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4421 - auc: 0.6898 - val_loss: 1.4706 - val_auc: 0.7155\n",
      "Epoch 694/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4354 - auc: 0.6922 - val_loss: 1.4984 - val_auc: 0.7154\n",
      "Epoch 695/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4408 - auc: 0.6894 - val_loss: 1.4104 - val_auc: 0.7154\n",
      "Epoch 696/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4441 - auc: 0.6872 - val_loss: 1.4471 - val_auc: 0.7153\n",
      "Epoch 697/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4319 - auc: 0.6960 - val_loss: 1.4750 - val_auc: 0.7153\n",
      "Epoch 698/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4328 - auc: 0.6904 - val_loss: 1.3992 - val_auc: 0.7154\n",
      "Epoch 699/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4388 - auc: 0.6873 - val_loss: 1.4172 - val_auc: 0.7154\n",
      "Epoch 700/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4355 - auc: 0.6935 - val_loss: 1.4752 - val_auc: 0.7153\n",
      "Epoch 701/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4327 - auc: 0.6971 - val_loss: 1.4251 - val_auc: 0.7154\n",
      "Epoch 702/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4332 - auc: 0.6855 - val_loss: 1.3924 - val_auc: 0.7154\n",
      "Epoch 703/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4356 - auc: 0.6881 - val_loss: 1.5224 - val_auc: 0.7155\n",
      "Epoch 704/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4366 - auc: 0.6936 - val_loss: 1.4116 - val_auc: 0.7153\n",
      "Epoch 705/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4304 - auc: 0.6959 - val_loss: 1.3404 - val_auc: 0.7153\n",
      "Epoch 706/1000\n",
      "1850/1850 [==============================] - 0s 30us/step - loss: 1.4421 - auc: 0.6886 - val_loss: 1.6514 - val_auc: 0.7153\n",
      "Epoch 707/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4639 - auc: 0.7009 - val_loss: 1.4982 - val_auc: 0.7151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4476 - auc: 0.6939 - val_loss: 1.3403 - val_auc: 0.7149\n",
      "Epoch 709/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4619 - auc: 0.6891 - val_loss: 1.3988 - val_auc: 0.7149\n",
      "Epoch 710/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4450 - auc: 0.6947 - val_loss: 1.5735 - val_auc: 0.7150\n",
      "Epoch 711/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4530 - auc: 0.6890 - val_loss: 1.4627 - val_auc: 0.7152\n",
      "Epoch 712/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4334 - auc: 0.6996 - val_loss: 1.3303 - val_auc: 0.7154\n",
      "Epoch 713/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4494 - auc: 0.6878 - val_loss: 1.5357 - val_auc: 0.7154\n",
      "Epoch 714/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4495 - auc: 0.6954 - val_loss: 1.5589 - val_auc: 0.7153\n",
      "Epoch 715/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4634 - auc: 0.6911 - val_loss: 1.4325 - val_auc: 0.7155\n",
      "Epoch 716/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4493 - auc: 0.6936 - val_loss: 1.3443 - val_auc: 0.7157\n",
      "Epoch 717/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4530 - auc: 0.6913 - val_loss: 1.5178 - val_auc: 0.7152\n",
      "Epoch 718/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4408 - auc: 0.6942 - val_loss: 1.4999 - val_auc: 0.7155\n",
      "Epoch 719/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4409 - auc: 0.7001 - val_loss: 1.3912 - val_auc: 0.7156\n",
      "Epoch 720/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4459 - auc: 0.7028 - val_loss: 1.3730 - val_auc: 0.7155\n",
      "Epoch 721/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4465 - auc: 0.6954 - val_loss: 1.4939 - val_auc: 0.7157\n",
      "Epoch 722/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4363 - auc: 0.6937 - val_loss: 1.4951 - val_auc: 0.7158\n",
      "Epoch 723/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4347 - auc: 0.6947 - val_loss: 1.3504 - val_auc: 0.7159\n",
      "Epoch 724/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4467 - auc: 0.6843 - val_loss: 1.6047 - val_auc: 0.7156\n",
      "Epoch 725/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4614 - auc: 0.6922 - val_loss: 1.5517 - val_auc: 0.7158\n",
      "Epoch 726/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4586 - auc: 0.6898 - val_loss: 1.3764 - val_auc: 0.7159\n",
      "Epoch 727/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4595 - auc: 0.6864 - val_loss: 1.3478 - val_auc: 0.7155\n",
      "Epoch 728/1000\n",
      "1850/1850 [==============================] - 0s 28us/step - loss: 1.4537 - auc: 0.6917 - val_loss: 1.5966 - val_auc: 0.7070\n",
      "Epoch 729/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4594 - auc: 0.6946 - val_loss: 1.4558 - val_auc: 0.7158\n",
      "Epoch 730/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4390 - auc: 0.6930 - val_loss: 1.3732 - val_auc: 0.7160\n",
      "Epoch 731/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4552 - auc: 0.6943 - val_loss: 1.4079 - val_auc: 0.7162\n",
      "Epoch 732/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4484 - auc: 0.6995 - val_loss: 1.5027 - val_auc: 0.7163\n",
      "Epoch 733/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4433 - auc: 0.6961 - val_loss: 1.4759 - val_auc: 0.7164\n",
      "Epoch 734/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4420 - auc: 0.6903 - val_loss: 1.4110 - val_auc: 0.7164\n",
      "Epoch 735/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4391 - auc: 0.6900 - val_loss: 1.5655 - val_auc: 0.7165\n",
      "Epoch 736/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4468 - auc: 0.7022 - val_loss: 1.4715 - val_auc: 0.7165\n",
      "Epoch 737/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4443 - auc: 0.6998 - val_loss: 1.3394 - val_auc: 0.7164\n",
      "Epoch 738/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4541 - auc: 0.6976 - val_loss: 1.4702 - val_auc: 0.7165\n",
      "Epoch 739/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4387 - auc: 0.6995 - val_loss: 1.5133 - val_auc: 0.7166\n",
      "Epoch 740/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4340 - auc: 0.7044 - val_loss: 1.3625 - val_auc: 0.7163\n",
      "Epoch 741/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4361 - auc: 0.6961 - val_loss: 1.4430 - val_auc: 0.7163\n",
      "Epoch 742/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4302 - auc: 0.6962 - val_loss: 1.4775 - val_auc: 0.7163\n",
      "Epoch 743/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4352 - auc: 0.6970 - val_loss: 1.4206 - val_auc: 0.7164\n",
      "Epoch 744/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4327 - auc: 0.7008 - val_loss: 1.4255 - val_auc: 0.7163\n",
      "Epoch 745/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4336 - auc: 0.6938 - val_loss: 1.4865 - val_auc: 0.7163\n",
      "Epoch 746/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4327 - auc: 0.6934 - val_loss: 1.4244 - val_auc: 0.7163\n",
      "Epoch 747/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4278 - auc: 0.6990 - val_loss: 1.4471 - val_auc: 0.7162\n",
      "Epoch 748/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4302 - auc: 0.6971 - val_loss: 1.4641 - val_auc: 0.7162\n",
      "Epoch 749/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4305 - auc: 0.7010 - val_loss: 1.3808 - val_auc: 0.7164\n",
      "Epoch 750/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4321 - auc: 0.7035 - val_loss: 1.5941 - val_auc: 0.7162\n",
      "Epoch 751/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4458 - auc: 0.7056 - val_loss: 1.4028 - val_auc: 0.7164\n",
      "Epoch 752/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4458 - auc: 0.6876 - val_loss: 1.3311 - val_auc: 0.7166\n",
      "Epoch 753/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4510 - auc: 0.6936 - val_loss: 1.5933 - val_auc: 0.7162\n",
      "Epoch 754/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4556 - auc: 0.6920 - val_loss: 1.5451 - val_auc: 0.7163\n",
      "Epoch 755/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4442 - auc: 0.7012 - val_loss: 1.3190 - val_auc: 0.7167\n",
      "Epoch 756/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4580 - auc: 0.6934 - val_loss: 1.4269 - val_auc: 0.7164\n",
      "Epoch 757/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4341 - auc: 0.7035 - val_loss: 1.5563 - val_auc: 0.7164\n",
      "Epoch 758/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4474 - auc: 0.6941 - val_loss: 1.4655 - val_auc: 0.7164\n",
      "Epoch 759/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4383 - auc: 0.6937 - val_loss: 1.3292 - val_auc: 0.7164\n",
      "Epoch 760/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4511 - auc: 0.6919 - val_loss: 1.5453 - val_auc: 0.7164\n",
      "Epoch 761/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4459 - auc: 0.6983 - val_loss: 1.5506 - val_auc: 0.7165\n",
      "Epoch 762/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4542 - auc: 0.6944 - val_loss: 1.4135 - val_auc: 0.7165\n",
      "Epoch 763/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4459 - auc: 0.6950 - val_loss: 1.3360 - val_auc: 0.7165\n",
      "Epoch 764/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4512 - auc: 0.6931 - val_loss: 1.5241 - val_auc: 0.7163\n",
      "Epoch 765/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4408 - auc: 0.7021 - val_loss: 1.5023 - val_auc: 0.7164\n",
      "Epoch 766/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4393 - auc: 0.7012 - val_loss: 1.3875 - val_auc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4457 - auc: 0.6940 - val_loss: 1.3897 - val_auc: 0.7167\n",
      "Epoch 768/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4419 - auc: 0.6948 - val_loss: 1.5336 - val_auc: 0.7166\n",
      "Epoch 769/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4389 - auc: 0.6994 - val_loss: 1.4783 - val_auc: 0.7167\n",
      "Epoch 770/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4328 - auc: 0.6952 - val_loss: 1.3320 - val_auc: 0.7168\n",
      "Epoch 771/1000\n",
      "1850/1850 [==============================] - 0s 30us/step - loss: 1.4446 - auc: 0.6984 - val_loss: 1.6171 - val_auc: 0.7165\n",
      "Epoch 772/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4637 - auc: 0.6927 - val_loss: 1.5545 - val_auc: 0.7168\n",
      "Epoch 773/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4605 - auc: 0.6936 - val_loss: 1.3998 - val_auc: 0.7166\n",
      "Epoch 774/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4609 - auc: 0.6877 - val_loss: 1.3530 - val_auc: 0.7165\n",
      "Epoch 775/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4644 - auc: 0.6868 - val_loss: 1.5984 - val_auc: 0.7053\n",
      "Epoch 776/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4645 - auc: 0.6877 - val_loss: 1.4216 - val_auc: 0.7169\n",
      "Epoch 777/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4351 - auc: 0.6964 - val_loss: 1.3751 - val_auc: 0.7168\n",
      "Epoch 778/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4455 - auc: 0.6954 - val_loss: 1.4595 - val_auc: 0.7169\n",
      "Epoch 779/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4424 - auc: 0.7027 - val_loss: 1.4935 - val_auc: 0.7169\n",
      "Epoch 780/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4486 - auc: 0.6985 - val_loss: 1.3991 - val_auc: 0.7168\n",
      "Epoch 781/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4498 - auc: 0.6914 - val_loss: 1.6395 - val_auc: 0.7169\n",
      "Epoch 782/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4608 - auc: 0.6961 - val_loss: 1.4908 - val_auc: 0.7171\n",
      "Epoch 783/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4367 - auc: 0.6969 - val_loss: 1.2953 - val_auc: 0.7170\n",
      "Epoch 784/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4651 - auc: 0.6977 - val_loss: 1.4818 - val_auc: 0.7168\n",
      "Epoch 785/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4500 - auc: 0.6979 - val_loss: 1.5664 - val_auc: 0.7167\n",
      "Epoch 786/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4636 - auc: 0.7052 - val_loss: 1.4880 - val_auc: 0.7165\n",
      "Epoch 787/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4575 - auc: 0.6986 - val_loss: 1.3643 - val_auc: 0.7163\n",
      "Epoch 788/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4459 - auc: 0.7004 - val_loss: 1.3681 - val_auc: 0.7165\n",
      "Epoch 789/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4464 - auc: 0.6923 - val_loss: 1.5250 - val_auc: 0.7150\n",
      "Epoch 790/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4408 - auc: 0.7020 - val_loss: 1.5001 - val_auc: 0.7167\n",
      "Epoch 791/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4367 - auc: 0.7017 - val_loss: 1.4006 - val_auc: 0.7169\n",
      "Epoch 792/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4440 - auc: 0.6946 - val_loss: 1.4265 - val_auc: 0.7172\n",
      "Epoch 793/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4388 - auc: 0.7023 - val_loss: 1.5096 - val_auc: 0.7172\n",
      "Epoch 794/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4399 - auc: 0.6883 - val_loss: 1.4335 - val_auc: 0.7174\n",
      "Epoch 795/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4354 - auc: 0.6902 - val_loss: 1.4886 - val_auc: 0.7171\n",
      "Epoch 796/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4345 - auc: 0.6944 - val_loss: 1.3560 - val_auc: 0.7169\n",
      "Epoch 797/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4355 - auc: 0.6961 - val_loss: 1.5960 - val_auc: 0.7168\n",
      "Epoch 798/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4566 - auc: 0.6940 - val_loss: 1.4843 - val_auc: 0.7169\n",
      "Epoch 799/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4462 - auc: 0.7018 - val_loss: 1.3366 - val_auc: 0.7161\n",
      "Epoch 800/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4585 - auc: 0.6947 - val_loss: 1.4626 - val_auc: 0.7113\n",
      "Epoch 801/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4372 - auc: 0.6959 - val_loss: 1.5073 - val_auc: 0.7127\n",
      "Epoch 802/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4366 - auc: 0.6920 - val_loss: 1.3929 - val_auc: 0.7173\n",
      "Epoch 803/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4371 - auc: 0.6944 - val_loss: 1.4226 - val_auc: 0.7170\n",
      "Epoch 804/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4356 - auc: 0.7025 - val_loss: 1.4863 - val_auc: 0.7175\n",
      "Epoch 805/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4365 - auc: 0.7015 - val_loss: 1.4128 - val_auc: 0.7175\n",
      "Epoch 806/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4366 - auc: 0.6953 - val_loss: 1.5672 - val_auc: 0.7176\n",
      "Epoch 807/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4398 - auc: 0.6964 - val_loss: 1.3659 - val_auc: 0.7175\n",
      "Epoch 808/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4286 - auc: 0.6983 - val_loss: 1.4940 - val_auc: 0.7176\n",
      "Epoch 809/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4322 - auc: 0.7012 - val_loss: 1.4473 - val_auc: 0.7172\n",
      "Epoch 810/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4335 - auc: 0.7070 - val_loss: 1.3639 - val_auc: 0.7171\n",
      "Epoch 811/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4406 - auc: 0.6973 - val_loss: 1.4994 - val_auc: 0.7175\n",
      "Epoch 812/1000\n",
      "1850/1850 [==============================] - 0s 33us/step - loss: 1.4318 - auc: 0.6990 - val_loss: 1.4585 - val_auc: 0.7169\n",
      "Epoch 813/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4339 - auc: 0.6912 - val_loss: 1.3494 - val_auc: 0.7167\n",
      "Epoch 814/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4320 - auc: 0.6984 - val_loss: 1.4508 - val_auc: 0.7167\n",
      "Epoch 815/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4253 - auc: 0.7055 - val_loss: 1.4987 - val_auc: 0.7167\n",
      "Epoch 816/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4292 - auc: 0.7022 - val_loss: 1.3567 - val_auc: 0.7167\n",
      "Epoch 817/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4350 - auc: 0.6957 - val_loss: 1.5685 - val_auc: 0.7168\n",
      "Epoch 818/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4402 - auc: 0.7037 - val_loss: 1.4788 - val_auc: 0.7167\n",
      "Epoch 819/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4391 - auc: 0.6973 - val_loss: 1.3340 - val_auc: 0.7165\n",
      "Epoch 820/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4424 - auc: 0.7059 - val_loss: 1.5372 - val_auc: 0.7168\n",
      "Epoch 821/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4367 - auc: 0.7090 - val_loss: 1.5044 - val_auc: 0.7167\n",
      "Epoch 822/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4303 - auc: 0.7075 - val_loss: 1.3412 - val_auc: 0.7169\n",
      "Epoch 823/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4359 - auc: 0.6958 - val_loss: 1.4881 - val_auc: 0.7169\n",
      "Epoch 824/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4292 - auc: 0.7015 - val_loss: 1.5128 - val_auc: 0.7170\n",
      "Epoch 825/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4326 - auc: 0.6983 - val_loss: 1.3917 - val_auc: 0.7170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4311 - auc: 0.6923 - val_loss: 1.3884 - val_auc: 0.7171\n",
      "Epoch 827/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4248 - auc: 0.7042 - val_loss: 1.5005 - val_auc: 0.7173\n",
      "Epoch 828/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4256 - auc: 0.6973 - val_loss: 1.4274 - val_auc: 0.7173\n",
      "Epoch 829/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4231 - auc: 0.6953 - val_loss: 1.3886 - val_auc: 0.7173\n",
      "Epoch 830/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4185 - auc: 0.7077 - val_loss: 1.4881 - val_auc: 0.7171\n",
      "Epoch 831/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4246 - auc: 0.7068 - val_loss: 1.4005 - val_auc: 0.7171\n",
      "Epoch 832/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4242 - auc: 0.6999 - val_loss: 1.4792 - val_auc: 0.7170\n",
      "Epoch 833/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4193 - auc: 0.7012 - val_loss: 1.4106 - val_auc: 0.7170\n",
      "Epoch 834/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4112 - auc: 0.7117 - val_loss: 1.3926 - val_auc: 0.7170\n",
      "Epoch 835/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4202 - auc: 0.7059 - val_loss: 1.5026 - val_auc: 0.7169\n",
      "Epoch 836/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4284 - auc: 0.6980 - val_loss: 1.3892 - val_auc: 0.7168\n",
      "Epoch 837/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4195 - auc: 0.7054 - val_loss: 1.4124 - val_auc: 0.7168\n",
      "Epoch 838/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4205 - auc: 0.7022 - val_loss: 1.4712 - val_auc: 0.7170\n",
      "Epoch 839/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4195 - auc: 0.7078 - val_loss: 1.3815 - val_auc: 0.7172\n",
      "Epoch 840/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4251 - auc: 0.6979 - val_loss: 1.5280 - val_auc: 0.7171\n",
      "Epoch 841/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4300 - auc: 0.6990 - val_loss: 1.3831 - val_auc: 0.7173\n",
      "Epoch 842/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4180 - auc: 0.7059 - val_loss: 1.4474 - val_auc: 0.7171\n",
      "Epoch 843/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4165 - auc: 0.7072 - val_loss: 1.4593 - val_auc: 0.7172\n",
      "Epoch 844/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4230 - auc: 0.6956 - val_loss: 1.4168 - val_auc: 0.7172\n",
      "Epoch 845/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4172 - auc: 0.7028 - val_loss: 1.4346 - val_auc: 0.7172\n",
      "Epoch 846/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4179 - auc: 0.6979 - val_loss: 1.4272 - val_auc: 0.7173\n",
      "Epoch 847/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4208 - auc: 0.6999 - val_loss: 1.4045 - val_auc: 0.7173\n",
      "Epoch 848/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4209 - auc: 0.7034 - val_loss: 1.4751 - val_auc: 0.7174\n",
      "Epoch 849/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4272 - auc: 0.6885 - val_loss: 1.4107 - val_auc: 0.7175\n",
      "Epoch 850/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4183 - auc: 0.7013 - val_loss: 1.4565 - val_auc: 0.7174\n",
      "Epoch 851/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4194 - auc: 0.7020 - val_loss: 1.4128 - val_auc: 0.7175\n",
      "Epoch 852/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4214 - auc: 0.6994 - val_loss: 1.4600 - val_auc: 0.7175\n",
      "Epoch 853/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4174 - auc: 0.7058 - val_loss: 1.3861 - val_auc: 0.7173\n",
      "Epoch 854/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4266 - auc: 0.6910 - val_loss: 1.5144 - val_auc: 0.7171\n",
      "Epoch 855/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4283 - auc: 0.6983 - val_loss: 1.3799 - val_auc: 0.7167\n",
      "Epoch 856/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4268 - auc: 0.7015 - val_loss: 1.4417 - val_auc: 0.7168\n",
      "Epoch 857/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4220 - auc: 0.6993 - val_loss: 1.4860 - val_auc: 0.7171\n",
      "Epoch 858/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4220 - auc: 0.7008 - val_loss: 1.3607 - val_auc: 0.7172\n",
      "Epoch 859/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4237 - auc: 0.7020 - val_loss: 1.5812 - val_auc: 0.7172\n",
      "Epoch 860/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4385 - auc: 0.6992 - val_loss: 1.4340 - val_auc: 0.7174\n",
      "Epoch 861/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4240 - auc: 0.7076 - val_loss: 1.3055 - val_auc: 0.7173\n",
      "Epoch 862/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4477 - auc: 0.6937 - val_loss: 1.6154 - val_auc: 0.7174\n",
      "Epoch 863/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4549 - auc: 0.6997 - val_loss: 1.5216 - val_auc: 0.7175\n",
      "Epoch 864/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4428 - auc: 0.7040 - val_loss: 1.3529 - val_auc: 0.7174\n",
      "Epoch 865/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4475 - auc: 0.7058 - val_loss: 1.3714 - val_auc: 0.7174\n",
      "Epoch 866/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4379 - auc: 0.6999 - val_loss: 1.5746 - val_auc: 0.7145\n",
      "Epoch 867/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4418 - auc: 0.7076 - val_loss: 1.4372 - val_auc: 0.7175\n",
      "Epoch 868/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4262 - auc: 0.7083 - val_loss: 1.3809 - val_auc: 0.7178\n",
      "Epoch 869/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4367 - auc: 0.7065 - val_loss: 1.5040 - val_auc: 0.7175\n",
      "Epoch 870/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4365 - auc: 0.7033 - val_loss: 1.4982 - val_auc: 0.7173\n",
      "Epoch 871/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4340 - auc: 0.7042 - val_loss: 1.3223 - val_auc: 0.7173\n",
      "Epoch 872/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4549 - auc: 0.6914 - val_loss: 1.7470 - val_auc: 0.7169\n",
      "Epoch 873/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5013 - auc: 0.7024 - val_loss: 1.5734 - val_auc: 0.7170\n",
      "Epoch 874/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4816 - auc: 0.7053 - val_loss: 1.3776 - val_auc: 0.7146\n",
      "Epoch 875/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5162 - auc: 0.6831 - val_loss: 1.3355 - val_auc: 0.7154\n",
      "Epoch 876/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5054 - auc: 0.6887 - val_loss: 1.5150 - val_auc: 0.6796\n",
      "Epoch 877/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4740 - auc: 0.6839 - val_loss: 1.5448 - val_auc: 0.7087\n",
      "Epoch 878/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4596 - auc: 0.6959 - val_loss: 1.4072 - val_auc: 0.7187\n",
      "Epoch 879/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4484 - auc: 0.7039 - val_loss: 1.4901 - val_auc: 0.7185\n",
      "Epoch 880/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4590 - auc: 0.6996 - val_loss: 1.5373 - val_auc: 0.7179\n",
      "Epoch 881/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4665 - auc: 0.6943 - val_loss: 1.4207 - val_auc: 0.7177\n",
      "Epoch 882/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4560 - auc: 0.6975 - val_loss: 1.4826 - val_auc: 0.7178\n",
      "Epoch 883/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4491 - auc: 0.6935 - val_loss: 1.4622 - val_auc: 0.7180\n",
      "Epoch 884/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4368 - auc: 0.7020 - val_loss: 1.4156 - val_auc: 0.7184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 885/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4384 - auc: 0.6965 - val_loss: 1.5309 - val_auc: 0.7178\n",
      "Epoch 886/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4407 - auc: 0.7023 - val_loss: 1.3912 - val_auc: 0.7192\n",
      "Epoch 887/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4421 - auc: 0.6976 - val_loss: 1.4078 - val_auc: 0.7189\n",
      "Epoch 888/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4446 - auc: 0.7010 - val_loss: 1.5306 - val_auc: 0.7186\n",
      "Epoch 889/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4474 - auc: 0.7004 - val_loss: 1.4188 - val_auc: 0.7185\n",
      "Epoch 890/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4349 - auc: 0.7020 - val_loss: 1.4203 - val_auc: 0.7178\n",
      "Epoch 891/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4299 - auc: 0.7041 - val_loss: 1.4601 - val_auc: 0.7172\n",
      "Epoch 892/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4215 - auc: 0.7134 - val_loss: 1.4419 - val_auc: 0.7167\n",
      "Epoch 893/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4229 - auc: 0.7097 - val_loss: 1.4294 - val_auc: 0.7164\n",
      "Epoch 894/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4265 - auc: 0.7071 - val_loss: 1.4912 - val_auc: 0.7165\n",
      "Epoch 895/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4282 - auc: 0.7096 - val_loss: 1.3791 - val_auc: 0.7168\n",
      "Epoch 896/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4338 - auc: 0.7028 - val_loss: 1.6418 - val_auc: 0.7168\n",
      "Epoch 897/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4552 - auc: 0.7037 - val_loss: 1.4281 - val_auc: 0.7174\n",
      "Epoch 898/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4289 - auc: 0.7018 - val_loss: 1.3014 - val_auc: 0.7180\n",
      "Epoch 899/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4464 - auc: 0.6997 - val_loss: 1.6185 - val_auc: 0.7183\n",
      "Epoch 900/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4558 - auc: 0.7010 - val_loss: 1.5377 - val_auc: 0.7187\n",
      "Epoch 901/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4499 - auc: 0.7034 - val_loss: 1.3756 - val_auc: 0.7189\n",
      "Epoch 902/1000\n",
      "1850/1850 [==============================] - 0s 28us/step - loss: 1.4474 - auc: 0.6980 - val_loss: 1.3461 - val_auc: 0.7190\n",
      "Epoch 903/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4498 - auc: 0.6933 - val_loss: 1.5667 - val_auc: 0.7137\n",
      "Epoch 904/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4469 - auc: 0.6989 - val_loss: 1.5091 - val_auc: 0.7188\n",
      "Epoch 905/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4342 - auc: 0.7061 - val_loss: 1.3726 - val_auc: 0.7188\n",
      "Epoch 906/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4374 - auc: 0.7052 - val_loss: 1.3859 - val_auc: 0.7185\n",
      "Epoch 907/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4406 - auc: 0.7039 - val_loss: 1.5095 - val_auc: 0.7177\n",
      "Epoch 908/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4377 - auc: 0.7052 - val_loss: 1.4885 - val_auc: 0.7178\n",
      "Epoch 909/1000\n",
      "1850/1850 [==============================] - 0s 29us/step - loss: 1.4320 - auc: 0.7004 - val_loss: 1.3315 - val_auc: 0.7180\n",
      "Epoch 910/1000\n",
      "1850/1850 [==============================] - 0s 29us/step - loss: 1.4378 - auc: 0.7001 - val_loss: 1.6564 - val_auc: 0.7178\n",
      "Epoch 911/1000\n",
      "1850/1850 [==============================] - 0s 29us/step - loss: 1.4683 - auc: 0.7029 - val_loss: 1.5528 - val_auc: 0.7181\n",
      "Epoch 912/1000\n",
      "1850/1850 [==============================] - 0s 28us/step - loss: 1.4596 - auc: 0.7062 - val_loss: 1.3583 - val_auc: 0.7175\n",
      "Epoch 913/1000\n",
      "1850/1850 [==============================] - 0s 28us/step - loss: 1.4807 - auc: 0.6929 - val_loss: 1.3008 - val_auc: 0.7165\n",
      "Epoch 914/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4822 - auc: 0.6897 - val_loss: 1.5080 - val_auc: 0.6841\n",
      "Epoch 915/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4558 - auc: 0.6773 - val_loss: 1.5584 - val_auc: 0.7116\n",
      "Epoch 916/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4461 - auc: 0.7028 - val_loss: 1.4067 - val_auc: 0.7185\n",
      "Epoch 917/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4357 - auc: 0.7047 - val_loss: 1.4010 - val_auc: 0.7187\n",
      "Epoch 918/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4427 - auc: 0.7075 - val_loss: 1.5399 - val_auc: 0.7189\n",
      "Epoch 919/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4538 - auc: 0.7032 - val_loss: 1.4810 - val_auc: 0.7189\n",
      "Epoch 920/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4439 - auc: 0.7033 - val_loss: 1.3478 - val_auc: 0.7188\n",
      "Epoch 921/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4493 - auc: 0.6942 - val_loss: 1.7920 - val_auc: 0.7187\n",
      "Epoch 922/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5153 - auc: 0.6977 - val_loss: 1.6752 - val_auc: 0.7182\n",
      "Epoch 923/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4949 - auc: 0.7100 - val_loss: 1.4090 - val_auc: 0.7179\n",
      "Epoch 924/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4911 - auc: 0.7006 - val_loss: 1.3054 - val_auc: 0.7180\n",
      "Epoch 925/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5272 - auc: 0.6553 - val_loss: 1.3314 - val_auc: 0.6780\n",
      "Epoch 926/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4986 - auc: 0.6617 - val_loss: 1.5489 - val_auc: 0.6701\n",
      "Epoch 927/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4851 - auc: 0.6473 - val_loss: 1.5176 - val_auc: 0.6879\n",
      "Epoch 928/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4640 - auc: 0.6851 - val_loss: 1.4750 - val_auc: 0.7180\n",
      "Epoch 929/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4570 - auc: 0.7066 - val_loss: 1.4754 - val_auc: 0.7174\n",
      "Epoch 930/1000\n",
      "1850/1850 [==============================] - 0s 27us/step - loss: 1.4684 - auc: 0.7030 - val_loss: 1.4645 - val_auc: 0.7176\n",
      "Epoch 931/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4675 - auc: 0.7073 - val_loss: 1.5098 - val_auc: 0.7176\n",
      "Epoch 932/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4587 - auc: 0.6974 - val_loss: 1.4991 - val_auc: 0.7178\n",
      "Epoch 933/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4504 - auc: 0.6961 - val_loss: 1.4099 - val_auc: 0.7179\n",
      "Epoch 934/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4477 - auc: 0.6878 - val_loss: 1.6629 - val_auc: 0.7181\n",
      "Epoch 935/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4729 - auc: 0.6896 - val_loss: 1.4858 - val_auc: 0.7180\n",
      "Epoch 936/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4466 - auc: 0.6986 - val_loss: 1.2578 - val_auc: 0.7184\n",
      "Epoch 937/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4952 - auc: 0.6971 - val_loss: 1.6661 - val_auc: 0.7186\n",
      "Epoch 938/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5095 - auc: 0.7046 - val_loss: 1.7053 - val_auc: 0.7183\n",
      "Epoch 939/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5538 - auc: 0.7018 - val_loss: 1.5656 - val_auc: 0.7185\n",
      "Epoch 940/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.5540 - auc: 0.6976 - val_loss: 1.4776 - val_auc: 0.6882\n",
      "Epoch 941/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5487 - auc: 0.6851 - val_loss: 1.4615 - val_auc: 0.6867\n",
      "Epoch 942/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.5273 - auc: 0.6797 - val_loss: 1.4854 - val_auc: 0.6849\n",
      "Epoch 943/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.5147 - auc: 0.6811 - val_loss: 1.4672 - val_auc: 0.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 944/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4919 - auc: 0.6997 - val_loss: 1.4656 - val_auc: 0.7187\n",
      "Epoch 945/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4772 - auc: 0.7086 - val_loss: 1.5047 - val_auc: 0.7190\n",
      "Epoch 946/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4768 - auc: 0.6900 - val_loss: 1.5250 - val_auc: 0.7196\n",
      "Epoch 947/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4739 - auc: 0.7041 - val_loss: 1.4962 - val_auc: 0.7196\n",
      "Epoch 948/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4772 - auc: 0.6823 - val_loss: 1.4938 - val_auc: 0.7196\n",
      "Epoch 949/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4664 - auc: 0.7007 - val_loss: 1.4645 - val_auc: 0.7195\n",
      "Epoch 950/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4615 - auc: 0.7005 - val_loss: 1.5126 - val_auc: 0.7195\n",
      "Epoch 951/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4570 - auc: 0.6995 - val_loss: 1.4762 - val_auc: 0.7191\n",
      "Epoch 952/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4501 - auc: 0.7070 - val_loss: 1.4026 - val_auc: 0.7187\n",
      "Epoch 953/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4514 - auc: 0.7020 - val_loss: 1.4985 - val_auc: 0.7185\n",
      "Epoch 954/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4455 - auc: 0.7078 - val_loss: 1.4466 - val_auc: 0.7180\n",
      "Epoch 955/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4397 - auc: 0.7077 - val_loss: 1.3896 - val_auc: 0.7178\n",
      "Epoch 956/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4405 - auc: 0.7087 - val_loss: 1.5379 - val_auc: 0.7138\n",
      "Epoch 957/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4481 - auc: 0.6984 - val_loss: 1.4583 - val_auc: 0.7176\n",
      "Epoch 958/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4399 - auc: 0.6949 - val_loss: 1.3638 - val_auc: 0.7177\n",
      "Epoch 959/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4468 - auc: 0.7006 - val_loss: 1.5609 - val_auc: 0.7174\n",
      "Epoch 960/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4441 - auc: 0.7064 - val_loss: 1.5173 - val_auc: 0.7174\n",
      "Epoch 961/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4362 - auc: 0.7123 - val_loss: 1.3289 - val_auc: 0.7176\n",
      "Epoch 962/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4564 - auc: 0.6922 - val_loss: 1.5588 - val_auc: 0.7175\n",
      "Epoch 963/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4505 - auc: 0.7028 - val_loss: 1.5424 - val_auc: 0.7178\n",
      "Epoch 964/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4509 - auc: 0.7075 - val_loss: 1.4104 - val_auc: 0.7179\n",
      "Epoch 965/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4484 - auc: 0.7043 - val_loss: 1.3688 - val_auc: 0.7179\n",
      "Epoch 966/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4493 - auc: 0.7031 - val_loss: 1.5235 - val_auc: 0.7184\n",
      "Epoch 967/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4448 - auc: 0.6987 - val_loss: 1.4758 - val_auc: 0.7183\n",
      "Epoch 968/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4327 - auc: 0.7013 - val_loss: 1.4035 - val_auc: 0.7184\n",
      "Epoch 969/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4325 - auc: 0.7070 - val_loss: 1.4275 - val_auc: 0.7185\n",
      "Epoch 970/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4300 - auc: 0.7058 - val_loss: 1.4946 - val_auc: 0.7182\n",
      "Epoch 971/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4293 - auc: 0.7060 - val_loss: 1.4587 - val_auc: 0.7184\n",
      "Epoch 972/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4216 - auc: 0.7077 - val_loss: 1.3567 - val_auc: 0.7183\n",
      "Epoch 973/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4288 - auc: 0.6943 - val_loss: 1.6036 - val_auc: 0.7181\n",
      "Epoch 974/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4496 - auc: 0.6972 - val_loss: 1.5191 - val_auc: 0.7181\n",
      "Epoch 975/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4403 - auc: 0.7043 - val_loss: 1.3492 - val_auc: 0.7185\n",
      "Epoch 976/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4568 - auc: 0.6991 - val_loss: 1.3379 - val_auc: 0.7185\n",
      "Epoch 977/1000\n",
      "1850/1850 [==============================] - 0s 22us/step - loss: 1.4430 - auc: 0.7086 - val_loss: 1.5472 - val_auc: 0.7149\n",
      "Epoch 978/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4405 - auc: 0.7007 - val_loss: 1.4790 - val_auc: 0.7181\n",
      "Epoch 979/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4245 - auc: 0.7068 - val_loss: 1.3655 - val_auc: 0.7182\n",
      "Epoch 980/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4335 - auc: 0.7064 - val_loss: 1.4507 - val_auc: 0.7179\n",
      "Epoch 981/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4349 - auc: 0.7047 - val_loss: 1.4745 - val_auc: 0.7176\n",
      "Epoch 982/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4288 - auc: 0.7049 - val_loss: 1.4201 - val_auc: 0.7176\n",
      "Epoch 983/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4239 - auc: 0.7055 - val_loss: 1.4594 - val_auc: 0.7176\n",
      "Epoch 984/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4218 - auc: 0.7051 - val_loss: 1.4288 - val_auc: 0.7176\n",
      "Epoch 985/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4192 - auc: 0.7040 - val_loss: 1.4358 - val_auc: 0.7178\n",
      "Epoch 986/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4198 - auc: 0.7076 - val_loss: 1.4264 - val_auc: 0.7178\n",
      "Epoch 987/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4190 - auc: 0.7087 - val_loss: 1.4201 - val_auc: 0.7179\n",
      "Epoch 988/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4182 - auc: 0.7045 - val_loss: 1.4496 - val_auc: 0.7178\n",
      "Epoch 989/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4168 - auc: 0.7071 - val_loss: 1.4059 - val_auc: 0.7179\n",
      "Epoch 990/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4184 - auc: 0.7055 - val_loss: 1.4536 - val_auc: 0.7179\n",
      "Epoch 991/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4177 - auc: 0.7035 - val_loss: 1.4333 - val_auc: 0.7179\n",
      "Epoch 992/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4134 - auc: 0.7067 - val_loss: 1.4252 - val_auc: 0.7180\n",
      "Epoch 993/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4048 - auc: 0.7159 - val_loss: 1.4094 - val_auc: 0.7180\n",
      "Epoch 994/1000\n",
      "1850/1850 [==============================] - 0s 25us/step - loss: 1.4057 - auc: 0.7120 - val_loss: 1.4450 - val_auc: 0.7181\n",
      "Epoch 995/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4149 - auc: 0.7053 - val_loss: 1.4394 - val_auc: 0.7180\n",
      "Epoch 996/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4166 - auc: 0.7033 - val_loss: 1.3941 - val_auc: 0.7183\n",
      "Epoch 997/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4153 - auc: 0.7062 - val_loss: 1.5356 - val_auc: 0.7184\n",
      "Epoch 998/1000\n",
      "1850/1850 [==============================] - 0s 23us/step - loss: 1.4190 - auc: 0.7120 - val_loss: 1.3878 - val_auc: 0.7185\n",
      "Epoch 999/1000\n",
      "1850/1850 [==============================] - 0s 24us/step - loss: 1.4196 - auc: 0.7073 - val_loss: 1.4369 - val_auc: 0.7187\n",
      "Epoch 1000/1000\n",
      "1850/1850 [==============================] - 0s 26us/step - loss: 1.4166 - auc: 0.7112 - val_loss: 1.4828 - val_auc: 0.7187\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"178c3fb2-5758-4803-bd07-6ed37c4cdb41\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"178c3fb2-5758-4803-bd07-6ed37c4cdb41\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# history = model.fit_generator(generator=training_generator,\n",
    "#                               epochs=100,callbacks=[overfitCallback],validation_data=(testX, to_categorical(testY)))\n",
    "\n",
    "history = model.fit(X, to_categorical(y),batch_size=X.shape[0],validation_data=(testX, to_categorical(testY)),\n",
    "                    epochs=1000,callbacks=[overfitCallback],class_weight='balanced')\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ROC score:  0.7182317019722425\n",
      "test ROC score:  0.712841424628009\n"
     ]
    }
   ],
   "source": [
    "predictions_2['LABEL_Sepsis'] = pd.DataFrame(model.predict_proba(X_test_)).iloc[:,1]\n",
    "print(\"train ROC score: \",roc_auc_score(y,pd.DataFrame(model.predict_proba(X)).iloc[:,1]))\n",
    "print(\"test ROC score: \",roc_auc_score(testY,pd.DataFrame(model.predict_proba(testX)).iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x165a01210>"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trains_labels['LABEL_Sepsis'].plot.kde(color='green',legend=True,label='train labels')\n",
    "predictions_2['LABEL_Sepsis'].plot.kde(color='blue',legend=True,label='predicted train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1673a0690>"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testY.plot.kde(color='green',legend=True,label='train labels')\n",
    "pd.DataFrame(model.predict_proba(testX)).iloc[:,1].plot.kde(color='blue',legend=True,label='predicted train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['auc']) \n",
    "plt.plot(history.history['val_auc']) \n",
    "plt.title('auc') \n",
    "plt.ylabel('auc') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2.to_pickle(\"./pickled_data/task2CrossValidated\")\n",
    "\n",
    "# predictions_2 = pd.read_pickle(\"./pickled_data/task2CrossValidated\")\n",
    "# predictions_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "# labels_task3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "# params = {\n",
    "#     'C':[2**-1, 2**1, 2**3, 2**5],\n",
    "#     'kernel':['poly', 'rbf'],\n",
    "#     'degree':np.arange(2,5)\n",
    "# } \n",
    "\n",
    "# X = fullTrain.iloc[:,:]\n",
    "# X_test_ = fullTest\n",
    "\n",
    "# for label in labels_task3:\n",
    "\n",
    "#     print('>>> Starting to fit test {}'.format(label))\n",
    "#     clf = GridSearchCV(SVR(cache_size=7000),params,scoring='neg_root_mean_squared_error',n_jobs=-1,verbose=1,cv=3)\n",
    "#     clf.fit(X,trains_labels.loc[:,label])\n",
    "    \n",
    "#     print(' > Best estimator: {}'.format(clf.best_estimator_))\n",
    "#     print(mean_squared_error(trains_labels.loc[:,label], clf.predict(X), squared=False))\n",
    "    \n",
    "#     predictions_3[label] = clf.predict(X_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subtask 3 - ANN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X.shape[1], kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(l2=1e-3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "usualCallback = EarlyStopping()\n",
    "overfitCallback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=[r2_keras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_RRate\n",
      "WARNING:tensorflow:From /Users/farzamf/Anaconda3/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 17095 samples, validate on 1900 samples\n",
      "Epoch 1/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 29.6013 - r2_keras: -1.3597 - val_loss: 28.0809 - val_r2_keras: -1.0246\n",
      "Epoch 2/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 15.2153 - r2_keras: -0.0695 - val_loss: 27.3230 - val_r2_keras: -1.0265\n",
      "Epoch 3/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 13.0143 - r2_keras: 0.0774 - val_loss: 19.2785 - val_r2_keras: -0.3703\n",
      "Epoch 4/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 11.9071 - r2_keras: 0.1521 - val_loss: 12.2086 - val_r2_keras: 0.2018\n",
      "Epoch 5/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 11.1796 - r2_keras: 0.1849 - val_loss: 9.9630 - val_r2_keras: 0.3379\n",
      "Epoch 6/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 10.5794 - r2_keras: 0.2219 - val_loss: 10.0657 - val_r2_keras: 0.3175\n",
      "Epoch 7/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 10.2275 - r2_keras: 0.2117 - val_loss: 9.6348 - val_r2_keras: 0.3287\n",
      "Epoch 8/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 9.8544 - r2_keras: 0.2406 - val_loss: 10.0227 - val_r2_keras: 0.2495\n",
      "Epoch 9/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 9.5379 - r2_keras: 0.2459 - val_loss: 9.1453 - val_r2_keras: 0.3352\n",
      "Epoch 10/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 9.4788 - r2_keras: 0.2436 - val_loss: 10.8198 - val_r2_keras: 0.1925\n",
      "Epoch 11/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 9.1342 - r2_keras: 0.2620 - val_loss: 8.7493 - val_r2_keras: 0.3327\n",
      "Epoch 12/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 8.8378 - r2_keras: 0.2758 - val_loss: 8.8622 - val_r2_keras: 0.3308\n",
      "Epoch 13/2000\n",
      "17095/17095 [==============================] - 1s 71us/step - loss: 8.8176 - r2_keras: 0.2727 - val_loss: 8.5423 - val_r2_keras: 0.3270\n",
      "Epoch 14/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 8.7064 - r2_keras: 0.2716 - val_loss: 8.5469 - val_r2_keras: 0.3364\n",
      "Epoch 15/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 8.6056 - r2_keras: 0.2834 - val_loss: 8.3371 - val_r2_keras: 0.3418\n",
      "Epoch 16/2000\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 8.4739 - r2_keras: 0.2883 - val_loss: 8.4518 - val_r2_keras: 0.3416\n",
      "Epoch 17/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 8.3539 - r2_keras: 0.2971 - val_loss: 8.2837 - val_r2_keras: 0.3462\n",
      "Epoch 18/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 8.2752 - r2_keras: 0.3043 - val_loss: 10.8076 - val_r2_keras: 0.1578\n",
      "Epoch 19/2000\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 8.3452 - r2_keras: 0.2953 - val_loss: 8.2301 - val_r2_keras: 0.3475\n",
      "Epoch 20/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 8.3098 - r2_keras: 0.2969 - val_loss: 8.2254 - val_r2_keras: 0.3486\n",
      "Epoch 21/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 8.2396 - r2_keras: 0.3003 - val_loss: 8.5026 - val_r2_keras: 0.3346\n",
      "Epoch 22/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 8.2105 - r2_keras: 0.3046 - val_loss: 8.6246 - val_r2_keras: 0.3255\n",
      "Epoch 23/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 8.1735 - r2_keras: 0.3102 - val_loss: 8.4339 - val_r2_keras: 0.3351\n",
      "Epoch 24/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 8.1799 - r2_keras: 0.3013 - val_loss: 8.2975 - val_r2_keras: 0.3381\n",
      "Epoch 25/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 8.1534 - r2_keras: 0.3122 - val_loss: 8.4184 - val_r2_keras: 0.3315\n",
      "Epoch 26/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 8.0880 - r2_keras: 0.3140 - val_loss: 8.3362 - val_r2_keras: 0.3406\n",
      "Epoch 27/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 8.0119 - r2_keras: 0.3191 - val_loss: 8.1667 - val_r2_keras: 0.3401\n",
      "Epoch 28/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 8.0678 - r2_keras: 0.3073 - val_loss: 8.1996 - val_r2_keras: 0.3400\n",
      "Epoch 29/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 8.0328 - r2_keras: 0.3183 - val_loss: 8.1989 - val_r2_keras: 0.3458\n",
      "Epoch 30/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.9658 - r2_keras: 0.3233 - val_loss: 8.5986 - val_r2_keras: 0.3270\n",
      "Epoch 31/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 7.9748 - r2_keras: 0.3169 - val_loss: 8.4909 - val_r2_keras: 0.2993\n",
      "Epoch 32/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 7.8992 - r2_keras: 0.3279 - val_loss: 8.1029 - val_r2_keras: 0.3462\n",
      "Epoch 33/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.9250 - r2_keras: 0.3220 - val_loss: 8.5310 - val_r2_keras: 0.3213\n",
      "Epoch 34/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.8776 - r2_keras: 0.3263 - val_loss: 8.2438 - val_r2_keras: 0.3262\n",
      "Epoch 35/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 7.9120 - r2_keras: 0.3280 - val_loss: 8.2202 - val_r2_keras: 0.3441\n",
      "Epoch 36/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.9309 - r2_keras: 0.3278 - val_loss: 8.0996 - val_r2_keras: 0.3450\n",
      "Epoch 37/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.8797 - r2_keras: 0.3278 - val_loss: 8.4523 - val_r2_keras: 0.3286\n",
      "Epoch 38/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.8517 - r2_keras: 0.3342 - val_loss: 8.2720 - val_r2_keras: 0.3413\n",
      "Epoch 39/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 7.8939 - r2_keras: 0.3249 - val_loss: 8.1205 - val_r2_keras: 0.3480\n",
      "Epoch 40/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 7.8764 - r2_keras: 0.3297 - val_loss: 8.2007 - val_r2_keras: 0.3480\n",
      "Epoch 41/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 7.8608 - r2_keras: 0.3253 - val_loss: 8.7032 - val_r2_keras: 0.3048\n",
      "Epoch 42/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.9274 - r2_keras: 0.3184 - val_loss: 8.1202 - val_r2_keras: 0.3415\n",
      "Epoch 43/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.9260 - r2_keras: 0.3237 - val_loss: 8.2345 - val_r2_keras: 0.3457\n",
      "Epoch 44/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.8667 - r2_keras: 0.3367 - val_loss: 8.1825 - val_r2_keras: 0.3460\n",
      "Epoch 45/2000\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 7.8462 - r2_keras: 0.3257 - val_loss: 8.1825 - val_r2_keras: 0.3418\n",
      "Epoch 46/2000\n",
      "17095/17095 [==============================] - 2s 107us/step - loss: 7.7676 - r2_keras: 0.3392 - val_loss: 8.4804 - val_r2_keras: 0.3317\n",
      "Epoch 47/2000\n",
      "17095/17095 [==============================] - 1s 85us/step - loss: 7.8350 - r2_keras: 0.3313 - val_loss: 8.3050 - val_r2_keras: 0.3397\n",
      "Epoch 48/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 7.7617 - r2_keras: 0.3302 - val_loss: 8.1843 - val_r2_keras: 0.3450\n",
      "Epoch 49/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 7.8043 - r2_keras: 0.3381 - val_loss: 8.2021 - val_r2_keras: 0.3472\n",
      "Epoch 50/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.7629 - r2_keras: 0.3393 - val_loss: 8.1268 - val_r2_keras: 0.3408\n",
      "Epoch 51/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.7560 - r2_keras: 0.3323 - val_loss: 8.3946 - val_r2_keras: 0.3228\n",
      "Epoch 52/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.8149 - r2_keras: 0.3279 - val_loss: 8.1106 - val_r2_keras: 0.3492\n",
      "Epoch 53/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.7867 - r2_keras: 0.3333 - val_loss: 8.2037 - val_r2_keras: 0.3251\n",
      "Epoch 54/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.8364 - r2_keras: 0.3272 - val_loss: 8.0458 - val_r2_keras: 0.3462\n",
      "Epoch 55/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.7539 - r2_keras: 0.3343 - val_loss: 8.2987 - val_r2_keras: 0.3413\n",
      "Epoch 56/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.7395 - r2_keras: 0.3369 - val_loss: 8.2072 - val_r2_keras: 0.3467\n",
      "Epoch 57/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 7.8297 - r2_keras: 0.3320 - val_loss: 8.2850 - val_r2_keras: 0.3393\n",
      "Epoch 58/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.8226 - r2_keras: 0.3329 - val_loss: 8.1024 - val_r2_keras: 0.3438\n",
      "Epoch 59/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.7456 - r2_keras: 0.3335 - val_loss: 8.0800 - val_r2_keras: 0.3503\n",
      "Epoch 60/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.7844 - r2_keras: 0.3332 - val_loss: 8.1335 - val_r2_keras: 0.3481\n",
      "Epoch 61/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.7736 - r2_keras: 0.3355 - val_loss: 8.2308 - val_r2_keras: 0.3436\n",
      "Epoch 62/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 7.7561 - r2_keras: 0.3341 - val_loss: 8.1547 - val_r2_keras: 0.3407\n",
      "Epoch 63/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 7.6893 - r2_keras: 0.3411 - val_loss: 8.3113 - val_r2_keras: 0.3109\n",
      "Epoch 64/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 7.7367 - r2_keras: 0.3398 - val_loss: 8.2158 - val_r2_keras: 0.3337\n",
      "Epoch 65/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 7.7312 - r2_keras: 0.3371 - val_loss: 8.2557 - val_r2_keras: 0.3361\n",
      "Epoch 66/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 7.7714 - r2_keras: 0.3356 - val_loss: 8.0851 - val_r2_keras: 0.3500\n",
      "Epoch 67/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.7488 - r2_keras: 0.3377 - val_loss: 8.0890 - val_r2_keras: 0.3414\n",
      "Epoch 68/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.7693 - r2_keras: 0.3316 - val_loss: 8.5179 - val_r2_keras: 0.3270\n",
      "Epoch 69/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.7493 - r2_keras: 0.3415 - val_loss: 8.1814 - val_r2_keras: 0.3373\n",
      "Epoch 70/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.7764 - r2_keras: 0.3390 - val_loss: 8.4575 - val_r2_keras: 0.3000\n",
      "Epoch 71/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.7289 - r2_keras: 0.3381 - val_loss: 8.1834 - val_r2_keras: 0.3470\n",
      "Epoch 72/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 7.7937 - r2_keras: 0.3313 - val_loss: 8.2361 - val_r2_keras: 0.3346\n",
      "Epoch 73/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 7.6861 - r2_keras: 0.3392 - val_loss: 8.1343 - val_r2_keras: 0.3381\n",
      "Epoch 74/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.7582 - r2_keras: 0.3306 - val_loss: 8.2737 - val_r2_keras: 0.3456\n",
      "Epoch 75/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 7.7268 - r2_keras: 0.3402 - val_loss: 8.1753 - val_r2_keras: 0.3451\n",
      "Epoch 76/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.7421 - r2_keras: 0.3378 - val_loss: 8.3859 - val_r2_keras: 0.3366\n",
      "Epoch 77/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 7.6885 - r2_keras: 0.3421 - val_loss: 8.0677 - val_r2_keras: 0.3465\n",
      "Epoch 78/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.6817 - r2_keras: 0.3493 - val_loss: 8.2738 - val_r2_keras: 0.3424\n",
      "Epoch 79/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 7.7125 - r2_keras: 0.3391 - val_loss: 8.0900 - val_r2_keras: 0.3432\n",
      "Epoch 80/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.6606 - r2_keras: 0.3422 - val_loss: 8.2162 - val_r2_keras: 0.3464\n",
      "Epoch 81/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.7002 - r2_keras: 0.3394 - val_loss: 8.3349 - val_r2_keras: 0.3352\n",
      "Epoch 82/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.6759 - r2_keras: 0.3419 - val_loss: 8.0638 - val_r2_keras: 0.3479\n",
      "Epoch 83/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.6274 - r2_keras: 0.3471 - val_loss: 8.0890 - val_r2_keras: 0.3504\n",
      "Epoch 84/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.6727 - r2_keras: 0.3426 - val_loss: 8.0568 - val_r2_keras: 0.3509\n",
      "Epoch 85/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.6668 - r2_keras: 0.3390 - val_loss: 8.2982 - val_r2_keras: 0.3355\n",
      "Epoch 86/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 7.6999 - r2_keras: 0.3399 - val_loss: 8.0967 - val_r2_keras: 0.3442\n",
      "Epoch 87/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.6205 - r2_keras: 0.3472 - val_loss: 8.1191 - val_r2_keras: 0.3477\n",
      "Epoch 88/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 7.6050 - r2_keras: 0.3424 - val_loss: 8.2405 - val_r2_keras: 0.3405\n",
      "Epoch 89/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.7040 - r2_keras: 0.3384 - val_loss: 8.3192 - val_r2_keras: 0.3411\n",
      "Epoch 90/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.6452 - r2_keras: 0.3436 - val_loss: 8.3376 - val_r2_keras: 0.3373\n",
      "Epoch 91/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 7.6342 - r2_keras: 0.3494 - val_loss: 8.1109 - val_r2_keras: 0.3478\n",
      "Epoch 92/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.6146 - r2_keras: 0.3467 - val_loss: 8.1968 - val_r2_keras: 0.3401\n",
      "Epoch 93/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 7.6563 - r2_keras: 0.3393 - val_loss: 8.8327 - val_r2_keras: 0.3075\n",
      "Epoch 94/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 7.6634 - r2_keras: 0.3393 - val_loss: 8.2390 - val_r2_keras: 0.3382\n",
      "Epoch 95/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 7.6285 - r2_keras: 0.3421 - val_loss: 8.3085 - val_r2_keras: 0.3343\n",
      "Epoch 96/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 7.6260 - r2_keras: 0.3439 - val_loss: 9.9354 - val_r2_keras: 0.2011\n",
      "Epoch 97/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 7.5883 - r2_keras: 0.3458 - val_loss: 8.7587 - val_r2_keras: 0.3019\n",
      "Epoch 98/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.6417 - r2_keras: 0.3426 - val_loss: 8.4215 - val_r2_keras: 0.3337\n",
      "Epoch 99/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.6270 - r2_keras: 0.3465 - val_loss: 8.4342 - val_r2_keras: 0.3321\n",
      "Epoch 100/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 7.6495 - r2_keras: 0.3462 - val_loss: 8.2947 - val_r2_keras: 0.3416\n",
      "Epoch 101/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 7.6297 - r2_keras: 0.3425 - val_loss: 8.3243 - val_r2_keras: 0.3399\n",
      "Epoch 102/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 7.5845 - r2_keras: 0.3436 - val_loss: 8.1735 - val_r2_keras: 0.3414\n",
      "Epoch 103/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 7.6582 - r2_keras: 0.3424 - val_loss: 8.3376 - val_r2_keras: 0.3336\n",
      "Epoch 104/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 7.6095 - r2_keras: 0.3484 - val_loss: 8.4336 - val_r2_keras: 0.3292\n",
      "Epoch 00104: early stopping\n",
      "LABEL_ABPm\n",
      "Train on 17095 samples, validate on 1900 samples\n",
      "Epoch 1/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 330.0350 - r2_keras: -1.2342 - val_loss: 251.4097 - val_r2_keras: -0.6164\n",
      "Epoch 2/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 211.0997 - r2_keras: -0.3779 - val_loss: 228.2774 - val_r2_keras: -0.4628\n",
      "Epoch 3/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 43us/step - loss: 174.0643 - r2_keras: -0.1189 - val_loss: 194.9113 - val_r2_keras: -0.2434\n",
      "Epoch 4/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 153.8413 - r2_keras: 0.0067 - val_loss: 142.2713 - val_r2_keras: 0.0918\n",
      "Epoch 5/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 122.3731 - r2_keras: 0.2049 - val_loss: 103.3547 - val_r2_keras: 0.3354\n",
      "Epoch 6/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 99.4020 - r2_keras: 0.3524 - val_loss: 111.8128 - val_r2_keras: 0.2784\n",
      "Epoch 7/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 93.0984 - r2_keras: 0.3938 - val_loss: 98.9389 - val_r2_keras: 0.3629\n",
      "Epoch 8/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 88.4205 - r2_keras: 0.4205 - val_loss: 96.1943 - val_r2_keras: 0.3828\n",
      "Epoch 9/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 86.6253 - r2_keras: 0.4304 - val_loss: 84.3716 - val_r2_keras: 0.4577\n",
      "Epoch 10/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 83.7363 - r2_keras: 0.4534 - val_loss: 89.5775 - val_r2_keras: 0.4252\n",
      "Epoch 11/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 82.6599 - r2_keras: 0.4592 - val_loss: 89.3654 - val_r2_keras: 0.4277\n",
      "Epoch 12/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 80.3616 - r2_keras: 0.4761 - val_loss: 98.1418 - val_r2_keras: 0.3715\n",
      "Epoch 13/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 79.5515 - r2_keras: 0.4742 - val_loss: 78.9268 - val_r2_keras: 0.4931\n",
      "Epoch 14/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 78.9645 - r2_keras: 0.4840 - val_loss: 77.5556 - val_r2_keras: 0.5009\n",
      "Epoch 15/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 77.2641 - r2_keras: 0.4929 - val_loss: 87.6238 - val_r2_keras: 0.4420\n",
      "Epoch 16/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 76.9353 - r2_keras: 0.4943 - val_loss: 83.3798 - val_r2_keras: 0.4653\n",
      "Epoch 17/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 76.2008 - r2_keras: 0.4990 - val_loss: 88.3174 - val_r2_keras: 0.4358\n",
      "Epoch 18/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 75.8278 - r2_keras: 0.5043 - val_loss: 84.9718 - val_r2_keras: 0.4589\n",
      "Epoch 19/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 74.8225 - r2_keras: 0.5107 - val_loss: 91.7672 - val_r2_keras: 0.4121\n",
      "Epoch 20/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 73.9977 - r2_keras: 0.5134 - val_loss: 88.1457 - val_r2_keras: 0.4386\n",
      "Epoch 21/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 74.8035 - r2_keras: 0.5099 - val_loss: 81.8677 - val_r2_keras: 0.4772\n",
      "Epoch 22/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 73.1570 - r2_keras: 0.5175 - val_loss: 80.7648 - val_r2_keras: 0.4841\n",
      "Epoch 23/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 73.9034 - r2_keras: 0.5153 - val_loss: 84.2868 - val_r2_keras: 0.4635\n",
      "Epoch 24/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 73.4631 - r2_keras: 0.5163 - val_loss: 79.5386 - val_r2_keras: 0.4897\n",
      "Epoch 25/2000\n",
      "17095/17095 [==============================] - 1s 72us/step - loss: 72.8543 - r2_keras: 0.5240 - val_loss: 82.1769 - val_r2_keras: 0.4744\n",
      "Epoch 26/2000\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 73.5565 - r2_keras: 0.5159 - val_loss: 79.1974 - val_r2_keras: 0.4947\n",
      "Epoch 27/2000\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 72.4534 - r2_keras: 0.5213 - val_loss: 79.9671 - val_r2_keras: 0.4887\n",
      "Epoch 28/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 72.6711 - r2_keras: 0.5257 - val_loss: 83.0470 - val_r2_keras: 0.4685\n",
      "Epoch 29/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 72.6634 - r2_keras: 0.5218 - val_loss: 80.9638 - val_r2_keras: 0.4836\n",
      "Epoch 30/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 72.0729 - r2_keras: 0.5274 - val_loss: 77.7650 - val_r2_keras: 0.4987\n",
      "Epoch 31/2000\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 72.0987 - r2_keras: 0.5302 - val_loss: 80.3205 - val_r2_keras: 0.4874\n",
      "Epoch 32/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 71.8183 - r2_keras: 0.5240 - val_loss: 74.4979 - val_r2_keras: 0.5208\n",
      "Epoch 33/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 71.3929 - r2_keras: 0.5303 - val_loss: 81.9980 - val_r2_keras: 0.4768\n",
      "Epoch 34/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 71.9256 - r2_keras: 0.5293 - val_loss: 78.0290 - val_r2_keras: 0.5012\n",
      "Epoch 35/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 72.2139 - r2_keras: 0.5254 - val_loss: 87.1801 - val_r2_keras: 0.4421\n",
      "Epoch 36/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 72.4101 - r2_keras: 0.5226 - val_loss: 82.5662 - val_r2_keras: 0.4726\n",
      "Epoch 37/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 72.1494 - r2_keras: 0.5247 - val_loss: 79.5224 - val_r2_keras: 0.4919\n",
      "Epoch 38/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 71.5900 - r2_keras: 0.5257 - val_loss: 84.2859 - val_r2_keras: 0.4630\n",
      "Epoch 39/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 71.5645 - r2_keras: 0.5345 - val_loss: 83.6721 - val_r2_keras: 0.4665\n",
      "Epoch 40/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 71.6155 - r2_keras: 0.5321 - val_loss: 86.6807 - val_r2_keras: 0.4484\n",
      "Epoch 41/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 71.6943 - r2_keras: 0.5256 - val_loss: 84.8606 - val_r2_keras: 0.4603\n",
      "Epoch 42/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 70.5247 - r2_keras: 0.5372 - val_loss: 80.9872 - val_r2_keras: 0.4840\n",
      "Epoch 43/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.8845 - r2_keras: 0.5346 - val_loss: 89.8566 - val_r2_keras: 0.4297\n",
      "Epoch 44/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 71.3961 - r2_keras: 0.5329 - val_loss: 72.6514 - val_r2_keras: 0.5295\n",
      "Epoch 45/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 71.3065 - r2_keras: 0.5371 - val_loss: 83.0718 - val_r2_keras: 0.4685\n",
      "Epoch 46/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 71.6712 - r2_keras: 0.5313 - val_loss: 86.8594 - val_r2_keras: 0.4464\n",
      "Epoch 47/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.7750 - r2_keras: 0.5346 - val_loss: 79.2264 - val_r2_keras: 0.4944\n",
      "Epoch 48/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.4051 - r2_keras: 0.5422 - val_loss: 84.7923 - val_r2_keras: 0.4602\n",
      "Epoch 49/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.6642 - r2_keras: 0.5357 - val_loss: 77.7871 - val_r2_keras: 0.5032\n",
      "Epoch 50/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.8118 - r2_keras: 0.5362 - val_loss: 73.6184 - val_r2_keras: 0.5256\n",
      "Epoch 51/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 70.8591 - r2_keras: 0.5336 - val_loss: 82.4587 - val_r2_keras: 0.4750\n",
      "Epoch 52/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.6730 - r2_keras: 0.5366 - val_loss: 77.8578 - val_r2_keras: 0.5020\n",
      "Epoch 53/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.5431 - r2_keras: 0.5385 - val_loss: 77.3227 - val_r2_keras: 0.5062\n",
      "Epoch 54/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 71.3452 - r2_keras: 0.5331 - val_loss: 74.0361 - val_r2_keras: 0.5234\n",
      "Epoch 55/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.6325 - r2_keras: 0.5361 - val_loss: 79.7095 - val_r2_keras: 0.4923\n",
      "Epoch 56/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.3694 - r2_keras: 0.5372 - val_loss: 75.5551 - val_r2_keras: 0.5160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.6364 - r2_keras: 0.5350 - val_loss: 73.3021 - val_r2_keras: 0.5267\n",
      "Epoch 58/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.8217 - r2_keras: 0.5314 - val_loss: 77.4695 - val_r2_keras: 0.5031\n",
      "Epoch 59/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 70.2224 - r2_keras: 0.5357 - val_loss: 74.6066 - val_r2_keras: 0.5208\n",
      "Epoch 60/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.7942 - r2_keras: 0.5328 - val_loss: 77.0950 - val_r2_keras: 0.5066\n",
      "Epoch 61/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.6198 - r2_keras: 0.5372 - val_loss: 77.3442 - val_r2_keras: 0.5048\n",
      "Epoch 62/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.2665 - r2_keras: 0.5392 - val_loss: 80.4927 - val_r2_keras: 0.4870\n",
      "Epoch 63/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.1858 - r2_keras: 0.5381 - val_loss: 73.9843 - val_r2_keras: 0.5243\n",
      "Epoch 64/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.1335 - r2_keras: 0.5409 - val_loss: 74.5991 - val_r2_keras: 0.5205\n",
      "Epoch 65/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 70.0001 - r2_keras: 0.5372 - val_loss: 76.2354 - val_r2_keras: 0.5122\n",
      "Epoch 66/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.6593 - r2_keras: 0.5470 - val_loss: 75.9270 - val_r2_keras: 0.5134\n",
      "Epoch 67/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.9088 - r2_keras: 0.5401 - val_loss: 75.3729 - val_r2_keras: 0.5169\n",
      "Epoch 68/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.6563 - r2_keras: 0.5429 - val_loss: 74.9168 - val_r2_keras: 0.5192\n",
      "Epoch 69/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.2201 - r2_keras: 0.5383 - val_loss: 76.7969 - val_r2_keras: 0.5091\n",
      "Epoch 70/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.0702 - r2_keras: 0.5477 - val_loss: 78.3844 - val_r2_keras: 0.4963\n",
      "Epoch 71/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.5663 - r2_keras: 0.5450 - val_loss: 76.5267 - val_r2_keras: 0.5105\n",
      "Epoch 72/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.9230 - r2_keras: 0.5421 - val_loss: 78.9128 - val_r2_keras: 0.4932\n",
      "Epoch 73/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.6849 - r2_keras: 0.5431 - val_loss: 73.3664 - val_r2_keras: 0.5281\n",
      "Epoch 74/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.7082 - r2_keras: 0.5419 - val_loss: 74.9485 - val_r2_keras: 0.5195\n",
      "Epoch 75/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.6040 - r2_keras: 0.5406 - val_loss: 78.1858 - val_r2_keras: 0.5001\n",
      "Epoch 76/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.1969 - r2_keras: 0.5396 - val_loss: 80.6623 - val_r2_keras: 0.4867\n",
      "Epoch 77/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.7886 - r2_keras: 0.5450 - val_loss: 73.3668 - val_r2_keras: 0.5278\n",
      "Epoch 78/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.1286 - r2_keras: 0.5466 - val_loss: 75.6461 - val_r2_keras: 0.5136\n",
      "Epoch 79/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.5574 - r2_keras: 0.5424 - val_loss: 76.1325 - val_r2_keras: 0.5127\n",
      "Epoch 80/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.5378 - r2_keras: 0.5445 - val_loss: 74.3738 - val_r2_keras: 0.5225\n",
      "Epoch 81/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 70.1280 - r2_keras: 0.5378 - val_loss: 73.4239 - val_r2_keras: 0.5283\n",
      "Epoch 82/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.6145 - r2_keras: 0.5406 - val_loss: 72.8966 - val_r2_keras: 0.5308\n",
      "Epoch 83/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.4031 - r2_keras: 0.5456 - val_loss: 78.3092 - val_r2_keras: 0.4958\n",
      "Epoch 84/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.3299 - r2_keras: 0.5462 - val_loss: 75.1634 - val_r2_keras: 0.5166\n",
      "Epoch 85/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 69.6182 - r2_keras: 0.5422 - val_loss: 76.4991 - val_r2_keras: 0.5102\n",
      "Epoch 86/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 68.5293 - r2_keras: 0.5491 - val_loss: 75.2535 - val_r2_keras: 0.5180\n",
      "Epoch 87/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 68.6264 - r2_keras: 0.5509 - val_loss: 73.2808 - val_r2_keras: 0.5293\n",
      "Epoch 88/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 68.8411 - r2_keras: 0.5460 - val_loss: 78.2312 - val_r2_keras: 0.5018\n",
      "Epoch 89/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 68.9720 - r2_keras: 0.5483 - val_loss: 74.6759 - val_r2_keras: 0.5212\n",
      "Epoch 90/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.0507 - r2_keras: 0.5466 - val_loss: 75.7973 - val_r2_keras: 0.5139\n",
      "Epoch 91/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.4003 - r2_keras: 0.5452 - val_loss: 81.9688 - val_r2_keras: 0.4765\n",
      "Epoch 92/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.0167 - r2_keras: 0.5442 - val_loss: 73.7135 - val_r2_keras: 0.5272\n",
      "Epoch 93/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 69.0973 - r2_keras: 0.5427 - val_loss: 74.7864 - val_r2_keras: 0.5196\n",
      "Epoch 94/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 69.0427 - r2_keras: 0.5515 - val_loss: 76.8468 - val_r2_keras: 0.5080\n",
      "Epoch 00094: early stopping\n",
      "LABEL_Heartrate\n",
      "Train on 17095 samples, validate on 1900 samples\n",
      "Epoch 1/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 213.0865 - r2_keras: -0.0231 - val_loss: 201.5510 - val_r2_keras: 0.0575\n",
      "Epoch 2/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 163.1120 - r2_keras: 0.2122 - val_loss: 141.0824 - val_r2_keras: 0.3376\n",
      "Epoch 3/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 145.3735 - r2_keras: 0.2956 - val_loss: 128.6426 - val_r2_keras: 0.3998\n",
      "Epoch 4/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 135.1653 - r2_keras: 0.3430 - val_loss: 124.0604 - val_r2_keras: 0.4202\n",
      "Epoch 5/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 130.7074 - r2_keras: 0.3635 - val_loss: 114.3389 - val_r2_keras: 0.4659\n",
      "Epoch 6/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 125.1531 - r2_keras: 0.3864 - val_loss: 129.5207 - val_r2_keras: 0.3911\n",
      "Epoch 7/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 120.7334 - r2_keras: 0.4112 - val_loss: 136.4586 - val_r2_keras: 0.3572\n",
      "Epoch 8/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 116.5203 - r2_keras: 0.4280 - val_loss: 117.0923 - val_r2_keras: 0.4527\n",
      "Epoch 9/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 114.0118 - r2_keras: 0.4471 - val_loss: 115.0840 - val_r2_keras: 0.4602\n",
      "Epoch 10/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 110.4957 - r2_keras: 0.4584 - val_loss: 123.7931 - val_r2_keras: 0.4210\n",
      "Epoch 11/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 109.9949 - r2_keras: 0.4611 - val_loss: 128.0263 - val_r2_keras: 0.3959\n",
      "Epoch 12/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 106.0560 - r2_keras: 0.4781 - val_loss: 119.7155 - val_r2_keras: 0.4391\n",
      "Epoch 13/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 105.7805 - r2_keras: 0.4822 - val_loss: 123.0461 - val_r2_keras: 0.4243\n",
      "Epoch 14/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 103.8419 - r2_keras: 0.4926 - val_loss: 134.3577 - val_r2_keras: 0.3684\n",
      "Epoch 15/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 102.6363 - r2_keras: 0.4957 - val_loss: 139.5962 - val_r2_keras: 0.3428\n",
      "Epoch 16/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 40us/step - loss: 101.2912 - r2_keras: 0.5068 - val_loss: 106.9215 - val_r2_keras: 0.4980\n",
      "Epoch 17/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 101.7216 - r2_keras: 0.5038 - val_loss: 114.3530 - val_r2_keras: 0.4682\n",
      "Epoch 18/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 100.4963 - r2_keras: 0.5077 - val_loss: 125.3375 - val_r2_keras: 0.4092\n",
      "Epoch 19/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 100.5124 - r2_keras: 0.5085 - val_loss: 119.4157 - val_r2_keras: 0.4388\n",
      "Epoch 20/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 100.2077 - r2_keras: 0.5110 - val_loss: 128.2968 - val_r2_keras: 0.3962\n",
      "Epoch 21/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 99.8251 - r2_keras: 0.5111 - val_loss: 120.1595 - val_r2_keras: 0.4370\n",
      "Epoch 22/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 98.5025 - r2_keras: 0.5138 - val_loss: 116.3453 - val_r2_keras: 0.4538\n",
      "Epoch 23/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 97.7913 - r2_keras: 0.5200 - val_loss: 107.9788 - val_r2_keras: 0.4937\n",
      "Epoch 24/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 96.7962 - r2_keras: 0.5203 - val_loss: 118.5208 - val_r2_keras: 0.4464\n",
      "Epoch 25/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 97.7363 - r2_keras: 0.5227 - val_loss: 114.0846 - val_r2_keras: 0.4673\n",
      "Epoch 26/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 96.6285 - r2_keras: 0.5327 - val_loss: 118.1755 - val_r2_keras: 0.4460\n",
      "Epoch 27/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 96.6274 - r2_keras: 0.5293 - val_loss: 118.4877 - val_r2_keras: 0.4441\n",
      "Epoch 28/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 96.0370 - r2_keras: 0.5284 - val_loss: 113.7193 - val_r2_keras: 0.4669\n",
      "Epoch 29/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 95.7762 - r2_keras: 0.5320 - val_loss: 116.4350 - val_r2_keras: 0.4592\n",
      "Epoch 30/2000\n",
      "17095/17095 [==============================] - 1s 44us/step - loss: 95.4228 - r2_keras: 0.5352 - val_loss: 128.3633 - val_r2_keras: 0.3968\n",
      "Epoch 31/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 95.4237 - r2_keras: 0.5339 - val_loss: 112.1823 - val_r2_keras: 0.4759\n",
      "Epoch 32/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 95.9604 - r2_keras: 0.5279 - val_loss: 117.4834 - val_r2_keras: 0.4477\n",
      "Epoch 33/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 94.4876 - r2_keras: 0.5405 - val_loss: 117.5754 - val_r2_keras: 0.4513\n",
      "Epoch 34/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 95.5110 - r2_keras: 0.5341 - val_loss: 130.6295 - val_r2_keras: 0.3879\n",
      "Epoch 35/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 95.4336 - r2_keras: 0.5342 - val_loss: 133.0246 - val_r2_keras: 0.3790\n",
      "Epoch 36/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 94.0638 - r2_keras: 0.5381 - val_loss: 116.4839 - val_r2_keras: 0.4553\n",
      "Epoch 37/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 95.1623 - r2_keras: 0.5330 - val_loss: 126.8890 - val_r2_keras: 0.4038\n",
      "Epoch 38/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 93.7123 - r2_keras: 0.5420 - val_loss: 127.7743 - val_r2_keras: 0.3996\n",
      "Epoch 39/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 93.3183 - r2_keras: 0.5435 - val_loss: 127.8821 - val_r2_keras: 0.3999\n",
      "Epoch 40/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 93.7503 - r2_keras: 0.5396 - val_loss: 106.0708 - val_r2_keras: 0.5062\n",
      "Epoch 41/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 94.4094 - r2_keras: 0.5350 - val_loss: 134.7661 - val_r2_keras: 0.3683\n",
      "Epoch 42/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 94.0925 - r2_keras: 0.5344 - val_loss: 116.1942 - val_r2_keras: 0.4573\n",
      "Epoch 43/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 93.4954 - r2_keras: 0.5426 - val_loss: 115.9209 - val_r2_keras: 0.4576\n",
      "Epoch 44/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 92.9535 - r2_keras: 0.5423 - val_loss: 122.6870 - val_r2_keras: 0.4240\n",
      "Epoch 45/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 93.1802 - r2_keras: 0.5422 - val_loss: 110.0517 - val_r2_keras: 0.4865\n",
      "Epoch 46/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 93.3934 - r2_keras: 0.5432 - val_loss: 129.4062 - val_r2_keras: 0.3921\n",
      "Epoch 47/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 93.0753 - r2_keras: 0.5463 - val_loss: 121.3268 - val_r2_keras: 0.4329\n",
      "Epoch 48/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 92.7358 - r2_keras: 0.5465 - val_loss: 112.1659 - val_r2_keras: 0.4745\n",
      "Epoch 49/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 93.4715 - r2_keras: 0.5440 - val_loss: 127.1697 - val_r2_keras: 0.4020\n",
      "Epoch 50/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 91.5824 - r2_keras: 0.5524 - val_loss: 125.4877 - val_r2_keras: 0.4120\n",
      "Epoch 51/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 91.8422 - r2_keras: 0.5504 - val_loss: 117.8917 - val_r2_keras: 0.4483\n",
      "Epoch 52/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 91.8262 - r2_keras: 0.5532 - val_loss: 136.2059 - val_r2_keras: 0.3564\n",
      "Epoch 53/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 91.7032 - r2_keras: 0.5516 - val_loss: 128.6611 - val_r2_keras: 0.3961\n",
      "Epoch 54/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 91.7896 - r2_keras: 0.5504 - val_loss: 134.5693 - val_r2_keras: 0.3687\n",
      "Epoch 55/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 92.4596 - r2_keras: 0.5452 - val_loss: 131.2593 - val_r2_keras: 0.3844\n",
      "Epoch 56/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 91.6298 - r2_keras: 0.5490 - val_loss: 122.0062 - val_r2_keras: 0.4284\n",
      "Epoch 57/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 91.4367 - r2_keras: 0.5511 - val_loss: 130.0162 - val_r2_keras: 0.3854\n",
      "Epoch 58/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 92.1483 - r2_keras: 0.5460 - val_loss: 136.4861 - val_r2_keras: 0.3579\n",
      "Epoch 59/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 91.2092 - r2_keras: 0.5539 - val_loss: 121.4489 - val_r2_keras: 0.4321\n",
      "Epoch 60/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 91.8128 - r2_keras: 0.5501 - val_loss: 121.3790 - val_r2_keras: 0.4301\n",
      "Epoch 61/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 92.0552 - r2_keras: 0.5508 - val_loss: 133.3331 - val_r2_keras: 0.3724\n",
      "Epoch 62/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 91.5917 - r2_keras: 0.5536 - val_loss: 131.3088 - val_r2_keras: 0.3840\n",
      "Epoch 63/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 90.8655 - r2_keras: 0.5534 - val_loss: 123.0059 - val_r2_keras: 0.4228\n",
      "Epoch 64/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 90.6398 - r2_keras: 0.5535 - val_loss: 118.4983 - val_r2_keras: 0.4454\n",
      "Epoch 65/2000\n",
      "17095/17095 [==============================] - 1s 42us/step - loss: 91.3541 - r2_keras: 0.5517 - val_loss: 133.4110 - val_r2_keras: 0.3746\n",
      "Epoch 66/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 91.3340 - r2_keras: 0.5508 - val_loss: 117.0429 - val_r2_keras: 0.4518\n",
      "Epoch 67/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 90.8646 - r2_keras: 0.5487 - val_loss: 134.2489 - val_r2_keras: 0.3653\n",
      "Epoch 68/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 90.9170 - r2_keras: 0.5553 - val_loss: 116.2701 - val_r2_keras: 0.4541\n",
      "Epoch 69/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 39us/step - loss: 90.7639 - r2_keras: 0.5544 - val_loss: 132.1042 - val_r2_keras: 0.3804\n",
      "Epoch 70/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 90.9911 - r2_keras: 0.5524 - val_loss: 122.4319 - val_r2_keras: 0.4251\n",
      "Epoch 71/2000\n",
      "17095/17095 [==============================] - 1s 41us/step - loss: 91.0436 - r2_keras: 0.5507 - val_loss: 150.3987 - val_r2_keras: 0.2907\n",
      "Epoch 72/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 91.5245 - r2_keras: 0.5487 - val_loss: 128.1566 - val_r2_keras: 0.3977\n",
      "Epoch 73/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 90.1561 - r2_keras: 0.5581 - val_loss: 126.4083 - val_r2_keras: 0.4052\n",
      "Epoch 74/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 90.3772 - r2_keras: 0.5572 - val_loss: 128.5743 - val_r2_keras: 0.4001\n",
      "Epoch 75/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 90.2532 - r2_keras: 0.5612 - val_loss: 128.2112 - val_r2_keras: 0.3984\n",
      "Epoch 76/2000\n",
      "17095/17095 [==============================] - 1s 39us/step - loss: 90.8620 - r2_keras: 0.5553 - val_loss: 122.3597 - val_r2_keras: 0.4301\n",
      "Epoch 77/2000\n",
      "17095/17095 [==============================] - 1s 40us/step - loss: 90.1979 - r2_keras: 0.5576 - val_loss: 133.0953 - val_r2_keras: 0.3740\n",
      "Epoch 78/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 89.9496 - r2_keras: 0.5572 - val_loss: 129.3525 - val_r2_keras: 0.3945\n",
      "Epoch 79/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 90.0686 - r2_keras: 0.5582 - val_loss: 152.3592 - val_r2_keras: 0.2814\n",
      "Epoch 80/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 90.0779 - r2_keras: 0.5567 - val_loss: 117.5780 - val_r2_keras: 0.4488\n",
      "Epoch 81/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 89.8570 - r2_keras: 0.5575 - val_loss: 135.6127 - val_r2_keras: 0.3638\n",
      "Epoch 82/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 89.7979 - r2_keras: 0.5565 - val_loss: 122.7917 - val_r2_keras: 0.4229\n",
      "Epoch 83/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 90.8717 - r2_keras: 0.5549 - val_loss: 120.4783 - val_r2_keras: 0.4355\n",
      "Epoch 84/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 89.7039 - r2_keras: 0.5590 - val_loss: 114.5317 - val_r2_keras: 0.4628\n",
      "Epoch 85/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 90.2010 - r2_keras: 0.5592 - val_loss: 125.1441 - val_r2_keras: 0.4144\n",
      "Epoch 86/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 89.6549 - r2_keras: 0.5612 - val_loss: 125.1737 - val_r2_keras: 0.4136\n",
      "Epoch 87/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 90.1441 - r2_keras: 0.5561 - val_loss: 112.5071 - val_r2_keras: 0.4735\n",
      "Epoch 88/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 90.0779 - r2_keras: 0.5612 - val_loss: 140.8595 - val_r2_keras: 0.3353\n",
      "Epoch 89/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 89.1236 - r2_keras: 0.5608 - val_loss: 127.6948 - val_r2_keras: 0.3990\n",
      "Epoch 90/2000\n",
      "17095/17095 [==============================] - 1s 43us/step - loss: 89.7201 - r2_keras: 0.5617 - val_loss: 120.6283 - val_r2_keras: 0.4339\n",
      "Epoch 00090: early stopping\n"
     ]
    }
   ],
   "source": [
    "labels_task3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_Heartrate']\n",
    "predictions_3 = pd.DataFrame(columns=labels_task3)\n",
    "X = fullTrain.iloc[:,:]\n",
    "X_test_ = fullTest\n",
    "\n",
    "for label in labels_task3:\n",
    "    print(label)\n",
    "    y = trains_labels.loc[:,label]\n",
    "    history = model.fit(X,y,epochs=2000,callbacks=[overfitCallback],validation_split=0.1)\n",
    "    predictions_3[label] = pd.DataFrame(model.predict(X_test_))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for LABEL_SpO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X.shape[1], kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l1_l2(l2=1e-3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "usualCallback = EarlyStopping()\n",
    "overfitCallback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=[r2_keras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_SpO2\n",
      "Train on 17095 samples, validate on 1900 samples\n",
      "Epoch 1/2000\n",
      "17095/17095 [==============================] - 1s 69us/step - loss: 467.6428 - r2_keras: -140.9240 - val_loss: 70.1662 - val_r2_keras: -19.3876\n",
      "Epoch 2/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 91.3811 - r2_keras: -25.1358 - val_loss: 245.7966 - val_r2_keras: -79.6089\n",
      "Epoch 3/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 75.4540 - r2_keras: -20.0378 - val_loss: 191.8211 - val_r2_keras: -61.1680\n",
      "Epoch 4/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 68.3298 - r2_keras: -17.5407 - val_loss: 138.9107 - val_r2_keras: -43.1647\n",
      "Epoch 5/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 59.9676 - r2_keras: -15.1388 - val_loss: 147.8838 - val_r2_keras: -46.4446\n",
      "Epoch 6/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 48.8601 - r2_keras: -11.6499 - val_loss: 14.4991 - val_r2_keras: -0.5014\n",
      "Epoch 7/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 41.3766 - r2_keras: -9.4114 - val_loss: 14.6559 - val_r2_keras: -0.8747\n",
      "Epoch 8/2000\n",
      "17095/17095 [==============================] - 1s 82us/step - loss: 35.7651 - r2_keras: -7.8632 - val_loss: 13.0262 - val_r2_keras: -0.5053\n",
      "Epoch 9/2000\n",
      "17095/17095 [==============================] - 1s 73us/step - loss: 32.2369 - r2_keras: -7.0197 - val_loss: 19.7810 - val_r2_keras: -3.2166\n",
      "Epoch 10/2000\n",
      "17095/17095 [==============================] - 1s 81us/step - loss: 29.9001 - r2_keras: -6.4165 - val_loss: 12.3449 - val_r2_keras: -0.6148\n",
      "Epoch 11/2000\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 27.9853 - r2_keras: -5.8532 - val_loss: 10.6289 - val_r2_keras: -0.3959\n",
      "Epoch 12/2000\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 26.2079 - r2_keras: -5.4936 - val_loss: 20.3009 - val_r2_keras: -4.0439\n",
      "Epoch 13/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 24.9382 - r2_keras: -5.2677 - val_loss: 10.9214 - val_r2_keras: -0.7233\n",
      "Epoch 14/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 23.3765 - r2_keras: -5.0066 - val_loss: 8.4514 - val_r2_keras: -0.2405\n",
      "Epoch 15/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 22.1835 - r2_keras: -4.6504 - val_loss: 7.1020 - val_r2_keras: 0.1385\n",
      "Epoch 16/2000\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 20.7555 - r2_keras: -4.3849 - val_loss: 6.8520 - val_r2_keras: -0.0117\n",
      "Epoch 17/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 19.3226 - r2_keras: -4.0460 - val_loss: 6.2025 - val_r2_keras: 0.1389\n",
      "Epoch 18/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 18.2343 - r2_keras: -3.8677 - val_loss: 5.3535 - val_r2_keras: 0.2111\n",
      "Epoch 19/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 17.1352 - r2_keras: -3.6107 - val_loss: 5.8515 - val_r2_keras: 0.0095\n",
      "Epoch 20/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 15.3881 - r2_keras: -3.1574 - val_loss: 4.6431 - val_r2_keras: 0.2666\n",
      "Epoch 21/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 14.3402 - r2_keras: -2.9371 - val_loss: 4.1571 - val_r2_keras: 0.3042\n",
      "Epoch 22/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 12.7148 - r2_keras: -2.4937 - val_loss: 7.5567 - val_r2_keras: -1.0483\n",
      "Epoch 23/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 11.5043 - r2_keras: -2.1494 - val_loss: 3.7360 - val_r2_keras: 0.2825\n",
      "Epoch 24/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 10.4353 - r2_keras: -1.8127 - val_loss: 4.0302 - val_r2_keras: 0.1432\n",
      "Epoch 25/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 9.6748 - r2_keras: -1.6371 - val_loss: 3.6396 - val_r2_keras: 0.2873\n",
      "Epoch 26/2000\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 8.4930 - r2_keras: -1.2483 - val_loss: 3.4127 - val_r2_keras: 0.3304\n",
      "Epoch 27/2000\n",
      "17095/17095 [==============================] - 1s 76us/step - loss: 7.7266 - r2_keras: -1.0392 - val_loss: 3.4134 - val_r2_keras: 0.3227\n",
      "Epoch 28/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 6.8848 - r2_keras: -0.7730 - val_loss: 3.5988 - val_r2_keras: 0.2641\n",
      "Epoch 29/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 6.3144 - r2_keras: -0.5916 - val_loss: 3.7288 - val_r2_keras: 0.1661\n",
      "Epoch 30/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 5.9085 - r2_keras: -0.4682 - val_loss: 4.3552 - val_r2_keras: 0.0276\n",
      "Epoch 31/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 5.1953 - r2_keras: -0.2468 - val_loss: 3.3123 - val_r2_keras: 0.3020\n",
      "Epoch 32/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 4.8835 - r2_keras: -0.1609 - val_loss: 3.8826 - val_r2_keras: 0.1531\n",
      "Epoch 33/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 4.6015 - r2_keras: -0.0558 - val_loss: 3.2350 - val_r2_keras: 0.3250\n",
      "Epoch 34/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 4.3783 - r2_keras: 0.0089 - val_loss: 3.4220 - val_r2_keras: 0.2855\n",
      "Epoch 35/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 4.2175 - r2_keras: 0.0612 - val_loss: 3.2153 - val_r2_keras: 0.3362\n",
      "Epoch 36/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 4.1073 - r2_keras: 0.0720 - val_loss: 3.9691 - val_r2_keras: 0.1091\n",
      "Epoch 37/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.9947 - r2_keras: 0.1111 - val_loss: 3.3702 - val_r2_keras: 0.2877\n",
      "Epoch 38/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.9317 - r2_keras: 0.1366 - val_loss: 3.9248 - val_r2_keras: 0.1144\n",
      "Epoch 39/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.8328 - r2_keras: 0.1657 - val_loss: 3.3276 - val_r2_keras: 0.2935\n",
      "Epoch 40/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.8067 - r2_keras: 0.1753 - val_loss: 3.3367 - val_r2_keras: 0.2930\n",
      "Epoch 41/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.8022 - r2_keras: 0.1740 - val_loss: 3.4554 - val_r2_keras: 0.2043\n",
      "Epoch 42/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.7536 - r2_keras: 0.1916 - val_loss: 3.1478 - val_r2_keras: 0.3321\n",
      "Epoch 43/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.7857 - r2_keras: 0.1757 - val_loss: 3.4673 - val_r2_keras: 0.1940\n",
      "Epoch 44/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.7863 - r2_keras: 0.1792 - val_loss: 3.1460 - val_r2_keras: 0.3191\n",
      "Epoch 45/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.7444 - r2_keras: 0.1933 - val_loss: 3.4654 - val_r2_keras: 0.2492\n",
      "Epoch 46/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.6532 - r2_keras: 0.2172 - val_loss: 3.2568 - val_r2_keras: 0.3079\n",
      "Epoch 47/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.6722 - r2_keras: 0.2169 - val_loss: 3.5152 - val_r2_keras: 0.1801\n",
      "Epoch 48/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.6442 - r2_keras: 0.2250 - val_loss: 3.1660 - val_r2_keras: 0.3133\n",
      "Epoch 49/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.6347 - r2_keras: 0.2289 - val_loss: 3.2167 - val_r2_keras: 0.3191\n",
      "Epoch 50/2000\n",
      "17095/17095 [==============================] - 1s 70us/step - loss: 3.5952 - r2_keras: 0.2345 - val_loss: 3.2440 - val_r2_keras: 0.3069\n",
      "Epoch 51/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.5858 - r2_keras: 0.2385 - val_loss: 3.1480 - val_r2_keras: 0.3265\n",
      "Epoch 52/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.5586 - r2_keras: 0.2496 - val_loss: 3.1463 - val_r2_keras: 0.3302\n",
      "Epoch 53/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.5722 - r2_keras: 0.2377 - val_loss: 3.1254 - val_r2_keras: 0.3302\n",
      "Epoch 54/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 65us/step - loss: 3.5468 - r2_keras: 0.2483 - val_loss: 3.1498 - val_r2_keras: 0.3300\n",
      "Epoch 55/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.5650 - r2_keras: 0.2428 - val_loss: 3.2021 - val_r2_keras: 0.2956\n",
      "Epoch 56/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.5479 - r2_keras: 0.2426 - val_loss: 3.1733 - val_r2_keras: 0.3071\n",
      "Epoch 57/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.5122 - r2_keras: 0.2559 - val_loss: 3.1598 - val_r2_keras: 0.2974\n",
      "Epoch 58/2000\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 3.5100 - r2_keras: 0.2532 - val_loss: 3.3455 - val_r2_keras: 0.2261\n",
      "Epoch 59/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.5453 - r2_keras: 0.2462 - val_loss: 3.2084 - val_r2_keras: 0.2821\n",
      "Epoch 60/2000\n",
      "17095/17095 [==============================] - 1s 64us/step - loss: 3.5543 - r2_keras: 0.2433 - val_loss: 3.1643 - val_r2_keras: 0.3165\n",
      "Epoch 61/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.5249 - r2_keras: 0.2469 - val_loss: 3.1543 - val_r2_keras: 0.3126\n",
      "Epoch 62/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.5158 - r2_keras: 0.2542 - val_loss: 3.1142 - val_r2_keras: 0.3327\n",
      "Epoch 63/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.5160 - r2_keras: 0.2539 - val_loss: 3.2154 - val_r2_keras: 0.2840\n",
      "Epoch 64/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4973 - r2_keras: 0.2505 - val_loss: 3.0961 - val_r2_keras: 0.3359\n",
      "Epoch 65/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.5150 - r2_keras: 0.2503 - val_loss: 3.1343 - val_r2_keras: 0.3301\n",
      "Epoch 66/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.5259 - r2_keras: 0.2469 - val_loss: 3.1156 - val_r2_keras: 0.3371\n",
      "Epoch 67/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.5031 - r2_keras: 0.2570 - val_loss: 3.1046 - val_r2_keras: 0.3191\n",
      "Epoch 68/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.5229 - r2_keras: 0.2513 - val_loss: 3.1338 - val_r2_keras: 0.3171\n",
      "Epoch 69/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.5084 - r2_keras: 0.2511 - val_loss: 3.1654 - val_r2_keras: 0.3074\n",
      "Epoch 70/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4915 - r2_keras: 0.2609 - val_loss: 3.0928 - val_r2_keras: 0.3403\n",
      "Epoch 71/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4976 - r2_keras: 0.2565 - val_loss: 3.1280 - val_r2_keras: 0.3321\n",
      "Epoch 72/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4755 - r2_keras: 0.2621 - val_loss: 3.0880 - val_r2_keras: 0.3300\n",
      "Epoch 73/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.5002 - r2_keras: 0.2570 - val_loss: 3.3920 - val_r2_keras: 0.2645\n",
      "Epoch 74/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4986 - r2_keras: 0.2570 - val_loss: 3.1826 - val_r2_keras: 0.2830\n",
      "Epoch 75/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4982 - r2_keras: 0.2578 - val_loss: 3.1609 - val_r2_keras: 0.3220\n",
      "Epoch 76/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.5061 - r2_keras: 0.2550 - val_loss: 3.2808 - val_r2_keras: 0.2861\n",
      "Epoch 77/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4923 - r2_keras: 0.2560 - val_loss: 3.0771 - val_r2_keras: 0.3406\n",
      "Epoch 78/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.5046 - r2_keras: 0.2613 - val_loss: 3.1571 - val_r2_keras: 0.3108\n",
      "Epoch 79/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4989 - r2_keras: 0.2538 - val_loss: 3.2096 - val_r2_keras: 0.2931\n",
      "Epoch 80/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4838 - r2_keras: 0.2596 - val_loss: 3.1137 - val_r2_keras: 0.3176\n",
      "Epoch 81/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4996 - r2_keras: 0.2480 - val_loss: 3.1943 - val_r2_keras: 0.3050\n",
      "Epoch 82/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4801 - r2_keras: 0.2546 - val_loss: 3.0758 - val_r2_keras: 0.3452\n",
      "Epoch 83/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4919 - r2_keras: 0.2595 - val_loss: 3.2086 - val_r2_keras: 0.2837\n",
      "Epoch 84/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4723 - r2_keras: 0.2602 - val_loss: 3.2170 - val_r2_keras: 0.3090\n",
      "Epoch 85/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.5050 - r2_keras: 0.2527 - val_loss: 3.0924 - val_r2_keras: 0.3358\n",
      "Epoch 86/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4709 - r2_keras: 0.2576 - val_loss: 3.0768 - val_r2_keras: 0.3397\n",
      "Epoch 87/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4786 - r2_keras: 0.2527 - val_loss: 3.0633 - val_r2_keras: 0.3410\n",
      "Epoch 88/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.5091 - r2_keras: 0.2513 - val_loss: 3.1067 - val_r2_keras: 0.3248\n",
      "Epoch 89/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4920 - r2_keras: 0.2505 - val_loss: 3.0882 - val_r2_keras: 0.3300\n",
      "Epoch 90/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4924 - r2_keras: 0.2542 - val_loss: 3.2542 - val_r2_keras: 0.2993\n",
      "Epoch 91/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.4753 - r2_keras: 0.2563 - val_loss: 3.0871 - val_r2_keras: 0.3413\n",
      "Epoch 92/2000\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 3.4743 - r2_keras: 0.2635 - val_loss: 3.0367 - val_r2_keras: 0.3529\n",
      "Epoch 93/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4831 - r2_keras: 0.2514 - val_loss: 3.0550 - val_r2_keras: 0.3486\n",
      "Epoch 94/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4725 - r2_keras: 0.2587 - val_loss: 3.0632 - val_r2_keras: 0.3449\n",
      "Epoch 95/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4792 - r2_keras: 0.2601 - val_loss: 3.1160 - val_r2_keras: 0.3365\n",
      "Epoch 96/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4781 - r2_keras: 0.2619 - val_loss: 3.1614 - val_r2_keras: 0.3221\n",
      "Epoch 97/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4715 - r2_keras: 0.2622 - val_loss: 3.1417 - val_r2_keras: 0.3183\n",
      "Epoch 98/2000\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 3.4843 - r2_keras: 0.2554 - val_loss: 3.0815 - val_r2_keras: 0.3310\n",
      "Epoch 99/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4842 - r2_keras: 0.2640 - val_loss: 3.2902 - val_r2_keras: 0.2497\n",
      "Epoch 100/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4817 - r2_keras: 0.2618 - val_loss: 3.1370 - val_r2_keras: 0.3079\n",
      "Epoch 101/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4856 - r2_keras: 0.2571 - val_loss: 3.2042 - val_r2_keras: 0.3097\n",
      "Epoch 102/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4765 - r2_keras: 0.2619 - val_loss: 3.1043 - val_r2_keras: 0.3314\n",
      "Epoch 103/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4647 - r2_keras: 0.2657 - val_loss: 3.0867 - val_r2_keras: 0.3340\n",
      "Epoch 104/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4684 - r2_keras: 0.2687 - val_loss: 3.0920 - val_r2_keras: 0.3271\n",
      "Epoch 105/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4617 - r2_keras: 0.2664 - val_loss: 3.1174 - val_r2_keras: 0.3338\n",
      "Epoch 106/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4860 - r2_keras: 0.2600 - val_loss: 3.0780 - val_r2_keras: 0.3479\n",
      "Epoch 107/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4506 - r2_keras: 0.2582 - val_loss: 3.0572 - val_r2_keras: 0.3479\n",
      "Epoch 108/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4911 - r2_keras: 0.2485 - val_loss: 3.0950 - val_r2_keras: 0.3294\n",
      "Epoch 109/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4691 - r2_keras: 0.2638 - val_loss: 3.1644 - val_r2_keras: 0.2934\n",
      "Epoch 110/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4784 - r2_keras: 0.2637 - val_loss: 3.0762 - val_r2_keras: 0.3408\n",
      "Epoch 111/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4644 - r2_keras: 0.2591 - val_loss: 3.0531 - val_r2_keras: 0.3468\n",
      "Epoch 112/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4657 - r2_keras: 0.2661 - val_loss: 3.1248 - val_r2_keras: 0.3290\n",
      "Epoch 113/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4605 - r2_keras: 0.2624 - val_loss: 3.0778 - val_r2_keras: 0.3357\n",
      "Epoch 114/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4695 - r2_keras: 0.2607 - val_loss: 3.0778 - val_r2_keras: 0.3370\n",
      "Epoch 115/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4652 - r2_keras: 0.2661 - val_loss: 3.0769 - val_r2_keras: 0.3460\n",
      "Epoch 116/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4875 - r2_keras: 0.2535 - val_loss: 3.1065 - val_r2_keras: 0.3191\n",
      "Epoch 117/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4478 - r2_keras: 0.2672 - val_loss: 3.0563 - val_r2_keras: 0.3363\n",
      "Epoch 118/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4737 - r2_keras: 0.2600 - val_loss: 3.1140 - val_r2_keras: 0.3354\n",
      "Epoch 119/2000\n",
      "17095/17095 [==============================] - 1s 60us/step - loss: 3.4620 - r2_keras: 0.2547 - val_loss: 3.0808 - val_r2_keras: 0.3407\n",
      "Epoch 120/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4875 - r2_keras: 0.2593 - val_loss: 3.0849 - val_r2_keras: 0.3410\n",
      "Epoch 121/2000\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 3.4641 - r2_keras: 0.2607 - val_loss: 3.0827 - val_r2_keras: 0.3404\n",
      "Epoch 122/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 3.4991 - r2_keras: 0.2459 - val_loss: 3.0861 - val_r2_keras: 0.3364\n",
      "Epoch 123/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4727 - r2_keras: 0.2603 - val_loss: 3.1173 - val_r2_keras: 0.3367\n",
      "Epoch 124/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4579 - r2_keras: 0.2639 - val_loss: 3.0402 - val_r2_keras: 0.3494\n",
      "Epoch 125/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4677 - r2_keras: 0.2564 - val_loss: 3.2206 - val_r2_keras: 0.3024\n",
      "Epoch 126/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4405 - r2_keras: 0.2634 - val_loss: 3.0769 - val_r2_keras: 0.3367\n",
      "Epoch 127/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4980 - r2_keras: 0.2565 - val_loss: 3.0855 - val_r2_keras: 0.3337\n",
      "Epoch 128/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4923 - r2_keras: 0.2563 - val_loss: 3.0877 - val_r2_keras: 0.3442\n",
      "Epoch 129/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4534 - r2_keras: 0.2676 - val_loss: 3.0480 - val_r2_keras: 0.3498\n",
      "Epoch 130/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4551 - r2_keras: 0.2655 - val_loss: 3.0865 - val_r2_keras: 0.3388\n",
      "Epoch 131/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4755 - r2_keras: 0.2602 - val_loss: 3.2119 - val_r2_keras: 0.3042\n",
      "Epoch 132/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4827 - r2_keras: 0.2556 - val_loss: 3.1354 - val_r2_keras: 0.3222\n",
      "Epoch 133/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4588 - r2_keras: 0.2646 - val_loss: 3.0322 - val_r2_keras: 0.3541\n",
      "Epoch 134/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4510 - r2_keras: 0.2683 - val_loss: 3.0771 - val_r2_keras: 0.3273\n",
      "Epoch 135/2000\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 3.4648 - r2_keras: 0.2582 - val_loss: 3.0966 - val_r2_keras: 0.3236\n",
      "Epoch 136/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 3.4495 - r2_keras: 0.2606 - val_loss: 3.0968 - val_r2_keras: 0.3228\n",
      "Epoch 137/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4546 - r2_keras: 0.2685 - val_loss: 3.1212 - val_r2_keras: 0.3293\n",
      "Epoch 138/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4418 - r2_keras: 0.2678 - val_loss: 3.1394 - val_r2_keras: 0.3279\n",
      "Epoch 139/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 3.4687 - r2_keras: 0.2559 - val_loss: 3.0784 - val_r2_keras: 0.3423\n",
      "Epoch 140/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 3.4354 - r2_keras: 0.2681 - val_loss: 3.0898 - val_r2_keras: 0.3413\n",
      "Epoch 141/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4717 - r2_keras: 0.2560 - val_loss: 3.1203 - val_r2_keras: 0.3081\n",
      "Epoch 142/2000\n",
      "17095/17095 [==============================] - 1s 65us/step - loss: 3.4396 - r2_keras: 0.2711 - val_loss: 3.0978 - val_r2_keras: 0.3216\n",
      "Epoch 143/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4711 - r2_keras: 0.2546 - val_loss: 3.0776 - val_r2_keras: 0.3359\n",
      "Epoch 144/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4432 - r2_keras: 0.2693 - val_loss: 3.0410 - val_r2_keras: 0.3504\n",
      "Epoch 145/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4832 - r2_keras: 0.2560 - val_loss: 3.1945 - val_r2_keras: 0.2840\n",
      "Epoch 146/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4425 - r2_keras: 0.2672 - val_loss: 3.0364 - val_r2_keras: 0.3426\n",
      "Epoch 147/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4643 - r2_keras: 0.2586 - val_loss: 3.2564 - val_r2_keras: 0.2606\n",
      "Epoch 148/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 3.4914 - r2_keras: 0.2561 - val_loss: 3.0667 - val_r2_keras: 0.3440\n",
      "Epoch 149/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4401 - r2_keras: 0.2744 - val_loss: 3.0775 - val_r2_keras: 0.3433\n",
      "Epoch 150/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4702 - r2_keras: 0.2599 - val_loss: 3.1480 - val_r2_keras: 0.2977\n",
      "Epoch 151/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4477 - r2_keras: 0.2679 - val_loss: 3.1435 - val_r2_keras: 0.3278\n",
      "Epoch 152/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4631 - r2_keras: 0.2593 - val_loss: 3.0518 - val_r2_keras: 0.3494\n",
      "Epoch 153/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 3.4894 - r2_keras: 0.2467 - val_loss: 3.0912 - val_r2_keras: 0.3344\n",
      "Epoch 154/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4682 - r2_keras: 0.2584 - val_loss: 3.1178 - val_r2_keras: 0.3337\n",
      "Epoch 155/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4543 - r2_keras: 0.2677 - val_loss: 3.0632 - val_r2_keras: 0.3488\n",
      "Epoch 156/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4414 - r2_keras: 0.2685 - val_loss: 3.4161 - val_r2_keras: 0.2467\n",
      "Epoch 157/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 3.4636 - r2_keras: 0.2585 - val_loss: 3.0631 - val_r2_keras: 0.3499\n",
      "Epoch 158/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 3.4467 - r2_keras: 0.2627 - val_loss: 3.1093 - val_r2_keras: 0.3327\n",
      "Epoch 159/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4778 - r2_keras: 0.2545 - val_loss: 3.1546 - val_r2_keras: 0.3213\n",
      "Epoch 160/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4474 - r2_keras: 0.2602 - val_loss: 3.0607 - val_r2_keras: 0.3409\n",
      "Epoch 161/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4439 - r2_keras: 0.2688 - val_loss: 3.3959 - val_r2_keras: 0.2001\n",
      "Epoch 162/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4747 - r2_keras: 0.2591 - val_loss: 3.0409 - val_r2_keras: 0.3445\n",
      "Epoch 163/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4544 - r2_keras: 0.2598 - val_loss: 3.0431 - val_r2_keras: 0.3453\n",
      "Epoch 164/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4879 - r2_keras: 0.2576 - val_loss: 3.0823 - val_r2_keras: 0.3244\n",
      "Epoch 165/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4410 - r2_keras: 0.2708 - val_loss: 3.0414 - val_r2_keras: 0.3397\n",
      "Epoch 166/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4629 - r2_keras: 0.2642 - val_loss: 3.1696 - val_r2_keras: 0.3178\n",
      "Epoch 167/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4439 - r2_keras: 0.2600 - val_loss: 3.0478 - val_r2_keras: 0.3483\n",
      "Epoch 168/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4395 - r2_keras: 0.2678 - val_loss: 3.0860 - val_r2_keras: 0.3323\n",
      "Epoch 169/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4396 - r2_keras: 0.2655 - val_loss: 3.0440 - val_r2_keras: 0.3466\n",
      "Epoch 170/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4329 - r2_keras: 0.2691 - val_loss: 3.0379 - val_r2_keras: 0.3565\n",
      "Epoch 171/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4538 - r2_keras: 0.2668 - val_loss: 3.2121 - val_r2_keras: 0.2757\n",
      "Epoch 172/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4740 - r2_keras: 0.2613 - val_loss: 3.0472 - val_r2_keras: 0.3454\n",
      "Epoch 173/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4756 - r2_keras: 0.2617 - val_loss: 3.1001 - val_r2_keras: 0.3356\n",
      "Epoch 174/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4526 - r2_keras: 0.2686 - val_loss: 3.0350 - val_r2_keras: 0.3456\n",
      "Epoch 175/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4494 - r2_keras: 0.2705 - val_loss: 3.1510 - val_r2_keras: 0.2996\n",
      "Epoch 176/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 3.4179 - r2_keras: 0.2772 - val_loss: 3.0211 - val_r2_keras: 0.3597\n",
      "Epoch 177/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4447 - r2_keras: 0.2718 - val_loss: 3.1101 - val_r2_keras: 0.3171\n",
      "Epoch 178/2000\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 3.4533 - r2_keras: 0.2664 - val_loss: 3.1611 - val_r2_keras: 0.3213\n",
      "Epoch 179/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 3.4372 - r2_keras: 0.2674 - val_loss: 3.1287 - val_r2_keras: 0.3262\n",
      "Epoch 180/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 3.4410 - r2_keras: 0.2658 - val_loss: 3.1017 - val_r2_keras: 0.3316\n",
      "Epoch 181/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4400 - r2_keras: 0.2705 - val_loss: 3.1954 - val_r2_keras: 0.2759\n",
      "Epoch 182/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4732 - r2_keras: 0.2596 - val_loss: 3.0448 - val_r2_keras: 0.3542\n",
      "Epoch 183/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4312 - r2_keras: 0.2686 - val_loss: 3.0729 - val_r2_keras: 0.3434\n",
      "Epoch 184/2000\n",
      "17095/17095 [==============================] - 1s 69us/step - loss: 3.4287 - r2_keras: 0.2701 - val_loss: 3.1650 - val_r2_keras: 0.3258\n",
      "Epoch 185/2000\n",
      "17095/17095 [==============================] - 1s 68us/step - loss: 3.4689 - r2_keras: 0.2600 - val_loss: 3.0633 - val_r2_keras: 0.3480\n",
      "Epoch 186/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4266 - r2_keras: 0.2705 - val_loss: 3.1636 - val_r2_keras: 0.3114\n",
      "Epoch 187/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4504 - r2_keras: 0.2636 - val_loss: 3.1600 - val_r2_keras: 0.3188\n",
      "Epoch 188/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4483 - r2_keras: 0.2652 - val_loss: 3.0508 - val_r2_keras: 0.3420\n",
      "Epoch 189/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4474 - r2_keras: 0.2668 - val_loss: 3.0740 - val_r2_keras: 0.3293\n",
      "Epoch 190/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4663 - r2_keras: 0.2615 - val_loss: 3.0847 - val_r2_keras: 0.3288\n",
      "Epoch 191/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 3.4453 - r2_keras: 0.2613 - val_loss: 3.1618 - val_r2_keras: 0.3193\n",
      "Epoch 192/2000\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 3.4582 - r2_keras: 0.2608 - val_loss: 3.1706 - val_r2_keras: 0.2917\n",
      "Epoch 193/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4643 - r2_keras: 0.2559 - val_loss: 3.0684 - val_r2_keras: 0.3425\n",
      "Epoch 194/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4493 - r2_keras: 0.2642 - val_loss: 3.1042 - val_r2_keras: 0.3353\n",
      "Epoch 195/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4324 - r2_keras: 0.2704 - val_loss: 3.0997 - val_r2_keras: 0.3345\n",
      "Epoch 196/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4351 - r2_keras: 0.2691 - val_loss: 3.1013 - val_r2_keras: 0.3398\n",
      "Epoch 197/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4355 - r2_keras: 0.2683 - val_loss: 3.0638 - val_r2_keras: 0.3414\n",
      "Epoch 198/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4440 - r2_keras: 0.2683 - val_loss: 3.0540 - val_r2_keras: 0.3427\n",
      "Epoch 199/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4409 - r2_keras: 0.2674 - val_loss: 3.1654 - val_r2_keras: 0.3238\n",
      "Epoch 200/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.4319 - r2_keras: 0.2664 - val_loss: 3.0180 - val_r2_keras: 0.3528\n",
      "Epoch 201/2000\n",
      "17095/17095 [==============================] - 1s 55us/step - loss: 3.4335 - r2_keras: 0.2630 - val_loss: 3.0758 - val_r2_keras: 0.3365\n",
      "Epoch 202/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4466 - r2_keras: 0.2638 - val_loss: 3.0843 - val_r2_keras: 0.3368\n",
      "Epoch 203/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.4358 - r2_keras: 0.2678 - val_loss: 3.1123 - val_r2_keras: 0.3324\n",
      "Epoch 204/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4374 - r2_keras: 0.2683 - val_loss: 3.1667 - val_r2_keras: 0.3228\n",
      "Epoch 205/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4182 - r2_keras: 0.2727 - val_loss: 3.0770 - val_r2_keras: 0.3302\n",
      "Epoch 206/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4353 - r2_keras: 0.2657 - val_loss: 3.0407 - val_r2_keras: 0.3426\n",
      "Epoch 207/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.4370 - r2_keras: 0.2651 - val_loss: 3.0427 - val_r2_keras: 0.3472\n",
      "Epoch 208/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4313 - r2_keras: 0.2700 - val_loss: 3.1535 - val_r2_keras: 0.3255\n",
      "Epoch 209/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4578 - r2_keras: 0.2584 - val_loss: 3.0767 - val_r2_keras: 0.3427\n",
      "Epoch 210/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4337 - r2_keras: 0.2723 - val_loss: 3.0461 - val_r2_keras: 0.3501\n",
      "Epoch 211/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4427 - r2_keras: 0.2660 - val_loss: 3.1190 - val_r2_keras: 0.3121\n",
      "Epoch 212/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4410 - r2_keras: 0.2665 - val_loss: 3.0477 - val_r2_keras: 0.3486\n",
      "Epoch 213/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4364 - r2_keras: 0.2681 - val_loss: 3.0923 - val_r2_keras: 0.3250\n",
      "Epoch 214/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4257 - r2_keras: 0.2754 - val_loss: 3.1103 - val_r2_keras: 0.3273\n",
      "Epoch 215/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4574 - r2_keras: 0.2668 - val_loss: 3.0635 - val_r2_keras: 0.3472\n",
      "Epoch 216/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4384 - r2_keras: 0.2747 - val_loss: 3.0571 - val_r2_keras: 0.3416\n",
      "Epoch 217/2000\n",
      "17095/17095 [==============================] - 1s 57us/step - loss: 3.4801 - r2_keras: 0.2578 - val_loss: 3.0345 - val_r2_keras: 0.3514\n",
      "Epoch 218/2000\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 3.4436 - r2_keras: 0.2670 - val_loss: 3.0757 - val_r2_keras: 0.3321\n",
      "Epoch 219/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4223 - r2_keras: 0.2742 - val_loss: 3.1019 - val_r2_keras: 0.3140\n",
      "Epoch 220/2000\n",
      "17095/17095 [==============================] - 1s 62us/step - loss: 3.4568 - r2_keras: 0.2627 - val_loss: 3.1310 - val_r2_keras: 0.3045\n",
      "Epoch 221/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 3.4424 - r2_keras: 0.2617 - val_loss: 3.1478 - val_r2_keras: 0.3245\n",
      "Epoch 222/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 3.4231 - r2_keras: 0.2707 - val_loss: 3.1044 - val_r2_keras: 0.3110\n",
      "Epoch 223/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 3.4315 - r2_keras: 0.2688 - val_loss: 2.9979 - val_r2_keras: 0.3572\n",
      "Epoch 224/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4316 - r2_keras: 0.2681 - val_loss: 3.0915 - val_r2_keras: 0.3442\n",
      "Epoch 225/2000\n",
      "17095/17095 [==============================] - 1s 52us/step - loss: 3.4184 - r2_keras: 0.2765 - val_loss: 3.0960 - val_r2_keras: 0.3156\n",
      "Epoch 226/2000\n",
      "17095/17095 [==============================] - 1s 61us/step - loss: 3.4460 - r2_keras: 0.2635 - val_loss: 3.0924 - val_r2_keras: 0.3424\n",
      "Epoch 227/2000\n",
      "17095/17095 [==============================] - 1s 59us/step - loss: 3.4274 - r2_keras: 0.2674 - val_loss: 3.0815 - val_r2_keras: 0.3436\n",
      "Epoch 228/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4355 - r2_keras: 0.2702 - val_loss: 3.1028 - val_r2_keras: 0.3314\n",
      "Epoch 229/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4535 - r2_keras: 0.2653 - val_loss: 3.1441 - val_r2_keras: 0.3245\n",
      "Epoch 230/2000\n",
      "17095/17095 [==============================] - 1s 63us/step - loss: 3.4567 - r2_keras: 0.2712 - val_loss: 3.1463 - val_r2_keras: 0.2973\n",
      "Epoch 231/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4597 - r2_keras: 0.2632 - val_loss: 3.1059 - val_r2_keras: 0.3361\n",
      "Epoch 232/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4634 - r2_keras: 0.2571 - val_loss: 3.1295 - val_r2_keras: 0.3238\n",
      "Epoch 233/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4393 - r2_keras: 0.2632 - val_loss: 3.0864 - val_r2_keras: 0.3185\n",
      "Epoch 234/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4406 - r2_keras: 0.2609 - val_loss: 3.0717 - val_r2_keras: 0.3475\n",
      "Epoch 235/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4426 - r2_keras: 0.2623 - val_loss: 3.2677 - val_r2_keras: 0.2967\n",
      "Epoch 236/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4313 - r2_keras: 0.2713 - val_loss: 3.0354 - val_r2_keras: 0.3538\n",
      "Epoch 237/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4362 - r2_keras: 0.2638 - val_loss: 3.0415 - val_r2_keras: 0.3468\n",
      "Epoch 238/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4509 - r2_keras: 0.2614 - val_loss: 3.1101 - val_r2_keras: 0.3335\n",
      "Epoch 239/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4327 - r2_keras: 0.2679 - val_loss: 3.0625 - val_r2_keras: 0.3488\n",
      "Epoch 240/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4074 - r2_keras: 0.2757 - val_loss: 3.0466 - val_r2_keras: 0.3505\n",
      "Epoch 241/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4317 - r2_keras: 0.2683 - val_loss: 3.0496 - val_r2_keras: 0.3533\n",
      "Epoch 242/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4333 - r2_keras: 0.2660 - val_loss: 3.0650 - val_r2_keras: 0.3447\n",
      "Epoch 243/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4171 - r2_keras: 0.2714 - val_loss: 3.0829 - val_r2_keras: 0.3330\n",
      "Epoch 244/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4112 - r2_keras: 0.2741 - val_loss: 3.0438 - val_r2_keras: 0.3571\n",
      "Epoch 245/2000\n",
      "17095/17095 [==============================] - 1s 54us/step - loss: 3.4275 - r2_keras: 0.2718 - val_loss: 3.1331 - val_r2_keras: 0.3306\n",
      "Epoch 246/2000\n",
      "17095/17095 [==============================] - 1s 58us/step - loss: 3.4177 - r2_keras: 0.2753 - val_loss: 3.0269 - val_r2_keras: 0.3505\n",
      "Epoch 247/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4258 - r2_keras: 0.2573 - val_loss: 3.0283 - val_r2_keras: 0.3562\n",
      "Epoch 248/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4153 - r2_keras: 0.2726 - val_loss: 3.0373 - val_r2_keras: 0.3453\n",
      "Epoch 249/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4213 - r2_keras: 0.2681 - val_loss: 3.0562 - val_r2_keras: 0.3461\n",
      "Epoch 250/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4251 - r2_keras: 0.2733 - val_loss: 3.1414 - val_r2_keras: 0.3240\n",
      "Epoch 251/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4272 - r2_keras: 0.2756 - val_loss: 3.0996 - val_r2_keras: 0.3375\n",
      "Epoch 252/2000\n",
      "17095/17095 [==============================] - 1s 56us/step - loss: 3.4194 - r2_keras: 0.2791 - val_loss: 3.0781 - val_r2_keras: 0.3317\n",
      "Epoch 253/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4419 - r2_keras: 0.2706 - val_loss: 3.1151 - val_r2_keras: 0.3059\n",
      "Epoch 254/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4310 - r2_keras: 0.2714 - val_loss: 3.1189 - val_r2_keras: 0.3072\n",
      "Epoch 255/2000\n",
      "17095/17095 [==============================] - 1s 50us/step - loss: 3.4402 - r2_keras: 0.2666 - val_loss: 3.0808 - val_r2_keras: 0.3238\n",
      "Epoch 256/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4571 - r2_keras: 0.2614 - val_loss: 3.0748 - val_r2_keras: 0.3436\n",
      "Epoch 257/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4266 - r2_keras: 0.2740 - val_loss: 3.1178 - val_r2_keras: 0.3170\n",
      "Epoch 258/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4169 - r2_keras: 0.2715 - val_loss: 3.3847 - val_r2_keras: 0.2058\n",
      "Epoch 259/2000\n",
      "17095/17095 [==============================] - 1s 53us/step - loss: 3.4506 - r2_keras: 0.2570 - val_loss: 3.0698 - val_r2_keras: 0.3481\n",
      "Epoch 260/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4121 - r2_keras: 0.2777 - val_loss: 3.0872 - val_r2_keras: 0.3259\n",
      "Epoch 261/2000\n",
      "17095/17095 [==============================] - 1s 51us/step - loss: 3.4285 - r2_keras: 0.2690 - val_loss: 3.0684 - val_r2_keras: 0.3431\n",
      "Epoch 262/2000\n",
      "17095/17095 [==============================] - 1s 49us/step - loss: 3.4299 - r2_keras: 0.2641 - val_loss: 3.0250 - val_r2_keras: 0.3535\n",
      "Epoch 263/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4036 - r2_keras: 0.2793 - val_loss: 3.1990 - val_r2_keras: 0.2755\n",
      "Epoch 264/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4241 - r2_keras: 0.2701 - val_loss: 3.0572 - val_r2_keras: 0.3280\n",
      "Epoch 265/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4241 - r2_keras: 0.2696 - val_loss: 3.0986 - val_r2_keras: 0.3267\n",
      "Epoch 266/2000\n",
      "17095/17095 [==============================] - 1s 45us/step - loss: 3.4423 - r2_keras: 0.2660 - val_loss: 3.0638 - val_r2_keras: 0.3445\n",
      "Epoch 267/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4394 - r2_keras: 0.2605 - val_loss: 3.1703 - val_r2_keras: 0.3178\n",
      "Epoch 268/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4282 - r2_keras: 0.2701 - val_loss: 3.2313 - val_r2_keras: 0.3022\n",
      "Epoch 269/2000\n",
      "17095/17095 [==============================] - 1s 48us/step - loss: 3.4124 - r2_keras: 0.2755 - val_loss: 3.0538 - val_r2_keras: 0.3547\n",
      "Epoch 270/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4137 - r2_keras: 0.2683 - val_loss: 3.0409 - val_r2_keras: 0.3547\n",
      "Epoch 271/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4259 - r2_keras: 0.2679 - val_loss: 3.1189 - val_r2_keras: 0.3340\n",
      "Epoch 272/2000\n",
      "17095/17095 [==============================] - 1s 47us/step - loss: 3.4352 - r2_keras: 0.2703 - val_loss: 3.0910 - val_r2_keras: 0.3447\n",
      "Epoch 273/2000\n",
      "17095/17095 [==============================] - 1s 46us/step - loss: 3.4271 - r2_keras: 0.2760 - val_loss: 3.0963 - val_r2_keras: 0.3355\n",
      "Epoch 00273: early stopping\n"
     ]
    }
   ],
   "source": [
    "labels_task3 = ['LABEL_SpO2']\n",
    "# predictions_3 = pd.DataFrame(columns=labels_task3)\n",
    "X = fullTrain.iloc[:,:]\n",
    "X_test_ = fullTest\n",
    "\n",
    "for label in labels_task3:\n",
    "    print(label)\n",
    "    y = trains_labels.loc[:,label]\n",
    "    history = model.fit(X,y,epochs=2000,callbacks=[overfitCallback],validation_split=0.1)\n",
    "    predictions_3[label] = pd.DataFrame(model.predict(X_test_))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd9bdbe7a90>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labels_task3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_Heartrate','LABEL_SpO2']\n",
    "\n",
    "trains_labels['LABEL_SpO2'].plot.kde(color='green',legend=True,label='train labels')\n",
    "predictions_3['LABEL_SpO2'].plot.kde(color='blue',legend=True,label='predicted train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.title('Loss / Mean Squared Error')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "      <th>pid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.962665e-02</td>\n",
       "      <td>0.964478</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.892084</td>\n",
       "      <td>0.016785</td>\n",
       "      <td>0.720460</td>\n",
       "      <td>0.996276</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.650935</td>\n",
       "      <td>15.775916</td>\n",
       "      <td>79.999153</td>\n",
       "      <td>98.404076</td>\n",
       "      <td>82.665451</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.281946e-02</td>\n",
       "      <td>0.296179</td>\n",
       "      <td>0.504610</td>\n",
       "      <td>0.565306</td>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.351879</td>\n",
       "      <td>0.358643</td>\n",
       "      <td>0.298018</td>\n",
       "      <td>0.185384</td>\n",
       "      <td>0.366033</td>\n",
       "      <td>0.295259</td>\n",
       "      <td>17.630539</td>\n",
       "      <td>79.175148</td>\n",
       "      <td>96.953094</td>\n",
       "      <td>84.057549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.285356e-03</td>\n",
       "      <td>0.308249</td>\n",
       "      <td>0.520870</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>0.646058</td>\n",
       "      <td>0.218340</td>\n",
       "      <td>0.077543</td>\n",
       "      <td>0.196450</td>\n",
       "      <td>0.126421</td>\n",
       "      <td>0.199568</td>\n",
       "      <td>0.326239</td>\n",
       "      <td>18.253086</td>\n",
       "      <td>73.622459</td>\n",
       "      <td>95.861542</td>\n",
       "      <td>67.078323</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.388120e-03</td>\n",
       "      <td>0.996520</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>0.999663</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>0.815925</td>\n",
       "      <td>0.996285</td>\n",
       "      <td>0.919072</td>\n",
       "      <td>0.996248</td>\n",
       "      <td>0.995949</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>16.477419</td>\n",
       "      <td>87.231560</td>\n",
       "      <td>98.217728</td>\n",
       "      <td>88.681755</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.087665e-04</td>\n",
       "      <td>0.193673</td>\n",
       "      <td>0.662192</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.818105</td>\n",
       "      <td>0.219341</td>\n",
       "      <td>0.321276</td>\n",
       "      <td>0.309741</td>\n",
       "      <td>0.691865</td>\n",
       "      <td>0.095273</td>\n",
       "      <td>0.281637</td>\n",
       "      <td>19.207298</td>\n",
       "      <td>88.118980</td>\n",
       "      <td>96.138718</td>\n",
       "      <td>84.440399</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12659</td>\n",
       "      <td>4.615258e-05</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.747130</td>\n",
       "      <td>0.792102</td>\n",
       "      <td>0.873933</td>\n",
       "      <td>0.173313</td>\n",
       "      <td>0.009985</td>\n",
       "      <td>0.153117</td>\n",
       "      <td>0.106723</td>\n",
       "      <td>0.171091</td>\n",
       "      <td>0.297396</td>\n",
       "      <td>16.303459</td>\n",
       "      <td>70.429771</td>\n",
       "      <td>96.912506</td>\n",
       "      <td>69.909584</td>\n",
       "      <td>31647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12660</td>\n",
       "      <td>7.434177e-03</td>\n",
       "      <td>0.531175</td>\n",
       "      <td>0.738237</td>\n",
       "      <td>0.792169</td>\n",
       "      <td>0.771258</td>\n",
       "      <td>0.484587</td>\n",
       "      <td>0.351465</td>\n",
       "      <td>0.904075</td>\n",
       "      <td>0.098077</td>\n",
       "      <td>0.485637</td>\n",
       "      <td>0.427397</td>\n",
       "      <td>16.240093</td>\n",
       "      <td>80.261917</td>\n",
       "      <td>96.520966</td>\n",
       "      <td>84.648643</td>\n",
       "      <td>31649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12661</td>\n",
       "      <td>1.147740e-08</td>\n",
       "      <td>0.242964</td>\n",
       "      <td>0.943535</td>\n",
       "      <td>0.984845</td>\n",
       "      <td>0.993120</td>\n",
       "      <td>0.359793</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.584926</td>\n",
       "      <td>0.052541</td>\n",
       "      <td>0.077609</td>\n",
       "      <td>0.351806</td>\n",
       "      <td>18.091932</td>\n",
       "      <td>75.191551</td>\n",
       "      <td>98.404076</td>\n",
       "      <td>81.862526</td>\n",
       "      <td>31651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12662</td>\n",
       "      <td>4.106078e-04</td>\n",
       "      <td>0.161213</td>\n",
       "      <td>0.739794</td>\n",
       "      <td>0.780186</td>\n",
       "      <td>0.818322</td>\n",
       "      <td>0.192073</td>\n",
       "      <td>0.322265</td>\n",
       "      <td>0.295749</td>\n",
       "      <td>0.389873</td>\n",
       "      <td>0.453649</td>\n",
       "      <td>0.293586</td>\n",
       "      <td>19.160038</td>\n",
       "      <td>89.935112</td>\n",
       "      <td>97.701996</td>\n",
       "      <td>97.258553</td>\n",
       "      <td>31652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12663</td>\n",
       "      <td>2.518383e-04</td>\n",
       "      <td>0.515617</td>\n",
       "      <td>0.875923</td>\n",
       "      <td>0.912620</td>\n",
       "      <td>0.940671</td>\n",
       "      <td>0.274970</td>\n",
       "      <td>0.046491</td>\n",
       "      <td>0.305259</td>\n",
       "      <td>0.351420</td>\n",
       "      <td>0.237793</td>\n",
       "      <td>0.386912</td>\n",
       "      <td>18.341702</td>\n",
       "      <td>79.943520</td>\n",
       "      <td>98.404076</td>\n",
       "      <td>94.969658</td>\n",
       "      <td>31655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate    pid\n",
       "0          4.962665e-02          0.964478   0.999959            0.999932               0.999988       0.892084         0.016785    0.720460                0.996276     0.040600      0.650935    15.775916   79.999153   98.404076        82.665451      0\n",
       "1          3.281946e-02          0.296179   0.504610            0.565306               0.625117       0.351879         0.358643    0.298018                0.185384     0.366033      0.295259    17.630539   79.175148   96.953094        84.057549      3\n",
       "2          1.285356e-03          0.308249   0.520870            0.576698               0.646058       0.218340         0.077543    0.196450                0.126421     0.199568      0.326239    18.253086   73.622459   95.861542        67.078323      5\n",
       "3          2.388120e-03          0.996520   0.999486            0.999663               0.999933       0.815925         0.996285    0.919072                0.996248     0.995949      0.727532    16.477419   87.231560   98.217728        88.681755      7\n",
       "4          7.087665e-04          0.193673   0.662192            0.801080               0.818105       0.219341         0.321276    0.309741                0.691865     0.095273      0.281637    19.207298   88.118980   96.138718        84.440399      9\n",
       "...                 ...               ...        ...                 ...                    ...            ...              ...         ...                     ...          ...           ...          ...         ...         ...              ...    ...\n",
       "12659      4.615258e-05          0.320400   0.747130            0.792102               0.873933       0.173313         0.009985    0.153117                0.106723     0.171091      0.297396    16.303459   70.429771   96.912506        69.909584  31647\n",
       "12660      7.434177e-03          0.531175   0.738237            0.792169               0.771258       0.484587         0.351465    0.904075                0.098077     0.485637      0.427397    16.240093   80.261917   96.520966        84.648643  31649\n",
       "12661      1.147740e-08          0.242964   0.943535            0.984845               0.993120       0.359793         0.009814    0.584926                0.052541     0.077609      0.351806    18.091932   75.191551   98.404076        81.862526  31651\n",
       "12662      4.106078e-04          0.161213   0.739794            0.780186               0.818322       0.192073         0.322265    0.295749                0.389873     0.453649      0.293586    19.160038   89.935112   97.701996        97.258553  31652\n",
       "12663      2.518383e-04          0.515617   0.875923            0.912620               0.940671       0.274970         0.046491    0.305259                0.351420     0.237793      0.386912    18.341702   79.943520   98.404076        94.969658  31655\n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=trains_labels.columns).fillna(0)\n",
    "results[predictions_1.columns] = predictions_1\n",
    "results[predictions_2.columns] = predictions_2\n",
    "results[predictions_3.columns] = predictions_3\n",
    "\n",
    "results['pid'] = fullTest.index\n",
    "\n",
    "results.fillna(0,inplace=True)\n",
    "#droping the index\n",
    "results.reset_index(inplace=True,drop=True)\n",
    "\n",
    "results.to_csv('./toSubmit-F/prediction.zip', index=False, float_format='%.3f', compression='zip')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5010950626832302 0.5006514798448042 0.5\n",
      "Score of sample.zip with itself as groundtruth 0.5005821808426781\n"
     ]
    }
   ],
   "source": [
    "# %load handout/score_submission.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "\n",
    "def get_score(df_true, df_submission):\n",
    "    df_submission = df_submission.sort_values('pid')\n",
    "    df_true = df_true.sort_values('pid')\n",
    "    task1 = np.mean([metrics.roc_auc_score(df_true[entry], df_submission[entry]) for entry in TESTS])\n",
    "    task2 = metrics.roc_auc_score(df_true['LABEL_Sepsis'], df_submission['LABEL_Sepsis'])\n",
    "    task3 = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(df_true[entry], df_submission[entry])) for entry in VITALS])\n",
    "    score = np.mean([task1, task2, task3])\n",
    "    print(task1, task2, task3)\n",
    "    return score\n",
    "\n",
    "\n",
    "filename = 'handout/sample.zip'\n",
    "# df_submission = pd.read_csv(\"results.csv\")\n",
    "df_submission = results\n",
    "\n",
    "# generate a baseline based on sample.zip\n",
    "df_true = pd.read_csv(filename)\n",
    "for label in TESTS + ['LABEL_Sepsis']:\n",
    "    # round classification labels\n",
    "    df_true[label] = np.around(df_true[label].values)\n",
    "\n",
    "print('Score of sample.zip with itself as groundtruth', get_score(df_true, df_submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "772px",
    "left": "1294px",
    "right": "20px",
    "top": "120px",
    "width": "366px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
